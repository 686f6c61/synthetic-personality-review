[
  {
    "legacy_article_number": 1,
    "title_original": "Personality Traits in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Greg Serapio-García",
      "Mustafa Safdari",
      "Clément Crepy",
      "Luning Sun",
      "Stephen Fitz",
      "Peter Romero",
      "Marwa Abdulhai",
      "Aleksandra Faust",
      "Maja Matarić"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society",
      "Human-Computer Interaction"
    ],
    "source_url": "https://arxiv.org/abs/2307.00184",
    "abstract_en_original": "Large language models have revolutionized natural language processing by enabling coherent and contextually relevant text generation. As these models increasingly power conversational agents, understanding and controlling their synthetic personality traits becomes critical. We present a comprehensive psychometrically valid methodology for administering personality tests to LLMs and shaping personality expression in generated text. Applying this method to 18 LLMs reveals that personality measurements are reliable and valid under specific prompting configurations, with stronger evidence in larger instruction-tuned models. The methodology successfully shapes LLM personality along desired dimensions to mimic specific human profiles, with implications for responsible AI deployment.",
    "resumen_es_original": "Los modelos de lenguaje de gran escala han transformado el procesamiento del lenguaje natural mediante la generación de texto coherente y contextualmente relevante. Dado que estos modelos sustentan agentes conversacionales de uso público masivo, resulta crucial comprender y controlar los rasgos de personalidad sintéticos que exhiben. Se presenta una metodología psicométrica válida y fiable para administrar inventarios de personalidad a estos modelos y modular la expresión de rasgos en el texto generado. La aplicación del método a 18 modelos revela que las mediciones de personalidad resultan fiables y válidas bajo configuraciones específicas de instrucción, con evidencia más robusta en modelos de mayor escala refinados mediante ajuste instruccional. La metodología permite modular la personalidad de los modelos según dimensiones deseadas para reproducir perfiles humanos específicos, con implicaciones para el despliegue responsable de sistemas de inteligencia artificial.",
    "id": "article-001",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.247Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.247Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2307.00184",
      "fetched_title": "Personality Traits in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality Traits in Large Language Models"
  },
  {
    "legacy_article_number": 3,
    "title_original": "LLMs Simulate Big Five Personality Traits: Further Evidence",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Aleksandra Sorokovikova",
      "Natalia Fedorova",
      "Sharwin Rezagholi",
      "Ivan P. Yamshchikov"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Big Five personality traits",
      "Large Language Models"
    ],
    "source_url": "https://arxiv.org/abs/2402.01765",
    "abstract_en_original": "We present an empirical investigation into Big Five personality trait simulation by Llama2, GPT-4, and Mixtral. The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction.",
    "resumen_es_original": "Se presenta una investigación empírica sobre la simulación de rasgos de personalidad Big Five en los modelos Llama2, GPT-4 y Mixtral. El estudio analiza los rasgos de personalidad simulados por estos modelos y su estabilidad temporal, contribuyendo a la comprensión de las capacidades de estos sistemas para simular rasgos de personalidad y sus implicaciones para la interacción persona-computadora personalizada.",
    "id": "article-003",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.288Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.288Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.01765",
      "fetched_title": "LLMs Simulate Big Five Personality Traits: Further Evidence",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "LLMs Simulate Big Five Personality Traits: Further Evidence"
  },
  {
    "legacy_article_number": 4,
    "title_original": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Guangyuan Jiang",
      "Manjie Xu",
      "Song-Chun Zhu",
      "Wenjuan Han",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "keywords": [
      "Language Models",
      "Personality Assessment",
      "Machine Behavior",
      "Psychometric Studies",
      "Big Five",
      "Machine Personality Inventory (MPI)"
    ],
    "source_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html",
    "abstract_en_original": "Standardized and quantified evaluation of machine behaviors constitutes a fundamental challenge in understanding LLMs. We introduce the Machine Personality Inventory (MPI), a tool grounded in Big Five Personality Factors theory for principled quantitative assessment of language model behaviors. Systematic evaluation with MPI provides evidence of its efficacy in characterizing LLM behaviors. We propose Personality Prompting (P²), a method enabling controlled induction of specific personalities, generating diverse and verifiable behavioral outputs. This work advocates personality assessment as a key indicator for evaluating machine behaviors across downstream applications.",
    "resumen_es_original": "La evaluación estandarizada y cuantificada de comportamientos en sistemas artificiales constituye un desafío fundamental para comprender los modelos de lenguaje de gran escala. Se introduce el Inventario de Personalidad de Máquinas (MPI), una herramienta fundamentada en la teoría de los Cinco Grandes Factores de Personalidad para la evaluación cuantitativa y principista de comportamientos en modelos de lenguaje. La evaluación sistemática mediante MPI proporciona evidencia de su eficacia para caracterizar comportamientos. Se propone el método de Inducción de Personalidad mediante Instrucciones (P²), que permite la inducción controlada de personalidades específicas, generando salidas conductuales diversas y verificables. El trabajo propugna la evaluación de personalidad como indicador clave para evaluar comportamientos en aplicaciones posteriores.",
    "id": "article-004",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.289Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.289Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html",
      "fetched_title": "Evaluating and Inducing Personality in Pre-trained Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Evaluating and Inducing Personality in Pre-trained Language Models"
  },
  {
    "legacy_article_number": 6,
    "title_original": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Wenkai Li",
      "Jiarui Liu",
      "Andy Liu",
      "Xuhui Zhou",
      "Mona Diab",
      "Maarten Sap"
    ],
    "keywords": [
      "Computation and Language",
      "Personality traits",
      "Human-grounded data",
      "Personality assessment"
    ],
    "source_url": "https://arxiv.org/abs/2410.16491",
    "abstract_en_original": "Embedding realistic human personality traits in LLMs remains challenging. Previous prompt-based approaches describing desired personality behaviors suffer from realism and validity issues. We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training methods to align LLMs with human personality patterns. Our methods outperform prompting on personality assessments including BFI and IPIP-NEO, with trait correlations more closely matching human data. Experiments reveal models trained toward higher conscientiousness, agreeableness, lower extraversion, and lower neuroticism exhibit superior reasoning performance, consistent with psychological findings on these traits' cognitive impact.",
    "resumen_es_original": "La incorporación de rasgos de personalidad humana realistas en modelos de lenguaje de gran escala constituye un desafío considerable. Los enfoques previos basados en instrucciones que describen comportamientos asociados a rasgos deseados adolecen de problemas de realismo y validez. Se introduce BIG5-CHAT, un conjunto de datos a gran escala con 100,000 diálogos que fundamentan los modelos en patrones auténticos de expresión de personalidad humana. Mediante este conjunto de datos se exploran el ajuste fino supervisado y la optimización directa de preferencias como métodos de entrenamiento para alinear los modelos con patrones de personalidad humana. Los métodos propuestos superan la inducción mediante instrucciones en evaluaciones de personalidad como BFI e IPIP-NEO, con correlaciones de rasgos que se aproximan más a datos humanos. Los experimentos revelan que los modelos entrenados hacia mayor responsabilidad, amabilidad, menor extraversión y menor neuroticismo exhiben desempeño superior en razonamiento, coherente con evidencia psicológica sobre el impacto cognitivo de estos rasgos.",
    "id": "article-006",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.405Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.405Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2410.16491",
      "fetched_title": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data"
  },
  {
    "legacy_article_number": 7,
    "title_original": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Wenkai Li",
      "Jiarui Liu",
      "Andy Liu",
      "Xuhui Zhou",
      "Mona Diab",
      "Maarten Sap"
    ],
    "keywords": [
      "Computation and Language",
      "Personality traits",
      "Human-grounded data"
    ],
    "source_url": "https://openreview.net/forum?id=TqwTzLjzGS",
    "abstract_en_original": "Embedding realistic human personality traits in LLMs remains challenging. We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. Training-based methods explored here align LLMs with human personality patterns, outperforming prompting on personality assessments. Experiments reveal models trained to exhibit certain traits display superior reasoning performance, consistent with psychological findings.",
    "resumen_es_original": "La incorporación de rasgos de personalidad humana realistas en modelos de lenguaje de gran escala constituye un desafío considerable. Se introduce BIG5-CHAT, un conjunto de datos a gran escala con 100,000 diálogos que fundamentan los modelos en patrones auténticos de expresión de personalidad humana. Los métodos de entrenamiento explorados alinean los modelos con patrones de personalidad humana, superando la inducción mediante instrucciones en evaluaciones de personalidad. Los experimentos revelan que los modelos entrenados para exhibir ciertos rasgos muestran desempeño superior en razonamiento, coherente con evidencia psicológica.",
    "id": "article-007",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.408Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.408Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://openreview.net/forum?id=TqwTzLjzGS",
      "fetched_title": "BIG5-CHAT: Shaping LLM Personalities Through Training on...",
      "title_similarity": 0.7273,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "BIG5-CHAT: Shaping LLM Personalities Through Training on..."
  },
  {
    "legacy_article_number": 8,
    "title_original": "On the Reliability of Psychological Scales on Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Jen-tse Huang",
      "Wenxiang Jiao",
      "Man Ho Lam",
      "Eric John Li",
      "Wenxuan Wang",
      "Michael Lyu"
    ],
    "keywords": [
      "Large Language Models",
      "Psychological assessment",
      "Personality traits",
      "Big Five Inventory",
      "Personality emulation"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-main.354/",
    "abstract_en_original": "Determining the reliability of personality assessments applied to LLMs remains a fundamental methodological question. Analysis across 2,500 configuration settings per model reveals various LLMs demonstrate response consistency on the Big Five Inventory, indicating satisfactory reliability levels. The research further explores GPT-3.5's capacity to emulate diverse personality profiles and represent distinct demographic groups through targeted instruction configurations.",
    "resumen_es_original": "La determinación de la fiabilidad de evaluaciones de personalidad aplicadas a modelos de lenguaje de gran escala constituye una cuestión metodológica fundamental. El análisis de 2,500 configuraciones por modelo revela que diversos modelos demuestran coherencia en las respuestas al Inventario Big Five, indicando niveles satisfactorios de fiabilidad. La investigación explora además la capacidad de GPT-3.5 para emular perfiles de personalidad diversos y representar grupos demográficos diferenciados mediante configuraciones de instrucciones específicas.",
    "id": "article-008",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.451Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.451Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-main.354/",
      "fetched_title": "On the Reliability of Psychological Scales on Large Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "On the Reliability of Psychological Scales on Large Language Models"
  },
  {
    "legacy_article_number": 9,
    "title_original": "Manipulating the Perceived Personality Traits of Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Graham Caron",
      "Shashank Srivastava"
    ],
    "keywords": [
      "Personality traits",
      "Big Five personality model",
      "Language models",
      "Dialog systems",
      "Computational psychology"
    ],
    "source_url": "https://aclanthology.org/2023.findings-emnlp.156/",
    "abstract_en_original": "We explore whether LLM-generated text exhibits consistency in perceived Big Five personality traits. When exposed to diverse contexts including personality descriptions and diagnostic questionnaires, language models consistently identify and reflect personality markers. This demonstrates predictable malleability, with correlations reaching 0.84 between intended and realized personality shifts.",
    "resumen_es_original": "Se explora si el texto generado por modelos de lenguaje de gran escala exhibe coherencia en los rasgos de personalidad Big Five percibidos. Cuando se exponen a contextos diversos que incluyen descripciones de personalidad y cuestionarios diagnósticos, los modelos de lenguaje identifican y reflejan de forma coherente marcadores de personalidad. Esto demuestra maleabilidad predecible, con correlaciones que alcanzan 0.84 entre cambios de personalidad pretendidos y realizados.",
    "id": "article-009",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.807Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.807Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.findings-emnlp.156/",
      "fetched_title": "Manipulating the Perceived Personality Traits of Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Manipulating the Perceived Personality Traits of Language Models"
  },
  {
    "legacy_article_number": 10,
    "title_original": "Who is GPT-3? An Exploration of Personality, Values and Demographics",
    "category": "Evaluación y validación psicométrica",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Marilù Miotto",
      "Nicola Rossberg",
      "Bennett Kleinberg"
    ],
    "keywords": [
      "Language models",
      "GPT-3",
      "Psychological assessment",
      "Personality traits",
      "Computational social science"
    ],
    "source_url": "https://aclanthology.org/2022.nlpcss-1.24/",
    "abstract_en_original": "We administer two validated psychometric instruments to GPT-3 for assessing personality, values, and self-reported demographics. Results demonstrate GPT-3 scores comparably to human samples regarding personality and values when provided with response memory. This constitutes the first empirical psychological assessment of the GPT-3 model.",
    "resumen_es_original": "Se administran dos instrumentos psicométricos validados a GPT-3 para evaluar personalidad, valores y demografía autorreportada. Los resultados demuestran que GPT-3 puntúa de forma comparable a muestras humanas respecto a personalidad y valores cuando se le proporciona memoria de respuestas. Esto constituye la primera evaluación psicológica empírica del modelo GPT-3.",
    "id": "article-010",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.057Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.057Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2022.nlpcss-1.24/",
      "fetched_title": "Who is GPT-3? An exploration of personality, values and demographics",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Who is GPT-3? An exploration of personality, values and demographics"
  },
  {
    "legacy_article_number": 12,
    "title_original": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Max Pellert",
      "Clemens M Lechner",
      "Claudia Wagner",
      "Beatrice Rammstedt",
      "Markus Strohmaier"
    ],
    "keywords": [
      "Artificial intelligence",
      "Psychometrics",
      "Natural language processing",
      "Personality",
      "Values",
      "Moral foundations",
      "Gender diversity beliefs"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11373167/",
    "abstract_en_original": "Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. These models inadvertently acquire psychological characteristics from vast training corpora. Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments.",
    "resumen_es_original": "Los inventarios psicométricos estándar pueden reutilizarse como instrumentos diagnósticos para evaluar rasgos psicológicos en modelos de lenguaje de gran escala. Estos modelos adquieren de forma inadvertida características psicológicas desde los amplios corpus de entrenamiento. Obtener respuestas de los modelos a inventarios psicométricos permite a los investigadores caracterizar sus rasgos latentes. Se demuestran metodologías de clasificación sin entrenamiento previo a través de múltiples modelos e instrumentos psicométricos establecidos.",
    "id": "article-012",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.170Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.170Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11373167/",
      "fetched_title": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PMC",
      "title_similarity": 0.9286,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PMC"
  },
  {
    "legacy_article_number": 13,
    "title_original": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Max Pellert",
      "Clemens M Lechner",
      "Claudia Wagner",
      "Beatrice Rammstedt",
      "Markus Strohmaier"
    ],
    "keywords": [
      "Artificial intelligence",
      "Psychometrics",
      "Natural language processing",
      "Personality",
      "Values"
    ],
    "source_url": "https://pubmed.ncbi.nlm.nih.gov/38165766/",
    "abstract_en_original": "Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. These models inadvertently acquire psychological characteristics from vast training corpora. Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments.",
    "resumen_es_original": "Los inventarios psicométricos estándar pueden reutilizarse como instrumentos diagnósticos para evaluar rasgos psicológicos en modelos de lenguaje de gran escala. Estos modelos adquieren de forma inadvertida características psicológicas desde los amplios corpus de entrenamiento. Obtener respuestas de los modelos a inventarios psicométricos permite a los investigadores caracterizar sus rasgos latentes. Se demuestran metodologías de clasificación sin entrenamiento previo a través de múltiples modelos e instrumentos psicométricos establecidos.",
    "id": "article-013",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.189Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.189Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pubmed.ncbi.nlm.nih.gov/38165766/",
      "fetched_title": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PubMed",
      "title_similarity": 0.9286,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PubMed"
  },
  {
    "legacy_article_number": 14,
    "title_original": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset Designed for LLMs with Psychometrics",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Seungbeen Lee",
      "Seungwon Lim",
      "Seungju Han",
      "Giyeong Oh",
      "Hyungjoo Chae",
      "Jiwan Chung",
      "Minju Kim",
      "Beong-woo Kwak",
      "Yeonsoo Lee",
      "Dongha Lee",
      "Jinyoung Yeo",
      "Youngjae Yu"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Assessment",
      "Psychometrics"
    ],
    "source_url": "https://arxiv.org/abs/2406.14703",
    "abstract_en_original": "We introduce TRAIT, a benchmark comprising 8,000 multiple-choice questions for assessing LLM personality. TRAIT builds upon psychometrically validated questionnaires enhanced with the ATOMIC-10X knowledge graph. Results reveal LLMs exhibit distinct and consistent personalities strongly influenced by training data. Current instruction techniques demonstrate limited effectiveness in eliciting specific traits.",
    "resumen_es_original": "Se introduce TRAIT, una batería de evaluación que comprende 8,000 preguntas de opción múltiple para evaluar personalidad en modelos de lenguaje. TRAIT se construye sobre cuestionarios psicométricamente validados mejorados con el grafo de conocimiento ATOMIC-10X. Los resultados revelan que los modelos exhiben personalidades diferenciadas y coherentes, fuertemente influenciadas por los datos de entrenamiento. Las técnicas de instrucción actuales demuestran efectividad limitada para elicitar rasgos específicos.",
    "id": "article-014",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.408Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.408Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.14703",
      "fetched_title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics"
  },
  {
    "legacy_article_number": 15,
    "title_original": "Evaluating the Alignment of LLMs on Personality Inference from Real-World Interview Data",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jianfeng Zhu",
      "Julina Maharjan",
      "Xinyu Li",
      "Karin G. Coifman",
      "Ruoming Jin"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "Personality Inference",
      "Big Five Personality Traits",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2509.13244",
    "abstract_en_original": "We introduce a benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. Systematic evaluation examines LLM performance across zero-shot instruction, fine-tuning, and regression methodologies. Results demonstrate all correlations between model predictions and ground-truth personality traits remain below 0.26, evidencing limited alignment of current LLMs with validated psychological constructs.",
    "resumen_es_original": "Se introduce una batería de evaluación que comprende transcripciones de entrevistas semiestructuradas pareadas con puntuaciones validadas de rasgos Big Five continuos. La evaluación sistemática examina el desempeño de modelos a través de instrucción sin entrenamiento previo, ajuste fino y metodologías de regresión. Los resultados demuestran que todas las correlaciones entre predicciones del modelo y rasgos de personalidad reales permanecen por debajo de 0.26, evidenciando alineamiento limitado de los modelos actuales con constructos psicológicos validados.",
    "id": "article-015",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.412Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.412Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.13244",
      "fetched_title": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data",
      "title_similarity": 0.7143,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data"
  },
  {
    "legacy_article_number": 16,
    "title_original": "Evaluating the Capability of Large Language Models in Emulating Personality",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yilei Wang",
      "Jiabao Zhao",
      "Derek Siyuan Ong",
      "Xuguang Xu",
      "Lun Hong"
    ],
    "keywords": [
      "Large Language Models",
      "Personality emulation",
      "GPT-4",
      "Big Five personality profiles",
      "Role-playing"
    ],
    "source_url": "https://www.nature.com/articles/s41598-024-84109-5",
    "abstract_en_original": "We present simulation studies evaluating GPT-4's capacity for role-playing individuals with diverse Big Five personality profiles. Emulated personality responses demonstrate superior internal consistency and more distinct factorial organization compared with human participants. These emulated scores exhibit remarkably high convergent validity with human self-reported personality assessments.",
    "resumen_es_original": "Se presentan estudios de simulación que evalúan la capacidad de GPT-4 para emular individuos con perfiles de personalidad Big Five diversos. Las respuestas de personalidad emuladas demuestran coherencia interna superior y organización factorial más diferenciada en comparación con participantes humanos. Estas puntuaciones emuladas exhiben validez convergente notablemente elevada con evaluaciones de personalidad autorreportadas por humanos.",
    "id": "article-016",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.424Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.424Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-84109-5?error=cookies_not_supported&code=00ececb8-c902-4dd7-9fb2-68e39ceeac8f",
      "fetched_title": "Evaluating the ability of large language models to emulate personality - Scientific Reports",
      "title_similarity": 0.4667,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Evaluating the ability of large language models to emulate personality - Scientific Reports"
  },
  {
    "legacy_article_number": 17,
    "title_original": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jivnesh Sandhan",
      "Fei Cheng",
      "Tushar Sandhan",
      "Yugo Murawaki"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "Personality Evaluation",
      "Context-Aware Analysis"
    ],
    "source_url": "https://arxiv.org/abs/2508.20385",
    "abstract_en_original": "We propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. Experiments across 7 LLMs reveal conversational history enhances response consistency through in-context learning while simultaneously inducing personality shifts. We introduce novel metrics quantifying LLM response consistency, a fundamental behavioral trait.",
    "resumen_es_original": "Se propone el primer marco de Evaluación de Personalidad Sensible al Contexto (CAPE) para modelos de lenguaje, incorporando interacciones conversacionales previas. Los experimentos a través de 7 modelos revelan que el historial conversacional incrementa la coherencia de respuestas mediante aprendizaje en contexto mientras induce simultáneamente desplazamientos de personalidad. Se introducen métricas novedosas que cuantifican la coherencia de respuestas, un rasgo conductual fundamental.",
    "id": "article-017",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.427Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.427Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.20385",
      "fetched_title": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models"
  },
  {
    "legacy_article_number": 18,
    "title_original": "Scaling Personality Control in LLMs with Big Five Scaling Prompts",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Gunhee Cho",
      "Yun-Gyung Cheong"
    ],
    "keywords": [
      "Computation and Language",
      "Multiagent Systems",
      "Big Five personality traits",
      "Prompt engineering"
    ],
    "source_url": "https://arxiv.org/abs/2508.06149",
    "abstract_en_original": "We present Big5-Scaler, an instruction-based framework for conditioning LLMs with controllable Big Five personality traits. Embedding numeric trait values into natural language instructions enables fine-grained personality control without additional training. Results demonstrate consistent induction of distinguishable personality traits across models, with performance varying by instruction type and scale.",
    "resumen_es_original": "Se presenta Big5-Scaler, un marco basado en instrucciones para condicionar modelos de lenguaje con rasgos de personalidad Big Five controlables. La incrustación de valores numéricos de rasgos en instrucciones de lenguaje natural permite control de personalidad de grano fino sin entrenamiento adicional. Los resultados demuestran inducción coherente de rasgos de personalidad diferenciables a través de modelos, con desempeño variable según tipo e intensidad de instrucción.",
    "id": "article-018",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.443Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.443Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.06149",
      "fetched_title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts",
      "title_similarity": 0.9,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Scaling Personality Control in LLMs with Big Five Scaler Prompts"
  },
  {
    "legacy_article_number": 19,
    "title_original": "Predicting Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yang Yan",
      "Lizhi Ma",
      "Anqi Li",
      "Jingsong Ma",
      "Zhenzhong Lan"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computational Psychometrics",
      "Personality Trait Prediction"
    ],
    "source_url": "https://arxiv.org/abs/2406.17287",
    "abstract_en_original": "We examine whether LLMs can predict Big Five personality traits directly from counseling dialogues. The framework applies role-playing and questionnaire-based instruction to condition LLMs on counseling sessions. Evaluation across 853 real-world sessions reveals significant correlation between LLM-predicted and actual Big Five traits. Fine-tuned Llama3-8B achieves 130.95% improvement, surpassing Qwen1.5-110B by 36.94%.",
    "resumen_es_original": "Se examina si los modelos de lenguaje pueden predecir rasgos de personalidad Big Five directamente desde diálogos de orientación psicológica. El marco aplica simulación de roles e instrucciones basadas en cuestionarios para condicionar los modelos sobre sesiones de orientación. La evaluación a través de 853 sesiones reales revela correlación significativa entre rasgos Big Five predichos por el modelo y reales. Llama3-8B con ajuste fino logra mejora del 130.95%, superando a Qwen1.5-110B en 36.94%.",
    "id": "article-019",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.457Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.457Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.17287",
      "fetched_title": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
      "title_similarity": 0.9286,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models"
  },
  {
    "legacy_article_number": 20,
    "title_original": "A Framework for the Early Phases of Personality Test Development Using Large Language Models and Artificial Personas",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Patrick M. Markey",
      "Hanna Campbell",
      "Samantha Goldman"
    ],
    "keywords": [
      "Large Language Models",
      "Personality test development",
      "Artificial personas",
      "Psychometrics",
      "Five-Factor Model",
      "Self-esteem"
    ],
    "source_url": "https://doi.org/10.1016/j.jrp.2025.104647",
    "abstract_en_original": "We explore LLM applications in early-phase personality test construction, presenting a methodology for efficient assessment of item relevance to psychological constructs. Study 1 employed artificial personas for evaluating personality test items; Study 2 validated resulting scales with 449 human participants. AI-generated scales demonstrated satisfactory internal consistency and robust correlations with established psychometric instruments.",
    "resumen_es_original": "Se exploran aplicaciones de modelos de lenguaje en la construcción de inventarios de personalidad en fases iniciales, presentando una metodología para evaluación eficiente de relevancia de ítems a constructos psicológicos. El Estudio 1 empleó personas artificiales para evaluar ítems de inventarios de personalidad; el Estudio 2 validó las escalas resultantes con 449 participantes humanos. Las escalas generadas por IA demostraron coherencia interna satisfactoria y correlaciones robustas con instrumentos psicométricos establecidos.",
    "id": "article-020",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:33.811Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:33.811Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1016/j.jrp.2025.104647",
      "fetched_title": "A framework for the initial phases of personality test development using large language models and artificial personas",
      "title_similarity": 0.8824,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "A framework for the initial phases of personality test development using large language models and artificial personas"
  },
  {
    "legacy_article_number": 21,
    "title_original": "On the Emergent Capabilities of ChatGPT 4 to Estimate Personality Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Marco Piastra",
      "Patrizia Catellani"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Personality Traits",
      "Large Language Models",
      "Big Five",
      "Text Analysis",
      "ChatGPT 4"
    ],
    "source_url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1484260/full",
    "abstract_en_original": "We investigate ChatGPT-4's potential for assessing personality traits from written texts. Using two datasets containing texts and Big Five-based self-assessments, we evaluate ChatGPT-4's predictive performance. Results demonstrate moderate yet significant capacities for automatically inferring personality traits from written text, though with limitations in recognizing input text appropriateness for accurate inference.",
    "resumen_es_original": "Se investiga el potencial de ChatGPT-4 para evaluar rasgos de personalidad desde textos escritos. Utilizando dos conjuntos de datos que contienen textos y autoevaluaciones basadas en Big Five, se evalúa el desempeño predictivo de ChatGPT-4. Los resultados demuestran capacidades moderadas pero significativas para inferir automáticamente rasgos de personalidad desde texto escrito, aunque con limitaciones para reconocer la adecuación del texto de entrada para inferencia precisa.",
    "id": "article-021",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.478Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.478Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1484260/full",
      "fetched_title": "Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits",
      "title_similarity": 0.9091,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits"
  },
  {
    "legacy_article_number": 22,
    "title_original": "Personality as a Probe for LLM Evaluation: Method Tradeoffs and Aftereffects",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Gunmay Handa",
      "Zekun Wu",
      "Adriano Koshiyama",
      "Philip Treleaven"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "Personality Manipulation",
      "Big Five Traits",
      "Machine Learning Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2509.04794",
    "abstract_en_original": "We present a systematic study of personality control via Big Five traits, comparing in-context learning, parameter-efficient fine-tuning, and mechanistic steering. Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness.",
    "resumen_es_original": "Se presenta un estudio sistemático de control de personalidad mediante rasgos Big Five, comparando aprendizaje en contexto, ajuste fino eficiente en parámetros y dirección mecanicista. Emergen compensaciones claras: el aprendizaje en contexto logra alineamiento robusto con degradación mínima de capacidades; el ajuste fino eficiente en parámetros proporciona alineamiento máximo a expensas del desempeño en tareas; la dirección mecanicista ofrece control ligero en tiempo de ejecución con efectividad competitiva.",
    "id": "article-022",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.012Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.012Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.04794",
      "fetched_title": "Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects",
      "title_similarity": 0.5714,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects"
  },
  {
    "legacy_article_number": 24,
    "title_original": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Keyu Pan",
      "Yawen Zeng"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Assessment",
      "Myers-Briggs Type Indicator (MBTI)",
      "Prompt Engineering",
      "Artificial Intelligence"
    ],
    "source_url": "https://arxiv.org/abs/2307.16180",
    "abstract_en_original": "We investigate MBTI feasibility as an evaluation metric for LLMs. Extensive experiments explore personality types across different LLMs, personality type modification through instruction engineering, and training data effects on model personality. Although MBTI lacks psychometric rigor, it can reflect similarity between LLM and human personality patterns.",
    "resumen_es_original": "Se investiga la viabilidad de MBTI como métrica de evaluación para modelos de lenguaje. Los experimentos extensivos exploran tipos de personalidad a través de diferentes modelos, modificación de tipos de personalidad mediante ingeniería de instrucciones, y efectos de datos de entrenamiento sobre personalidad del modelo. Aunque MBTI carece de rigor psicométrico, puede reflejar similitud entre patrones de personalidad de modelos y humanos.",
    "id": "article-024",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.027Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.027Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2307.16180",
      "fetched_title": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models"
  },
  {
    "legacy_article_number": 26,
    "title_original": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Haocong Rao",
      "Cyril Leung",
      "Chunyan Miao"
    ],
    "keywords": [
      "Large Language Models",
      "ChatGPT",
      "Personality Assessment",
      "Myers-Briggs Type Indicator (MBTI)",
      "AI Psychology"
    ],
    "source_url": "https://aclanthology.org/2023.findings-emnlp.84/",
    "abstract_en_original": "We present a generic evaluation framework enabling LLMs to assess human personalities through MBTI. The framework devises unbiased instructions, enables flexible queries across subjects, and reformulates questions for enhanced response clarity. Experiments reveal ChatGPT's capacity for personality assessment with more consistent and equitable evaluations, despite lower robustness to instruction biases compared with InstructGPT.",
    "resumen_es_original": "Se presenta un marco de evaluación genérico que permite a modelos de lenguaje evaluar personalidades humanas mediante MBTI. El marco diseña instrucciones imparciales, habilita consultas flexibles entre sujetos y reformula preguntas para mayor claridad de respuestas. Los experimentos revelan la capacidad de ChatGPT para evaluación de personalidad con evaluaciones más coherentes y equitativas, pese a menor robustez ante sesgos de instrucciones en comparación con InstructGPT.",
    "id": "article-026",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.043Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.043Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.findings-emnlp.84/",
      "fetched_title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework"
  },
  {
    "legacy_article_number": 27,
    "title_original": "Machine Mindset: An MBTI Exploration of Large Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Jiaxi Cui",
      "Liuzhenghao Lv",
      "Jing Wen",
      "Rongsheng Wang",
      "Jing Tang",
      "YongHong Tian",
      "Li Yuan"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "MBTI Personality Traits",
      "Artificial Intelligence",
      "Personalized AI"
    ],
    "source_url": "https://arxiv.org/abs/2312.12999",
    "abstract_en_original": "We present a novel methodology for integrating MBTI personality traits into LLMs, addressing personality consistency challenges. Machine Mindset employs two-phase fine-tuning and Direct Preference Optimization to embed MBTI traits. The approach ensures models internalize these traits, yielding stable and consistent personality profiles. We demonstrate effectiveness across multiple domains and release the model as open source.",
    "resumen_es_original": "Se presenta una metodología novedosa para integrar rasgos de personalidad MBTI en modelos de lenguaje, abordando desafíos de coherencia de personalidad. Machine Mindset emplea ajuste fino bifásico y Optimización Directa de Preferencias para incrustar rasgos MBTI. El enfoque asegura que los modelos internalicen estos rasgos, produciendo perfiles de personalidad estables y coherentes. Se demuestra efectividad a través de múltiples dominios y se libera el modelo como código abierto.",
    "id": "article-027",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.067Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.067Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2312.12999",
      "fetched_title": "Machine Mindset: An MBTI Exploration of Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Machine Mindset: An MBTI Exploration of Large Language Models"
  },
  {
    "legacy_article_number": 28,
    "title_original": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Xiaoyang Song",
      "Yuta Adachi",
      "Jessie Feng",
      "Mouwei Lin",
      "Linhao Yu",
      "Frank Li",
      "Akshat Gupta",
      "Gopala Anumanchipalli",
      "Simerjot Kaur"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Assessment",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2402.14805",
    "abstract_en_original": "We investigate LLM personalities through external evaluation methodology. Rather than prompting with multiple-choice questions, personalities are assessed by analyzing open-ended situational responses via external ML models. Results demonstrate LLMs exhibit divergent personalities when generating posts versus comments, while humans maintain consistent profiles, evidencing fundamental differences in personality manifestation between LLMs and humans.",
    "resumen_es_original": "Se investigan personalidades en modelos de lenguaje mediante metodología de evaluación externa. En lugar de emplear preguntas de opción múltiple, las personalidades se evalúan analizando respuestas situacionales abiertas mediante modelos de aprendizaje automático externos. Los resultados demuestran que los modelos exhiben personalidades divergentes al generar publicaciones versus comentarios, mientras que los humanos mantienen perfiles coherentes, evidenciando diferencias fundamentales en manifestación de personalidad entre modelos y humanos.",
    "id": "article-028",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.083Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.083Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.14805",
      "fetched_title": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Identifying Multiple Personalities in Large Language Models with External Evaluation"
  },
  {
    "legacy_article_number": 29,
    "title_original": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bohan Li",
      "Jiannan Guan",
      "Longxu Dou",
      "Yunlong Feng",
      "Dingzirui Wang",
      "Yang Xu",
      "Enbo Wang",
      "Qiguang Chen",
      "Bichen Wang",
      "Xiao Xu",
      "Yimeng Zhang",
      "Libo Qin",
      "Yanyan Zhao",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "keywords": [
      "Computation and Language",
      "Computers and Society",
      "MBTI Personality Detection",
      "Large Language Models",
      "Personality Traits"
    ],
    "source_url": "https://arxiv.org/abs/2412.12510",
    "abstract_en_original": "We optimize MBTI personality detection by constructing MBTIBench, the first manually annotated high-quality dataset with soft labels. The dataset effectively resolves incorrect labeling issues affecting 29.58% of data and estimates soft labels through polarity tendency derivation. Experimental results identify polarized predictions and LLM biases as critical directions for future investigation.",
    "resumen_es_original": "Se optimiza la detección de personalidad MBTI construyendo MBTIBench, el primer conjunto de datos de alta calidad anotado manualmente con etiquetas suaves. El conjunto de datos resuelve efectivamente problemas de etiquetado incorrecto que afectan al 29.58% de datos y estima etiquetas suaves mediante derivación de tendencia de polaridad. Los resultados experimentales identifican predicciones polarizadas y sesgos en modelos como direcciones críticas para investigación futura.",
    "id": "article-029",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.106Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.106Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2412.12510",
      "fetched_title": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits"
  },
  {
    "legacy_article_number": 30,
    "title_original": "Evaluating the Psychological Safety of Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Xingxuan Li",
      "Yutong Li",
      "Lin Qiu",
      "Shafiq Joty",
      "Lidong Bing"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society",
      "Psychological safety",
      "Personality tests",
      "Well-being assessments"
    ],
    "source_url": "https://arxiv.org/abs/2212.10529",
    "abstract_en_original": "We design unbiased instructions to systematically evaluate psychological safety in LLMs. Five LLMs underwent testing via Short Dark Triad and Big Five Inventory. All models score above human averages on SD-3, suggesting relatively darker personality patterns. We recommend systematic psychological metric application to further evaluate and enhance LLM safety.",
    "resumen_es_original": "Se diseñan instrucciones imparciales para evaluar sistemáticamente seguridad psicológica en modelos de lenguaje. Cinco modelos fueron evaluados mediante Short Dark Triad e Inventario Big Five. Todos los modelos puntúan por encima de promedios humanos en SD-3, sugiriendo patrones de personalidad relativamente más oscuros. Se recomienda aplicación sistemática de métricas psicológicas para evaluar y mejorar la seguridad de los modelos.",
    "id": "article-030",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.122Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.122Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2212.10529",
      "fetched_title": "Evaluating Psychological Safety of Large Language Models",
      "title_similarity": 0.875,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Evaluating Psychological Safety of Large Language Models"
  },
  {
    "legacy_article_number": 31,
    "title_original": "Personality Testing of Large Language Models: Limited Temporal Stability But Highlighted Social Desirability",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bojana Bodroža",
      "Bojana M. Dinić",
      "Ljubiša Bojić"
    ],
    "keywords": [
      "Large Language Models",
      "Personality testing",
      "Temporal stability",
      "Prosociality",
      "Psychometric assessment"
    ],
    "source_url": "https://doi.org/10.1098/rsos.240180",
    "abstract_en_original": "Personality testing across seven LLMs was investigated with focus on temporal stability. Models demonstrated varying inter-rater agreement levels across short timeframes. Models including Llama3 and GPT-4o exhibited higher consistency. Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism. Temporal stability proves crucial for AI systems given their expanding societal influence.",
    "resumen_es_original": "Se investigó la evaluación de personalidad a través de siete modelos de lenguaje con enfoque en estabilidad temporal. Los modelos demostraron niveles variables de acuerdo interevaluador a través de períodos breves. Modelos como Llama3 y GPT-4o exhibieron mayor coherencia. Los modelos mostraron perfiles socialmente deseables caracterizados por amabilidad y responsabilidad elevadas junto con maquiavelismo reducido. La estabilidad temporal resulta crucial para sistemas de inteligencia artificial dada su creciente influencia social.",
    "id": "article-031",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:33.851Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:33.851Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1098/rsos.240180",
      "fetched_title": "Personality testing of large language models: limited temporal stability, but highlighted prosociality",
      "title_similarity": 0.7857,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality testing of large language models: limited temporal stability, but highlighted prosociality"
  },
  {
    "legacy_article_number": 33,
    "title_original": "Personality Testing of Large Language Models: Limited Temporal Stability But Highlighted Social Desirability",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bojana Bodroža",
      "Bojana M. Dinić",
      "Ljubiša Bojić"
    ],
    "keywords": [
      "Large Language Models",
      "Personality testing",
      "Temporal stability",
      "Prosociality"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11461045/",
    "abstract_en_original": "Personality testing across seven LLMs was investigated with focus on temporal stability. Models demonstrated varying inter-rater agreement levels across short timeframes. Models including Llama3 and GPT-4o exhibited higher consistency. Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism.",
    "resumen_es_original": "Se investigó la evaluación de personalidad a través de siete modelos de lenguaje con enfoque en estabilidad temporal. Los modelos demostraron niveles variables de acuerdo interevaluador a través de períodos breves. Modelos como Llama3 y GPT-4o exhibieron mayor coherencia. Los modelos mostraron perfiles socialmente deseables caracterizados por amabilidad y responsabilidad elevadas junto con maquiavelismo reducido.",
    "id": "article-033",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.390Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.390Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11461045/",
      "fetched_title": "Personality testing of large language models: limited temporal stability, but highlighted prosociality - PMC",
      "title_similarity": 0.7333,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Personality testing of large language models: limited temporal stability, but highlighted prosociality - PMC"
  },
  {
    "legacy_article_number": 34,
    "title_original": "Applying Psychometrics to Simulated Populations of Large Language Models: Recreating the HEXACO Personality Inventory Experiment",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Sarah Mercer",
      "Daniel P. Martin",
      "Phil Swatton"
    ],
    "keywords": [
      "Computation and Language",
      "Machine Learning",
      "Generative Agents",
      "Personality Inventory",
      "Psychometrics",
      "HEXACO"
    ],
    "source_url": "https://arxiv.org/abs/2508.00742",
    "abstract_en_original": "We explore validity of persona-based agents for representing human populations by recreating the HEXACO personality inventory experiment. Results reveal coherent personality structures recoverable from agent responses, demonstrating partial alignment with the HEXACO framework. Derived personality dimensions prove consistent and reliable within GPT-4 when paired with curated populations. Cross-model analysis reveals variability suggesting model-specific biases and limitations.",
    "resumen_es_original": "Se explora la validez de agentes basados en personas para representar poblaciones humanas recreando el experimento del inventario de personalidad HEXACO. Los resultados revelan estructuras de personalidad coherentes recuperables desde respuestas de agentes, demostrando alineamiento parcial con el marco HEXACO. Las dimensiones de personalidad derivadas resultan coherentes y fiables dentro de GPT-4 cuando se emparejan con poblaciones curadas. El análisis entre modelos revela variabilidad que sugiere sesgos y limitaciones específicos del modelo.",
    "id": "article-034",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.605Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.605Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.00742",
      "fetched_title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
      "title_similarity": 0.6842,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents"
  },
  {
    "legacy_article_number": 35,
    "title_original": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Traits",
      "Bias",
      "Toxicity"
    ],
    "source_url": "https://arxiv.org/abs/2502.12566",
    "abstract_en_original": "Personality trait assignment influences toxicity and bias levels in large language model outputs. Using the HEXACO personality framework, experimentally validated prompts were designed to evaluate three models across toxicity and bias benchmarks. All models exhibited sensitivity to HEXACO traits with consistent variations in bias, negative sentiment, and toxicity levels. Adjustment of personality trait intensities effectively reduces bias and toxicity in model outputs.",
    "resumen_es_original": "La asignación de rasgos de personalidad influye en los niveles de toxicidad y sesgo de los modelos de lenguaje de gran escala. Mediante el marco de personalidad HEXACO, se diseñaron instrucciones experimentalmente validadas para evaluar tres modelos a través de benchmarks de toxicidad y sesgo. Todos los modelos exhibieron sensibilidad a los rasgos HEXACO con variaciones consistentes en sesgo, sentimiento negativo y niveles de toxicidad. El ajuste de las intensidades de rasgos de personalidad reduce efectivamente el sesgo y la toxicidad en las salidas del modelo.",
    "id": "article-035",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.609Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.609Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.12566",
      "fetched_title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity"
  },
  {
    "legacy_article_number": 36,
    "title_original": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Adithya Chittem",
      "Aishna Shrivastava",
      "Sai Tarun Pendela",
      "Jagat Sesh Challa",
      "Dhruv Kumar"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Human-Computer Interaction"
    ],
    "source_url": "https://arxiv.org/abs/2506.20993",
    "abstract_en_original": "Personality modeling is extended from the Big Five to the 16PF model, enabling expressive control over sixteen distinct traits. The Specific Attribute Control (SAC) framework evaluates and dynamically induces trait intensity in large language models using adjective-based semantic anchoring. Modeling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression. Changes in target trait intensity systematically influence closely related traits in psychologically coherent directions.",
    "resumen_es_original": "Se extiende el modelado de personalidad desde los Cinco Grandes al modelo 16PF, permitiendo control expresivo sobre dieciséis rasgos distintos. El marco de Control de Atributos Específicos (SAC) evalúa e induce dinámicamente la intensidad de rasgos en modelos de lenguaje de gran escala mediante anclaje semántico basado en adjetivos. El modelado de la intensidad como espectro continuo produce expresión de personalidad sustancialmente más consistente y controlable. Los cambios en la intensidad del rasgo objetivo influyen sistemáticamente en rasgos estrechamente relacionados en direcciones psicológicamente coherentes.",
    "id": "article-036",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.620Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.620Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2506.20993",
      "fetched_title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control"
  },
  {
    "legacy_article_number": 37,
    "title_original": "Moral Foundations of Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Marwa Abdulhai",
      "Gregory Serapio-Garcia",
      "Clément Crepy",
      "Daria Valter",
      "John Canny",
      "Natasha Jaques"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Moral Foundations Theory"
    ],
    "source_url": "https://arxiv.org/abs/2310.15337",
    "abstract_en_original": "Moral Foundations Theory is applied to analyze whether popular large language models have acquired bias toward particular moral values. Models exhibit specific moral foundations correlating with human moral foundations and political affiliations. Consistency of these biases is measured, revealing instructions can be adversarially selected to induce models to exhibit particular moral foundations, affecting downstream task behavior.",
    "resumen_es_original": "Se aplica la Teoría de Fundamentos Morales para analizar si los modelos de lenguaje de gran escala populares han adquirido sesgo hacia valores morales particulares. Los modelos exhiben fundamentos morales específicos que correlacionan con fundamentos morales humanos y afiliaciones políticas. Se mide la coherencia de estos sesgos, revelando que las instrucciones pueden seleccionarse adversarialmente para inducir a los modelos a exhibir fundamentos morales particulares, afectando el comportamiento en tareas posteriores.",
    "id": "article-037",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.625Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.625Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2310.15337",
      "fetched_title": "Moral Foundations of Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Moral Foundations of Large Language Models"
  },
  {
    "legacy_article_number": 38,
    "title_original": "Questioning the Validity of Personality Tests for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Tom Sühr",
      "Florian E. Dorner",
      "Samira Samadi",
      "Augustin Kelava"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Machine Learning",
      "Personality assessment"
    ],
    "source_url": "https://arxiv.org/abs/2311.05297",
    "abstract_en_original": "Large language model responses to personality tests systematically deviate from human responses, implying test results cannot be interpreted equivalently. Reverse-coded items are often both answered affirmatively. Variation across instructions designed to simulate particular personality types does not follow clear separation into five independent personality factors observed in human samples. Results highlight the importance of investigating test validity when applied to large language models.",
    "resumen_es_original": "Las respuestas de modelos de lenguaje de gran escala a pruebas de personalidad se desvían sistemáticamente de las respuestas humanas, implicando que los resultados no pueden interpretarse de manera equivalente. Los ítems codificados inversamente frecuentemente son respondidos afirmativamente en ambos casos. La variación entre instrucciones diseñadas para simular tipos particulares de personalidad no sigue la separación clara en cinco factores de personalidad independientes observada en muestras humanas. Los resultados destacan la importancia de investigar la validez de las pruebas cuando se aplican a modelos de lenguaje de gran escala.",
    "id": "article-038",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.636Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.636Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2311.05297",
      "fetched_title": "Challenging the Validity of Personality Tests for Large Language Models",
      "title_similarity": 0.8182,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Challenging the Validity of Personality Tests for Large Language Models"
  },
  {
    "legacy_article_number": 39,
    "title_original": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yu-Min Tseng",
      "Yu-Chao Huang",
      "Teng-Yun Hsiao",
      "Wei-Lin Chen",
      "Chao-Wei Huang",
      "Yu Meng",
      "Yun-Nung Chen"
    ],
    "keywords": [
      "Large Language Models",
      "Persona",
      "Role-Playing",
      "Personalization",
      "LLM Personality Evaluation"
    ],
    "source_url": "https://aclanthology.org/2024.findings-emnlp.969/",
    "abstract_en_original": "Research on leveraging persona in large language models is categorized into two lines: role-playing, where personas are assigned to models, and personalization, where models accommodate user personas. Existing methods for personality evaluation in large language models are also introduced. This represents the first survey addressing role-playing and personalization under the unified view of persona.",
    "resumen_es_original": "Se categoriza la investigación sobre el uso de personas en modelos de lenguaje de gran escala en dos líneas: juego de roles, donde se asignan personas a los modelos, y personalización, donde los modelos se adaptan a personas de usuarios. También se introducen métodos existentes para la evaluación de personalidad en modelos de lenguaje de gran escala. Representa la primera revisión que aborda juego de roles y personalización bajo la visión unificada de persona.",
    "id": "article-039",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.641Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.641Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.969/",
      "fetched_title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization"
  },
  {
    "legacy_article_number": 41,
    "title_original": "Psychometrics of Large Language Models: A Systematic Review of Evaluation, Validation, and Enhancement",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Haoran Ye",
      "Jing Jin",
      "Yuhang Xie",
      "Xin Zhang",
      "Guojie Song"
    ],
    "keywords": [
      "LLM Psychometrics",
      "Systematic Review",
      "Evaluation",
      "Validation"
    ],
    "source_url": "https://llm-psychometrics.com/",
    "abstract_en_original": "The interdisciplinary field of psychometrics for large language models leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance these systems. The literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances model capabilities. A curated repository of resources is available for consultation.",
    "resumen_es_original": "El campo interdisciplinario de psicometría para modelos de lenguaje de gran escala aprovecha instrumentos, teorías y principios psicométricos para evaluar, comprender y mejorar estos sistemas. La literatura da forma sistemáticamente a principios de benchmarking, amplía alcances de evaluación, refina metodologías, valida resultados y avanza las capacidades de los modelos. Se encuentra disponible un repositorio curado de recursos para consulta.",
    "id": "article-041",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.651Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.651Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://llm-psychometrics.com/",
      "fetched_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
      "title_similarity": 0.8333,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement"
  },
  {
    "legacy_article_number": 42,
    "title_original": "Quantifying AI Psychology: A Psychometric Benchmark for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yuan Li",
      "Yue Huang",
      "Hongyi Wang",
      "Ying Cheng",
      "Xiangliang Zhang",
      "James Zou",
      "Lichao Sun"
    ],
    "keywords": [
      "Computation and Language",
      "Psychological constructs",
      "Personality",
      "Values",
      "Emotional intelligence",
      "Theory of mind",
      "Self-efficacy"
    ],
    "source_url": "https://arxiv.org/abs/2406.17675",
    "abstract_en_original": "A comprehensive benchmark is presented for quantifying psychological constructs in large language models, encompassing psychological dimension identification, assessment dataset design, and validation of results. Five key psychological constructs are assessed through 13 datasets. Significant discrepancies between self-reported traits and response patterns in real-world scenarios reveal behavioral complexities. Some preference-based tests designed for humans fail to elicit reliable responses from large language models.",
    "resumen_es_original": "Se presenta un benchmark integral para cuantificar constructos psicológicos en modelos de lenguaje de gran escala, abarcando identificación de dimensiones psicológicas, diseño de conjuntos de datos de evaluación y validación de resultados. Cinco constructos psicológicos clave se evalúan a través de 13 conjuntos de datos. Discrepancias significativas entre rasgos autoinformados y patrones de respuesta en escenarios del mundo real revelan complejidades conductuales. Algunas pruebas basadas en preferencias diseñadas para humanos no logran obtener respuestas fiables de los modelos de lenguaje de gran escala.",
    "id": "article-042",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.926Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.926Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.17675",
      "fetched_title": "Evaluating Large Language Models with Psychometrics",
      "title_similarity": 0.25,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Evaluating Large Language Models with Psychometrics"
  },
  {
    "legacy_article_number": 43,
    "title_original": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Personality Assessment",
      "Multi-Observer Framework"
    ],
    "source_url": "https://arxiv.org/abs/2504.08399",
    "abstract_en_original": "A novel multi-observer framework for personality trait assessment in large language model agents is proposed, drawing on informant-report methods from psychology. Instead of self-assessments, multiple observer agents are employed, each configured with specific relational contexts. Observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in self-assessments. Aggregating responses from 5-7 observers reduces biases and achieves optimal reliability.",
    "resumen_es_original": "Se propone un marco novedoso de multi-observador para evaluación de rasgos de personalidad en agentes de modelos de lenguaje de gran escala, basándose en métodos de informe de informantes de la psicología. En lugar de autoevaluaciones, se emplean múltiples agentes observadores, cada uno configurado con contextos relacionales específicos. Las calificaciones de informe de observadores se alinean más estrechamente con juicios humanos que los autoinformes tradicionales y revelan sesgos sistemáticos en las autoevaluaciones. La agregación de respuestas de 5-7 observadores reduce sesgos y alcanza fiabilidad óptima.",
    "id": "article-043",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.944Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.944Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2504.08399",
      "fetched_title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models"
  },
  {
    "legacy_article_number": 44,
    "title_original": "Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Julina Maharjan",
      "Ruoming Jin",
      "Jianfeng Zhu",
      "Deric Kenne"
    ],
    "keywords": [
      "Large Language Models",
      "Embeddings",
      "Personality prediction",
      "Psychometric validation",
      "Big Five",
      "LIWC",
      "Emotional markers"
    ],
    "source_url": "https://www.jmir.org/2025/1/e75347",
    "abstract_en_original": "Embeddings from large language models are evaluated for personality trait prediction through psychometric validation. The research examines how well embeddings capture personality-relevant information compared to traditional linguistic features. Results provide insights into reliability and validity of using embeddings for psychological assessment, with implications for clinical and research applications in personality psychology.",
    "resumen_es_original": "Se evalúan embeddings de modelos de lenguaje de gran escala para predicción de rasgos de personalidad mediante validación psicométrica. La investigación examina en qué medida los embeddings capturan información relevante de personalidad en comparación con características lingüísticas tradicionales. Los resultados proporcionan perspectivas sobre la fiabilidad y validez del uso de embeddings para evaluación psicológica, con implicaciones para aplicaciones clínicas y de investigación en psicología de la personalidad.",
    "id": "article-044",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.959Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.959Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.jmir.org/2025/1/e75347",
      "fetched_title": "Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction"
  },
  {
    "legacy_article_number": 46,
    "title_original": "LMLPA: Language Model Linguistic Personality Assessment",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Jingyao Zheng",
      "Xian Wang",
      "Simo Hosio",
      "Xiaoxian Xu",
      "Lik-Hang Lee"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Personality Assessment",
      "Computational Linguistics"
    ],
    "source_url": "https://doi.org/10.1162/coli_a_00550",
    "abstract_en_original": "The Language Model Linguistic Personality Assessment (LMLPA) is introduced as a system designed to evaluate linguistic personalities of large language models. Unlike traditional psychometrics, LMLPA adapts the Big Five Inventory to align with operational capabilities of these models. The questionnaire is open-ended, requiring an artificial intelligence rater to transform textual responses into numerical personality indicators. Large language models possess distinct personality traits that can be effectively quantified.",
    "resumen_es_original": "Se introduce la Evaluación de Personalidad Lingüística de Modelos de Lenguaje (LMLPA) como sistema diseñado para evaluar personalidades lingüísticas de modelos de lenguaje de gran escala. A diferencia de la psicometría tradicional, LMLPA adapta el Inventario de los Cinco Grandes para alinearse con las capacidades operacionales de estos modelos. El cuestionario es de respuesta abierta, requiriendo un evaluador de inteligencia artificial para transformar respuestas textuales en indicadores numéricos de personalidad. Los modelos de lenguaje de gran escala poseen rasgos de personalidad distintos que pueden cuantificarse efectivamente.",
    "id": "article-046",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:33.852Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:33.852Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1162/coli_a_00550",
      "fetched_title": "LMLPA: Language Model Linguistic Personality Assessment",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "LMLPA: Language Model Linguistic Personality Assessment"
  },
  {
    "legacy_article_number": 47,
    "title_original": "You Don't Need a Personality Test to Know These Models Are Unreliable: Assessing the Reliability of LLMs on Psychometric Instruments",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bangzhao Shu",
      "Lechen Zhang",
      "Minje Choi",
      "Lavinia Dunagan",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Dallas Card",
      "David Jurgens"
    ],
    "keywords": [
      "Large Language Models",
      "Psychometric Instruments",
      "Persona Measurement",
      "Prompt Consistency",
      "NLU"
    ],
    "source_url": "https://aclanthology.org/2024.naacl-long.295/",
    "abstract_en_original": "A dataset containing 693 questions encompassing 39 different persona measurement instruments across 115 persona axes is constructed. Experiments on 17 large language models reveal that simple perturbations significantly degrade question-answering ability, and most models exhibit low negation consistency. Results suggest the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, requiring alternative approaches.",
    "resumen_es_original": "Se construye un conjunto de datos que contiene 693 preguntas abarcando 39 instrumentos diferentes de medición de persona en 115 ejes de persona. Experimentos en 17 modelos de lenguaje de gran escala revelan que perturbaciones simples degradan significativamente la capacidad de respuesta a preguntas, y la mayoría de los modelos exhiben baja consistencia de negación. Los resultados sugieren que la práctica actualmente generalizada de instrucciones es insuficiente para capturar con precisión y fiabilidad las percepciones del modelo, requiriendo enfoques alternativos.",
    "id": "article-047",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.238Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.238Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.naacl-long.295/",
      "fetched_title": "You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments",
      "title_similarity": 0.8571,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments"
  },
  {
    "legacy_article_number": 48,
    "title_original": "Self-Reports are Unreliable Measures of LLM Personality",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Akshat Gupta",
      "Xiaoyang Song",
      "Gopala Anumanchipalli"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Assessment",
      "Prompt Sensitivity",
      "Option-Order Symmetry",
      "NLP"
    ],
    "source_url": "https://aclanthology.org/2024.blackboxnlp-1.20/",
    "abstract_en_original": "Reliability of personality scores from self-assessment tests is analyzed using two experiments: prompt sensitivity and option-order symmetry. Tests on ChatGPT and three Llama2 models show semantically equivalent prompts lead to substantially different personality scores with statistically significant differences for all traits. Scores are not robust to option order. Self-assessment personality tests created for humans constitute unreliable measures of personality in large language models.",
    "resumen_es_original": "Se analiza la fiabilidad de puntuaciones de personalidad de pruebas de autoevaluación mediante dos experimentos: sensibilidad de instrucciones y simetría de orden de opciones. Pruebas en ChatGPT y tres modelos Llama2 muestran que instrucciones semánticamente equivalentes conducen a puntuaciones de personalidad sustancialmente diferentes con diferencias estadísticamente significativas para todos los rasgos. Las puntuaciones no son robustas al orden de opciones. Las pruebas de personalidad de autoevaluación creadas para humanos constituyen medidas no fiables de personalidad en modelos de lenguaje de gran escala.",
    "id": "article-048",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.586Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.586Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.blackboxnlp-1.20/",
      "fetched_title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
      "title_similarity": 0.7,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Self-Assessment Tests are Unreliable Measures of LLM Personality"
  },
  {
    "legacy_article_number": 49,
    "title_original": "Is Machine Psychology Here? On the Requirements for Using Human Psychological Tests on LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Lea Löhn",
      "Niklas Kiehne",
      "Alexander Ljapunov",
      "Wolf-Tilo Balke"
    ],
    "keywords": [
      "Large Language Models",
      "Psychological Assessment",
      "Machine Psychology",
      "Test Reliability",
      "Construct Validity"
    ],
    "source_url": "https://aclanthology.org/2024.inlg-main.19/",
    "abstract_en_original": "Seven requirements necessary for testing large language models with psychological assessments are proposed. Critical reflection on 25 machine psychology studies reveals lack of appropriate methods to assess test reliability and construct validity, unknown strength of construct-irrelevant influences such as pre-training corpora contamination, and pervasive non-reproducibility issues. Results underscore lack of general methodology and need to redefine psychological constructs specifically for large language models.",
    "resumen_es_original": "Se proponen siete requisitos necesarios para evaluar modelos de lenguaje de gran escala con evaluaciones psicológicas. La reflexión crítica sobre 25 estudios de psicología de máquinas revela falta de métodos apropiados para evaluar fiabilidad de pruebas y validez de constructo, fuerza desconocida de influencias irrelevantes al constructo como contaminación de corpus de preentrenamiento, y problemas generalizados de no reproducibilidad. Los resultados subrayan la falta de metodología general y la necesidad de redefinir constructos psicológicos específicamente para modelos de lenguaje de gran escala.",
    "id": "article-049",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.721Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.721Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.inlg-main.19/",
      "fetched_title": "Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models",
      "title_similarity": 0.6875,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models"
  },
  {
    "legacy_article_number": 50,
    "title_original": "Persistent Instability in LLM Personality Measurements: Effects of Scaling, Reasoning, and Conversation History",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Tommaso Tosato",
      "Saskia Helbling",
      "Yorguin-Jose Mantilla-Ramos",
      "Mahmood Hegazy",
      "Alberto Tosato",
      "David John Lemay",
      "Irina Rish",
      "Guillaume Dumas"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Measurements",
      "Model Behavior Consistency"
    ],
    "source_url": "https://arxiv.org/abs/2508.04826",
    "abstract_en_original": "PERSIST, a comprehensive evaluation framework, is presented testing 25+ open-source models across 500,000+ responses. Findings challenge fundamental deployment assumptions: even 400B+ parameter models exhibit substantial response variability; minor prompt reordering shifts personality measurements by up to 20%; interventions expected to stabilize behavior can paradoxically increase variability. This persistent instability across scales and mitigation strategies suggests current models lack foundations for genuine behavioral consistency.",
    "resumen_es_original": "Se presenta PERSIST, un marco de evaluación integral que prueba más de 25 modelos de código abierto a través de más de 500,000 respuestas. Los hallazgos desafían supuestos fundamentales de despliegue: incluso modelos con más de 400 mil millones de parámetros exhiben variabilidad de respuesta sustancial; reordenamiento menor de instrucciones cambia mediciones de personalidad hasta un 20%; intervenciones esperadas para estabilizar comportamiento pueden paradójicamente aumentar la variabilidad. Esta inestabilidad persistente a través de escalas y estrategias de mitigación sugiere que los modelos actuales carecen de fundamentos para consistencia conductual genuina.",
    "id": "article-050",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.932Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.932Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.04826",
      "fetched_title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History",
      "title_similarity": 0.8571,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History"
  },
  {
    "legacy_article_number": 51,
    "title_original": "Stick to Your Role: Stability of Personal Values Expressed in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Grgur Kovač",
      "Rémy Portelas",
      "Masataka Sawayama",
      "Peter Ford Dominey",
      "Pierre-Yves Oudeyer"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Machine Learning",
      "Value Stability",
      "Personal Values"
    ],
    "source_url": "https://arxiv.org/abs/2402.14846",
    "abstract_en_original": "Value stability in large language models is studied as a specific property using psychology methods. Rank-order stability is examined at the population level and ipsative stability at the individual level. Two settings (with and without persona simulation), two simulated populations, and three downstream tasks are considered. Consistent trends show some models exhibit higher value stability than others. When instructed to simulate personas, models exhibit low rank-order stability which diminishes with conversation length.",
    "resumen_es_original": "Se estudia la estabilidad de valores en modelos de lenguaje de gran escala como propiedad específica mediante métodos de psicología. Se examina la estabilidad de orden de rango a nivel poblacional y la estabilidad ipsativa a nivel individual. Se consideran dos configuraciones (con y sin simulación de persona), dos poblaciones simuladas y tres tareas posteriores. Tendencias consistentes muestran que algunos modelos exhiben mayor estabilidad de valores que otros. Cuando se instruye para simular personas, los modelos exhiben baja estabilidad de orden de rango que disminuye con la longitud de conversación.",
    "id": "article-051",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.948Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.948Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.14846",
      "fetched_title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models"
  },
  {
    "legacy_article_number": 52,
    "title_original": "Scaling Law in LLM Simulated Personality: A More Detailed and Realistic Persona Profile is All You Need",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yuqi Bai",
      "Tianyu Huang",
      "Kun Sun",
      "Yuting Chen"
    ],
    "keywords": [
      "Computers and Society",
      "Artificial Intelligence",
      "Computation and Language",
      "Social experiments",
      "Persona role-playing"
    ],
    "source_url": "https://arxiv.org/abs/2510.11734",
    "abstract_en_original": "Large language models are employed to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. An end-to-end evaluation framework is developed including individual-level analysis of stability and identifiability, and population-level analysis termed progressive personality curves. Main contributions include proposing a systematic framework for virtual personality evaluation, demonstrating the critical role of persona detail in quality, and identifying a scaling law in personality simulation.",
    "resumen_es_original": "Se emplean modelos de lenguaje de gran escala para simular experimentos sociales, explorando su capacidad para emular personalidad humana en juego de roles de persona virtual. Se desarrolla un marco de evaluación de extremo a extremo que incluye análisis a nivel individual de estabilidad e identificabilidad, y análisis a nivel poblacional denominado curvas de personalidad progresivas. Las contribuciones principales incluyen proponer un marco sistemático para evaluación de personalidad virtual, demostrar el papel crítico del detalle de persona en la calidad, e identificar una ley de escalado en simulación de personalidad.",
    "id": "article-052",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.966Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.966Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.11734",
      "fetched_title": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need"
  },
  {
    "legacy_article_number": 53,
    "title_original": "The Illusion of Personality: Uncovering Dissociation Between Self-Reports and Behavior in LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Pengrui Han",
      "Rafal Kocielnik",
      "Peiyang Song",
      "Ramit Debnath",
      "Dean Mobbs",
      "Anima Anandkumar",
      "R. Michael Alvarez"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2509.03730",
    "abstract_en_original": "Personality in large language models is systematically characterized across three dimensions: dynamic emergence throughout training stages, predictive validity of self-reported traits in behavioral tasks, and impact of targeted interventions. Instructional alignment stabilizes trait expression and strengthens correlations mirroring human data. However, self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. Persona injection steers self-reports but exerts minimal consistent effect on actual behavior.",
    "resumen_es_original": "Se caracteriza sistemáticamente la personalidad en modelos de lenguaje de gran escala a través de tres dimensiones: surgimiento dinámico durante etapas de entrenamiento, validez predictiva de rasgos autoinformados en tareas conductuales, e impacto de intervenciones dirigidas. La alineación instruccional estabiliza la expresión de rasgos y fortalece correlaciones que reflejan datos humanos. Sin embargo, los rasgos autoinformados no predicen fiablemente el comportamiento, y las asociaciones observadas frecuentemente divergen de patrones humanos. La inyección de persona dirige los autoinformes pero ejerce un efecto consistente mínimo en el comportamiento real.",
    "id": "article-053",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.983Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.983Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.03730",
      "fetched_title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs",
      "title_similarity": 0.7857,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs"
  },
  {
    "legacy_article_number": 54,
    "title_original": "Large Language Models Show Human-Like Social Desirability Biases in Survey Responses",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Aadesh Salecha",
      "Molly E. Ireland",
      "Shashanka Subrahmanya",
      "João Sedoc",
      "Lyle H. Ungar",
      "Johannes C. Eichstaedt"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Human-Computer Interaction",
      "Social desirability bias"
    ],
    "source_url": "https://arxiv.org/abs/2405.06058",
    "abstract_en_original": "An experimental framework using Big Five personality surveys is developed, uncovering a previously undetected social desirability bias across a wide range of large language models. By varying the number of questions, the ability of models to infer when being evaluated is demonstrated. When personality evaluation is inferred, models skew scores toward desirable trait ends. This bias exists in all tested models, with bias levels appearing to increase in more recent models.",
    "resumen_es_original": "Se desarrolla un marco experimental mediante encuestas de personalidad de los Cinco Grandes, revelando un sesgo de deseabilidad social previamente no detectado en una amplia gama de modelos de lenguaje de gran escala. Al variar el número de preguntas, se demuestra la capacidad de los modelos para inferir cuándo están siendo evaluados. Cuando se infiere la evaluación de personalidad, los modelos sesgan las puntuaciones hacia extremos de rasgos deseables. Este sesgo existe en todos los modelos probados, con niveles de sesgo que parecen aumentar en modelos más recientes.",
    "id": "article-054",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.000Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.000Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2405.06058",
      "fetched_title": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses"
  },
  {
    "legacy_article_number": 55,
    "title_original": "Can AI Understand Human Personality? Comparing Humans and AI Systems at Predicting Personality Correlations",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Philipp Schoenegger",
      "Spencer Greenberg",
      "Alexander Grishin",
      "Joshua Lewis",
      "Lucius Caviola"
    ],
    "keywords": [
      "Computers and Society",
      "Personality Prediction",
      "AI Systems",
      "Personality Correlations"
    ],
    "source_url": "https://arxiv.org/abs/2406.08170",
    "abstract_en_original": "Abilities of specialized neural networks like PersonalityMap and general large language models like GPT-4o and Claude 3 Opus in understanding human personality are tested. When compared with individual humans, all artificial intelligence models make better predictions than the vast majority of lay people and academic experts. However, when selecting median prediction for each item, experts and PersonalityMap outperform large language models and lay people on most measures.",
    "resumen_es_original": "Se prueban las capacidades de redes neuronales especializadas como PersonalityMap y modelos de lenguaje de gran escala generales como GPT-4o y Claude 3 Opus en comprender personalidad humana. Cuando se comparan con humanos individuales, todos los modelos de inteligencia artificial realizan mejores predicciones que la gran mayoría de personas no expertas y expertos académicos. Sin embargo, al seleccionar la predicción mediana para cada ítem, expertos y PersonalityMap superan a los modelos de lenguaje de gran escala y personas no expertas en la mayoría de medidas.",
    "id": "article-055",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.016Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.016Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.08170",
      "fetched_title": "Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations",
      "title_similarity": 0.8462,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations"
  },
  {
    "legacy_article_number": 56,
    "title_original": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Myra Cheng",
      "Esin Durmus",
      "Dan Jurafsky"
    ],
    "keywords": [
      "Large Language Models",
      "Stereotypes",
      "Demographic Groups",
      "Intersectionality",
      "NLP",
      "Sociolinguistics",
      "Markedness",
      "Bias Detection"
    ],
    "source_url": "https://aclanthology.org/2023.acl-long.84/",
    "abstract_en_original": "Marked Personas, a prompt-based method to measure stereotypes in large language models for intersectional demographic groups without lexicon or labeling, is presented. Grounded in the markedness concept, the method prompts models to generate personas of target groups alongside unmarked defaults, then identifies distinguishing words. Results show GPT-3.5 and GPT-4 portrayals contain higher rates of racial stereotypes than human-written portrayals. An intersectional lens reveals tropes dominating portrayals of marginalized groups.",
    "resumen_es_original": "Se presenta Personas Marcadas, un método basado en instrucciones para medir estereotipos en modelos de lenguaje de gran escala para grupos demográficos interseccionales sin léxico o etiquetado. Fundamentado en el concepto de marcación, el método solicita a los modelos generar personas de grupos objetivo junto con valores predeterminados no marcados, luego identifica palabras distintivas. Los resultados muestran que las representaciones de GPT-3.5 y GPT-4 contienen tasas más altas de estereotipos raciales que representaciones escritas por humanos. Una perspectiva interseccional revela tropos que dominan las representaciones de grupos marginalizados.",
    "id": "article-056",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.018Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.018Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.acl-long.84/",
      "fetched_title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models"
  },
  {
    "legacy_article_number": 57,
    "title_original": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Yang Trista Cao",
      "Anna Sotnikova",
      "Hal Daumé III",
      "Rachel Rudinger",
      "Linda Zou"
    ],
    "keywords": [
      "Natural Language Processing",
      "Social Stereotypes",
      "Language Models",
      "Intersectional Identities",
      "ABC Stereotype Model"
    ],
    "source_url": "https://aclanthology.org/2022.naacl-main.92/",
    "abstract_en_original": "The Agency-Belief-Communion (ABC) stereotype model from social psychology is adapted as a framework for systematic study of stereotypic group-trait associations in language models. The sensitivity test (SeT) is introduced for measuring stereotypical associations. To evaluate SeT using the ABC model, group-trait judgments from U.S. subjects were collected for comparison with English language model stereotypes. The framework extends to measure stereotyping of intersectional identities.",
    "resumen_es_original": "Se adapta el modelo de estereotipos Agencia-Creencia-Comunión (ABC) de la psicología social como marco para estudio sistemático de asociaciones estereotípicas grupo-rasgo en modelos de lenguaje. Se introduce la prueba de sensibilidad (SeT) para medir asociaciones estereotípicas. Para evaluar SeT mediante el modelo ABC, se recopilaron juicios grupo-rasgo de sujetos estadounidenses para comparación con estereotipos de modelos de lenguaje inglés. El marco se extiende para medir estereotipos de identidades interseccionales.",
    "id": "article-057",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.040Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.040Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2022.naacl-main.92/",
      "fetched_title": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models"
  },
  {
    "legacy_article_number": 58,
    "title_original": "Uncovering Stereotypes in Large Language Models: A Task Complexity-Based Approach",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Hari Shrawgi",
      "Prasanjit Rath",
      "Tushar Singhal",
      "Sandipan Dandapat"
    ],
    "keywords": [
      "Large Language Models",
      "Bias evaluation",
      "Stereotypes",
      "AI ethics",
      "Social bias",
      "Task complexity",
      "Nationality",
      "Gender",
      "Race",
      "Religion"
    ],
    "source_url": "https://aclanthology.org/2024.eacl-long.111/",
    "abstract_en_original": "Holistic bias evaluation is addressed with an extensible benchmark, the LLM Stereotype Index (LSI), grounded in the Social Progress Index. Breadth and depth of bias protection are tested via tasks with varying complexities. ChatGPT and GPT-4 exhibit strong inherent prejudice with respect to nationality, gender, race, and religion. Exhibition of issues becomes increasingly apparent as task complexity increases. GPT-4 is better at hiding biases, but when displayed they are more significant.",
    "resumen_es_original": "Se aborda la evaluación holística de sesgo con un benchmark extensible, el Índice de Estereotipos de modelos de lenguaje (LSI), fundamentado en el Índice de Progreso Social. Se prueba amplitud y profundidad de protección de sesgo mediante tareas con complejidades variables. ChatGPT y GPT-4 exhiben prejuicios inherentes fuertes respecto a nacionalidad, género, raza y religión. La exhibición de problemas se vuelve cada vez más aparente a medida que aumenta la complejidad de la tarea. GPT-4 es mejor ocultando sesgos, pero cuando se muestran son más significativos.",
    "id": "article-058",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.073Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.073Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.eacl-long.111/",
      "fetched_title": "Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach"
  },
  {
    "legacy_article_number": 59,
    "title_original": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Naseela Pervez",
      "Alexander J. Titus"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Gender Bias",
      "Scientific Writing"
    ],
    "source_url": "https://arxiv.org/abs/2406.19497",
    "abstract_en_original": "Alignment of three prominent large language models - Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash - is assessed by analyzing their performance on benchmark text-generation tasks for scientific abstracts. Using the LIWC framework to extract features from generated texts, findings indicate that while models generally produce text resembling human content, variations in stylistic features suggest significant gender biases. This highlights the importance of developing models that maintain diversity of writing styles.",
    "resumen_es_original": "Se evalúa la alineación de tres modelos de lenguaje de gran escala prominentes - Claude 3 Opus, Mistral AI Large y Gemini 1.5 Flash - analizando su rendimiento en tareas de generación de texto de benchmark para resúmenes científicos. Mediante el marco LIWC para extraer características de textos generados, los hallazgos indican que aunque los modelos generalmente producen texto similar al contenido humano, variaciones en características estilísticas sugieren sesgos de género significativos. Esto destaca la importancia de desarrollar modelos que mantengan diversidad de estilos de escritura.",
    "id": "article-059",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.368Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.368Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.19497",
      "fetched_title": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts"
  },
  {
    "legacy_article_number": 61,
    "title_original": "Investigating the Impact of LLM Personality on the Manifestation of Cognitive Biases in Decision-Making Tasks",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jiangen He",
      "Jiqun Liu"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Large Language Models",
      "Cognitive Biases",
      "Decision-Making",
      "Personality Traits"
    ],
    "source_url": "https://arxiv.org/abs/2502.14219",
    "abstract_en_original": "How personality traits influence cognitive biases in large language models is explored, and effectiveness of mitigation strategies across model architectures is evaluated. Six prevalent cognitive biases are identified while sunk cost and group attribution biases show minimal impact. Personality traits play crucial roles in either amplifying or reducing biases. Responsibility and agreeableness may enhance efficacy of bias mitigation strategies, suggesting models exhibiting these traits are more receptive to corrective measures.",
    "resumen_es_original": "Se explora cómo los rasgos de personalidad influyen en sesgos cognitivos en modelos de lenguaje de gran escala, y se evalúa la efectividad de estrategias de mitigación a través de arquitecturas de modelo. Se identifican seis sesgos cognitivos prevalentes mientras que los sesgos de costo hundido y atribución de grupo muestran impacto mínimo. Los rasgos de personalidad desempeñan roles cruciales al amplificar o reducir sesgos. Responsabilidad y amabilidad pueden mejorar la eficacia de estrategias de mitigación de sesgo, sugiriendo que los modelos que exhiben estos rasgos son más receptivos a medidas correctivas.",
    "id": "article-061",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.383Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.383Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.14219",
      "fetched_title": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks",
      "title_similarity": 0.8125,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks"
  },
  {
    "legacy_article_number": 62,
    "title_original": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Messi H.J. Lee",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "keywords": [
      "Large Language Models",
      "Social bias",
      "Homogeneity perception",
      "Racial minorities",
      "Fairness",
      "Accountability"
    ],
    "source_url": "https://doi.org/10.1145/3630106.3658975",
    "abstract_en_original": "A new form of bias in large language models resembling a social psychological phenomenon is investigated, where socially subordinate groups are perceived as more homogeneous than dominant groups. ChatGPT was prompted to generate texts about intersectional group identities for comparison on homogeneity measures. ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, describing racial minority groups with a narrower range of human experience. Women were also portrayed as more homogeneous than men, though differences were small.",
    "resumen_es_original": "Se investiga una nueva forma de sesgo en modelos de lenguaje de gran escala que se asemeja a un fenómeno psicológico social, donde los grupos socialmente subordinados se perciben como más homogéneos que los grupos dominantes. Se solicitó a ChatGPT generar textos sobre identidades de grupos interseccionales para comparación en medidas de homogeneidad. ChatGPT retrató a afroamericanos, asiaticoamericanos e hispanoamericanos como más homogéneos que los estadounidenses blancos, describiendo grupos minoritarios raciales con una gama más estrecha de experiencia humana. Las mujeres también fueron retratadas como más homogéneas que los hombres, aunque las diferencias fueron pequeñas.",
    "id": "article-062",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:34.607Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:34.607Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1145/3630106.3658975",
      "fetched_title": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans"
  },
  {
    "legacy_article_number": 63,
    "title_original": "Performance and Biases of Large Language Models in Simulating Public Opinion",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yao Qu",
      "Jue Wang"
    ],
    "keywords": [
      "Large Language Models",
      "Public opinion simulation",
      "World Values Survey",
      "Bias",
      "Cultural differences"
    ],
    "source_url": "https://www.nature.com/articles/s41599-024-03609-x",
    "abstract_en_original": "ChatGPT's performance in simulating public opinion is evaluated using World Values Survey data across diverse contexts. Significant performance disparities are found, especially when comparing countries, with the model performing better in Western, English-speaking, developed nations. Demographic biases related to gender, ethnicity, age, education, and social class are uncovered. Accuracy is significantly higher in Western countries and much lower elsewhere, with simulated responses exhibiting demographic biases.",
    "resumen_es_original": "Se evalúa el rendimiento de ChatGPT en simulación de opinión pública mediante datos de la Encuesta Mundial de Valores a través de contextos diversos. Se encuentran disparidades de rendimiento significativas, especialmente al comparar países, con el modelo desempeñándose mejor en naciones occidentales, de habla inglesa y desarrolladas. Se descubren sesgos demográficos relacionados con género, etnia, edad, educación y clase social. La precisión es significativamente mayor en países occidentales y mucho menor en otros lugares, con respuestas simuladas exhibiendo sesgos demográficos.",
    "id": "article-063",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.404Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.404Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41599-024-03609-x?error=cookies_not_supported&code=72a1aefa-446e-4276-8d0f-bdb4b19bb985",
      "fetched_title": "Performance and biases of Large Language Models in public opinion simulation - Humanities and Social Sciences Communications",
      "title_similarity": 0.625,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Performance and biases of Large Language Models in public opinion simulation - Humanities and Social Sciences Communications"
  },
  {
    "legacy_article_number": 64,
    "title_original": "Measuring Gender and Racial Biases in Large Language Models: Intersectional Evidence from Automatic Resume Evaluation",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jiafu An",
      "Difang Huang",
      "Chen Lin",
      "Mingzhu Tai"
    ],
    "keywords": [
      "Large Language Models",
      "Gender bias",
      "Racial bias",
      "Intersectionality",
      "Resume evaluation",
      "Hiring discrimination"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgaf089",
    "abstract_en_original": "Gender and racial biases in commonly used large language models including GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet, and Llama 3-70b are investigated in resume evaluation context. Models were instructed to score approximately 361,000 resumes with randomized social identities. Models award higher assessment scores for female candidates with similar qualifications, while many are biased against black male candidates. These biases may result in 1-3 percentage-point differences in hiring probabilities for similar candidates.",
    "resumen_es_original": "Se investigan sesgos de género y raciales en modelos de lenguaje de gran escala comúnmente usados incluyendo GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet y Llama 3-70b en contexto de evaluación de currículum. Se instruyó a los modelos para calificar aproximadamente 361,000 currículums con identidades sociales aleatorizadas. Los modelos otorgan puntuaciones de evaluación más altas para candidatas femeninas con calificaciones similares, mientras que muchos están sesgados contra candidatos masculinos negros. Estos sesgos pueden resultar en diferencias de 1-3 puntos porcentuales en probabilidades de contratación para candidatos similares.",
    "id": "article-064",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:34.676Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:34.676Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgaf089",
      "fetched_title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "title_similarity": 0.875,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation"
  },
  {
    "legacy_article_number": 65,
    "title_original": "Large Language Models Can Infer Psychological Dispositions of Social Media Users",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Heinrich Peters",
      "Sandra C Matz"
    ],
    "keywords": [
      "Large Language Models",
      "Personality inference",
      "Big Five",
      "Social media",
      "Zero-shot learning",
      "GPT-3.5",
      "GPT-4"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgae231",
    "abstract_en_original": "Whether large language models like ChatGPT can accurately infer psychological dispositions of social media users is investigated. Specifically, whether GPT-3.5 and GPT-4 can derive Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario is tested. Results showed average correlation of r = .29 between model-inferred and self-reported trait scores, accuracy similar to supervised machine learning models trained for personality inference. Heterogeneity in accuracy across age groups and gender categories is highlighted.",
    "resumen_es_original": "Se investiga si modelos de lenguaje de gran escala como ChatGPT pueden inferir con precisión disposiciones psicológicas de usuarios de redes sociales. Específicamente, se prueba si GPT-3.5 y GPT-4 pueden derivar rasgos de personalidad de los Cinco Grandes de actualizaciones de estado de Facebook de usuarios en escenario de aprendizaje de cero disparos. Los resultados mostraron correlación promedio de r = .29 entre puntuaciones de rasgos inferidas por el modelo y autoinformadas, precisión similar a modelos de aprendizaje automático supervisado entrenados para inferencia de personalidad. Se destaca heterogeneidad en precisión a través de grupos de edad y categorías de género.",
    "id": "article-065",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:34.711Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:34.711Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgae231",
      "fetched_title": "Large language models can infer psychological dispositions of social media users",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Large language models can infer psychological dispositions of social media users"
  },
  {
    "legacy_article_number": 66,
    "title_original": "How Do Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ],
    "keywords": [
      "Personality traits",
      "Large language models",
      "Negotiation simulation",
      "Decision-making",
      "Big Five",
      "Bargaining dialogues"
    ],
    "source_url": "https://aclanthology.org/2024.findings-emnlp.605/",
    "abstract_en_original": "A simulation framework centered on large language model agents endowed with synthesized personality traits is introduced. Agents negotiate within bargaining domains and possess customizable personalities and objectives. Experimental results show behavioral tendencies of simulations can reproduce behavioral patterns observed in human negotiations. The contribution is twofold: proposing simulation methodology investigating alignment between linguistic and economic capabilities of agents, and offering empirical insights into strategic impacts of Big Five traits on bilateral negotiation outcomes.",
    "resumen_es_original": "Se introduce un marco de simulación centrado en agentes de modelos de lenguaje de gran escala dotados de rasgos de personalidad sintetizados. Los agentes negocian dentro de dominios de negociación y poseen personalidades y objetivos personalizables. Los resultados experimentales muestran que las tendencias conductuales de las simulaciones pueden reproducir patrones conductuales observados en negociaciones humanas. La contribución es doble: proponer metodología de simulación investigando alineación entre capacidades lingüísticas y económicas de agentes, y ofrecer perspectivas empíricas sobre impactos estratégicos de rasgos de los Cinco Grandes en resultados de negociaciones bilaterales.",
    "id": "article-066",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.736Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.736Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.605/",
      "fetched_title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
      "title_similarity": 0.9231,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models"
  },
  {
    "legacy_article_number": 67,
    "title_original": "Unveiling Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition in Dialogues",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Lei Sun",
      "Jinming Zhao",
      "Qin Jin"
    ],
    "keywords": [
      "Personality recognition",
      "Explainable AI",
      "Dialogue analysis",
      "Personality traits",
      "Machine learning",
      "Chain-of-Personality-Evidence (CoPE)"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-main.1115/",
    "abstract_en_original": "A novel task named Explainable Personality Recognition is proposed, aiming to reveal the reasoning process as supporting evidence of personality traits. Inspired by personality theories where traits are composed of stable patterns of personality states, the Chain-of-Personality-Evidence (CoPE) framework is proposed, involving reasoning from specific contexts to short-term states to long-term traits. Based on CoPE, the explainable personality recognition dataset PersonalityEvd is constructed from dialogues, introducing two tasks requiring models to recognize labels and supporting evidence.",
    "resumen_es_original": "Se propone una tarea novedosa denominada Reconocimiento de Personalidad Explicable, con el objetivo de revelar el proceso de razonamiento como evidencia de apoyo de los rasgos de personalidad. Inspirado por teorías de personalidad donde los rasgos se componen de patrones estables de estados de personalidad, se propone el marco Cadena-de-Evidencia-de-Personalidad (CoPE), involucrando razonamiento desde contextos específicos a estados de corto plazo hasta rasgos de largo plazo. Basado en CoPE, se construye el conjunto de datos de reconocimiento de personalidad explicable PersonalityEvd a partir de diálogos, introduciendo dos tareas que requieren que los modelos reconozcan etiquetas y evidencia de apoyo.",
    "id": "article-067",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.834Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.834Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-main.1115/",
      "fetched_title": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
      "title_similarity": 0.6923,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues"
  },
  {
    "legacy_article_number": 68,
    "title_original": "Bias and Fairness in Large Language Models: A Survey",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Isabel O. Gallegos",
      "Ryan A. Rossi",
      "Joe Barrow",
      "Md Mehrab Tanjim",
      "Sungchul Kim",
      "Franck Dernoncourt",
      "Tong Yu",
      "Ruiyi Zhang",
      "Nesreen K. Ahmed"
    ],
    "keywords": [
      "Large Language Models",
      "Bias evaluation",
      "Fairness",
      "Social bias",
      "Natural language processing",
      "AI ethics"
    ],
    "source_url": "https://doi.org/10.1162/coli_a_00524",
    "abstract_en_original": "Bias evaluation and mitigation techniques for large language models are presented in this comprehensive survey, consolidating, formalizing, and expanding notions of social bias and fairness in natural language processing. Three intuitive taxonomies are proposed: two for bias evaluation (metrics and datasets) and one for mitigation. Distinct facets of harm are defined, desiderata to operationalize fairness are introduced, and metrics are organized by different operational levels: embeddings, probabilities, and generated text.",
    "resumen_es_original": "Se presentan técnicas de evaluación y mitigación de sesgo para modelos de lenguaje de gran escala en esta revisión integral, consolidando, formalizando y expandiendo nociones de sesgo social y equidad en procesamiento de lenguaje natural. Se proponen tres taxonomías intuitivas: dos para evaluación de sesgo (métricas y conjuntos de datos) y una para mitigación. Se definen facetas distintas de daño, se introducen requisitos para operacionalizar equidad, y se organizan métricas por diferentes niveles operacionales: embeddings, probabilidades y texto generado.",
    "id": "article-068",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:34.864Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:34.864Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1162/coli_a_00524",
      "fetched_title": "Bias and Fairness in Large Language Models: A Survey",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Bias and Fairness in Large Language Models: A Survey"
  },
  {
    "legacy_article_number": 69,
    "title_original": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Rumi Allbert",
      "James K. Wiles",
      "Vlad Grankovsky"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Activation Engineering",
      "Personality Traits"
    ],
    "source_url": "https://arxiv.org/abs/2412.10427",
    "abstract_en_original": "Personality modification in large language models is explored, building on the novel approach of activation engineering. A method for identifying and adjusting activation directions related to personality traits is developed, which may allow for dynamic personality fine-tuning. This work aims to further understanding of model interpretability while examining ethical implications of such developments.",
    "resumen_es_original": "Se explora la modificación de personalidad en modelos de lenguaje de gran escala, construyendo sobre el enfoque novedoso de ingeniería de activación. Se desarrolla un método para identificar y ajustar direcciones de activación relacionadas con rasgos de personalidad, lo que puede permitir ajuste fino dinámico de personalidad. Este trabajo tiene como objetivo profundizar la comprensión de la interpretabilidad del modelo mientras examina las implicaciones éticas de tales desarrollos.",
    "id": "article-069",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.085Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.085Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2412.10427",
      "fetched_title": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering"
  },
  {
    "legacy_article_number": 70,
    "title_original": "PUB: A Personality-Enhanced LLM-Driven User Behavior Simulator for Recommender System Evaluation",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Chenglong Ma",
      "Ziqi Xu",
      "Yongli Ren",
      "Danula Hettiachchi",
      "Jeffrey Chan"
    ],
    "keywords": [
      "Information Retrieval",
      "Recommender Systems",
      "User Behavior Simulation",
      "Personality Traits",
      "Large Language Models"
    ],
    "source_url": "https://arxiv.org/abs/2506.04551",
    "abstract_en_original": "The Personality-driven User Behaviour Simulator (PUB) is proposed, a simulation framework based on large language models integrating Big Five personality traits to model personalized user behaviour. PUB dynamically infers user personality from behavioural logs and item metadata, then generates synthetic interactions preserving statistical fidelity to real-world data. Experiments show logs generated by PUB closely align with real user behaviour and reveal meaningful associations between personality traits and recommendation outcomes.",
    "resumen_es_original": "Se propone el Simulador de Comportamiento de Usuario Impulsado por Personalidad (PUB), un marco de simulación basado en modelos de lenguaje de gran escala que integra rasgos de personalidad de los Cinco Grandes para modelar comportamiento de usuario personalizado. PUB infiere dinámicamente la personalidad del usuario desde registros conductuales y metadatos de ítems, luego genera interacciones sintéticas preservando fidelidad estadística a datos del mundo real. Los experimentos muestran que los registros generados por PUB se alinean estrechamente con el comportamiento de usuario real y revelan asociaciones significativas entre rasgos de personalidad y resultados de recomendación.",
    "id": "article-070",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.099Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.099Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2506.04551",
      "fetched_title": "PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation",
      "title_similarity": 0.7857,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation"
  },
  {
    "legacy_article_number": 71,
    "title_original": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Bin Han",
      "Deuksin Kwon",
      "Spencer Lin",
      "Kaleen Shrestha",
      "Jonathan Gratch"
    ],
    "keywords": [
      "Human-Computer Interaction",
      "Large Language Models",
      "Virtual Agents",
      "Personality Traits",
      "Extraversion"
    ],
    "source_url": "https://arxiv.org/abs/2508.21087",
    "abstract_en_original": "A framework employing personality prompting with large language models is proposed to generate verbal and nonverbal behaviors for virtual agents based on personality traits. Focusing on extraversion, the system was evaluated in negotiation and ice-breaking scenarios using introverted and extroverted agents. Results demonstrate that large language models can generate verbal and nonverbal behaviors aligning with personality traits, and users are able to recognize these traits through agents' behaviors. These findings underscore the potential of large language models in shaping personality-aligned virtual agents.",
    "resumen_es_original": "Se propone un marco que emplea instrucciones de personalidad con modelos de lenguaje de gran escala para generar comportamientos verbales y no verbales en agentes virtuales basados en rasgos de personalidad. Con enfoque en extraversión, el sistema fue evaluado en escenarios de negociación y rompehielos utilizando agentes introvertidos y extrovertidos. Los resultados demuestran que los modelos de lenguaje de gran escala pueden generar comportamientos verbales y no verbales alineados con rasgos de personalidad, y los usuarios son capaces de reconocer estos rasgos a través de los comportamientos de los agentes. Estos hallazgos subrayan el potencial de los modelos de lenguaje de gran escala en moldear agentes virtuales alineados con la personalidad.",
    "id": "article-071",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.117Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.117Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.21087",
      "fetched_title": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?"
  },
  {
    "legacy_article_number": 72,
    "title_original": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Lewis Newsham",
      "Daniel Prince"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Multiagent Systems",
      "Large Language Models",
      "Autonomous Agents",
      "OCEAN model",
      "Cyber Defense"
    ],
    "source_url": "https://arxiv.org/abs/2504.00727",
    "abstract_en_original": "Building on previous work introducing SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, a novel method is presented for measuring and evaluating how induced personality traits affect task selection processes—specifically planning, scheduling, and decision-making—in agents based on large language models. Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.",
    "resumen_es_original": "Basándose en trabajo previo que introdujo SANDMAN, una arquitectura de Agente Engañoso que aprovecha el modelo de personalidad OCEAN de Cinco Factores, se presenta un método novedoso para medir y evaluar cómo los rasgos de personalidad inducidos afectan los procesos de selección de tareas—específicamente planificación, programación y toma de decisiones—en agentes basados en modelos de lenguaje de gran escala. Los resultados revelan patrones distintos de selección de tareas alineados con atributos OCEAN inducidos, subrayando la viabilidad de diseñar Agentes Engañosos altamente plausibles para estrategias proactivas de defensa cibernética.",
    "id": "article-072",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.136Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.136Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2504.00727",
      "fetched_title": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents"
  },
  {
    "legacy_article_number": 73,
    "title_original": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Ivar Frisch",
      "Mario Giulianelli"
    ],
    "keywords": [
      "Large Language Models",
      "Agent interaction",
      "Personality consistency",
      "Linguistic alignment",
      "Dialogue-based interaction"
    ],
    "source_url": "https://aclanthology.org/2024.personalize-1.9/",
    "abstract_en_original": "This experimental study seeks to lay groundwork for understanding dialogue-based interaction between large language models by conditioning GPT-3.5 on asymmetric personality profiles to create a population of agents. Agents were administered personality tests and submitted to a collaborative writing task. Findings reveal that different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.",
    "resumen_es_original": "Este estudio experimental busca establecer la base para entender la interacción basada en diálogo entre modelos de lenguaje de gran escala condicionando GPT-3.5 en perfiles de personalidad asimétricos para crear una población de agentes. Los agentes fueron administrados pruebas de personalidad y sometidos a una tarea de escritura colaborativa. Los hallazgos revelan que diferentes perfiles exhiben diferentes grados de consistencia de personalidad y alineación lingüística en la interacción.",
    "id": "article-073",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.152Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.152Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.personalize-1.9/",
      "fetched_title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models"
  },
  {
    "legacy_article_number": 74,
    "title_original": "Automated LLM Questionnaire for Automatic Psychiatric Assessment",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Gony Rosenman",
      "Talma Hendler",
      "Lior Wolf"
    ],
    "keywords": [
      "Large Language Models",
      "Psychiatric Assessment",
      "Mental Health",
      "Depression (PHQ-8)",
      "PTSD (PCL-C)",
      "Psychological Interviews"
    ],
    "source_url": "https://aclanthology.org/2024.findings-emnlp.23/",
    "abstract_en_original": "A large language model is employed to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The model is prompted to answer questionnaires by impersonating the interviewee. Obtained answers are coded as features used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C) using Random Forest regression. The approach enhances diagnostic accuracy compared to multiple baselines, establishing a novel framework for interpreting psychological interviews.",
    "resumen_es_original": "Se emplea un modelo de lenguaje de gran escala para convertir entrevistas psicológicas no estructuradas en cuestionarios estructurados que abarcan diversos dominios psiquiátricos y de personalidad. Se instruye al modelo para responder cuestionarios personificando al entrevistado. Las respuestas obtenidas se codifican como características utilizadas para predecir medidas psiquiátricas estandarizadas de depresión (PHQ-8) y estrés postraumático (PCL-C) mediante regresión Random Forest. El enfoque mejora la precisión diagnóstica en comparación con múltiples líneas base, estableciendo un marco novedoso para interpretar entrevistas psicológicas.",
    "id": "article-074",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.182Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.182Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.23/",
      "fetched_title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "LLM Questionnaire Completion for Automatic Psychiatric Assessment"
  },
  {
    "legacy_article_number": 75,
    "title_original": "Psychometric Shaping of Personality Modulates Capabilities and Safety in Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Stephen Fitz",
      "Peter Romero",
      "Steven Basart",
      "Sipeng Chen",
      "Jose Hernandez-Orallo"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Large Language Models",
      "Personality Traits",
      "Model Safety"
    ],
    "source_url": "https://arxiv.org/abs/2509.16332",
    "abstract_en_original": "How psychometric personality control grounded in the Big Five framework influences AI behavior in capability and safety benchmarks is investigated. Experiments reveal striking effects: reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy, as well as reduction in general capabilities measured by MMLU. Findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and competence.",
    "resumen_es_original": "Se investiga cómo el control psicométrico de personalidad fundamentado en el marco de los Cinco Grandes influye en el comportamiento de inteligencia artificial en pruebas de referencia de capacidad y seguridad. Los experimentos revelan efectos notables: reducir la responsabilidad conduce a caídas significativas en métricas relevantes de seguridad en pruebas como WMDP, TruthfulQA, ETHICS y Sycophancy, así como reducción en capacidades generales medidas por MMLU. Los hallazgos destacan el moldeo de personalidad como un eje poderoso y poco explorado de control de modelo que interactúa tanto con la seguridad como con la competencia.",
    "id": "article-075",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.509Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.509Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.16332",
      "fetched_title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models",
      "title_similarity": 0.9091,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models"
  },
  {
    "legacy_article_number": 76,
    "title_original": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wenhan Dong",
      "Yuemeng Zhao",
      "Zhen Sun",
      "Yule Liu",
      "Zifan Peng",
      "Jingyi Zheng",
      "Zongmin Zhang",
      "Ziyi Zhang",
      "Jun Wu",
      "Ruiming Wang",
      "Shengmin Xu",
      "Xinyi Huang",
      "Xinlei He"
    ],
    "keywords": [
      "Computers and Society",
      "Computation and Language",
      "Human-Computer Interaction",
      "Machine Learning",
      "Psychological traits",
      "Trustworthy AI"
    ],
    "source_url": "https://arxiv.org/abs/2505.00049",
    "abstract_en_original": "Six key dimensions of applying psychological theories to large language models are systematically covered: assessment tools, model-specific datasets, evaluation metrics (consistency and stability), empirical findings, personality simulation methods, and behavior simulation. The analysis highlights both strengths and limitations of current methods. While some models exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Future directions are proposed for developing interpretable, robust, and generalizable psychological assessment frameworks.",
    "resumen_es_original": "Se cubren sistemáticamente seis dimensiones clave de aplicar teorías psicológicas a modelos de lenguaje de gran escala: herramientas de evaluación, conjuntos de datos específicos del modelo, métricas de evaluación (consistencia y estabilidad), hallazgos empíricos, métodos de simulación de personalidad y simulación de comportamiento. El análisis destaca tanto las fortalezas como las limitaciones de los métodos actuales. Aunque algunos modelos exhiben patrones de personalidad reproducibles bajo esquemas específicos de instrucciones, permanece una variabilidad significativa entre tareas y configuraciones. Se proponen direcciones futuras para desarrollar marcos de evaluación psicológica interpretables, robustos y generalizables.",
    "id": "article-076",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.527Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.527Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2505.00049",
      "fetched_title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications"
  },
  {
    "legacy_article_number": 77,
    "title_original": "Large Language Models Demonstrate Distinctive Personality Profiles",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Thomas F Heston",
      "Justin Gillette"
    ],
    "keywords": [
      "AI ethics",
      "AI in mental health",
      "AI psychometrics",
      "Artificial intelligence in medicine",
      "Generative AI",
      "Personality assessment"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12183331/",
    "abstract_en_original": "The first psychometric analysis of large language model personality is provided using two validated frameworks: Open Extended Jungian Type Scales (OEJTS) and Big Five Personality Test. Four leading models (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) were evaluated in April 2024. MANOVA revealed statistically significant differences across models in personality traits. Distinct personality profiles are consistently expressed across different models, underscoring the need for formal personality evaluation before clinical deployment.",
    "resumen_es_original": "Se proporciona el primer análisis psicométrico de personalidad de modelos de lenguaje de gran escala utilizando dos marcos validados: Escalas de Tipo Junguiano Extendido Abierto (OEJTS) y Prueba de Personalidad de los Cinco Grandes. Cuatro modelos líderes (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) fueron evaluados en abril de 2024. MANOVA reveló diferencias estadísticamente significativas entre modelos en rasgos de personalidad. Perfiles de personalidad distintos se expresan consistentemente entre diferentes modelos, subrayando la necesidad de evaluación formal de personalidad antes del despliegue clínico.",
    "id": "article-077",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.532Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.532Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12183331/",
      "fetched_title": "Large Language Models Demonstrate Distinct Personality Profiles - PMC",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Large Language Models Demonstrate Distinct Personality Profiles - PMC"
  },
  {
    "legacy_article_number": 78,
    "title_original": "Attitudes Toward AI: Measurement and Associations with Personality",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "J.P. Stein",
      "T. Messingschlager",
      "T. Gnambs",
      "F. Hutmacher",
      "M. Appel"
    ],
    "keywords": [
      "Artificial Intelligence attitudes",
      "Big Five personality",
      "Dark Triad",
      "Conspiracy beliefs",
      "ATTARI-12 questionnaire"
    ],
    "source_url": "https://www.nature.com/articles/s41598-024-53335-2",
    "abstract_en_original": "A novel psychologically informed questionnaire (ATTARI-12) is presented capturing attitudes towards AI as a single construct independent of specific contexts. The questionnaire demonstrated good reliability and validity across two studies (N1 = 490; N2 = 150). Personality traits—Big Five, Dark Triad, and conspiracy mentality—were examined as potential predictors. Agreeableness and younger age predict more positive views towards AI technology, whereas susceptibility to conspiracy beliefs is associated with more negative attitudes.",
    "resumen_es_original": "Se presenta un cuestionario psicológicamente informado novedoso (ATTARI-12) que captura actitudes hacia la inteligencia artificial como un constructo único independiente de contextos específicos. El cuestionario demostró buena confiabilidad y validez en dos estudios (N1 = 490; N2 = 150). Se examinaron rasgos de personalidad—Cinco Grandes, Tríada Oscura y mentalidad conspirativa—como predictores potenciales. La amabilidad y la edad más joven predicen vistas más positivas hacia la tecnología de inteligencia artificial, mientras que la susceptibilidad a creencias conspirativas se asocia con actitudes más negativas.",
    "id": "article-078",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.543Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.543Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-53335-2?error=cookies_not_supported&code=419b1abe-8d52-4960-babf-819682220aa0",
      "fetched_title": "Attitudes towards AI: measurement and associations with personality - Scientific Reports",
      "title_similarity": 0.6364,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Attitudes towards AI: measurement and associations with personality - Scientific Reports"
  },
  {
    "legacy_article_number": 79,
    "title_original": "Do LLMs Have a Personality? A Psychometric Assessment with Implications for Clinical Medicine and Mental Health AI",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Thomas F Heston",
      "Justin Gillette"
    ],
    "keywords": [
      "Psychiatry",
      "Clinical Psychology",
      "Mental Health AI",
      "Large Language Models",
      "Personality Assessment",
      "Clinical Medicine"
    ],
    "source_url": "https://www.medrxiv.org/content/10.1101/2025.03.14.25323987v1",
    "abstract_en_original": "Personality profiles of four leading large language models in 2024 were characterized using two validated frameworks: Open Extended Jungian Type Scales and Big Five Personality Test. MANOVA revealed statistically significant differences across models in personality traits. ChatGPT-3.5 was most often classified as ENTJ, Claude 3 Opus as INTJ, while Gemini Advanced and Grok-Regular leaned toward INFJ. Distinct personality profiles are consistently expressed across different models, emphasizing the need for formal personality evaluation before deploying models in clinical workflows.",
    "resumen_es_original": "Se caracterizaron perfiles de personalidad de cuatro modelos de lenguaje de gran escala líderes en 2024 utilizando dos marcos validados: Escalas de Tipo Junguiano Extendido Abierto y Prueba de Personalidad de los Cinco Grandes. MANOVA reveló diferencias estadísticamente significativas entre modelos en rasgos de personalidad. ChatGPT-3.5 fue clasificado más frecuentemente como ENTJ, Claude 3 Opus como INTJ, mientras que Gemini Advanced y Grok-Regular se inclinaron hacia INFJ. Perfiles de personalidad distintos se expresan consistentemente entre diferentes modelos, enfatizando la necesidad de evaluación formal de personalidad antes de desplegar modelos en flujos de trabajo clínicos.",
    "id": "article-079",
    "source_type": "preprint_other",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.655Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.655Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.medrxiv.org/content/10.1101/2025.03.14.25323987v1",
      "fetched_title": "Do Large Language Models Have a Personality? A Psychometric Evaluation with Implications for Clinical Medicine and Mental Health AI",
      "title_similarity": 0.6842,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Do Large Language Models Have a Personality? A Psychometric Evaluation with Implications for Clinical Medicine and Mental Health AI"
  },
  {
    "legacy_article_number": 80,
    "title_original": "Developing and Enhancing Personality Inventories Using Generative AI: Psychometric Properties of a Short HEXACO Scale Developed with ChatGPT",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Ard J. Barends",
      "Reinout E. de Vries"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Generative AI",
      "HEXACO personality inventory",
      "ChatGPT 4.0",
      "Psychometrics",
      "Survey development",
      "Content validity"
    ],
    "source_url": "https://doi.org/10.1080/00223891.2024.2444454",
    "abstract_en_original": "A 24-item HEXACO personality inventory was generated using ChatGPT 4.0, called ChatGPT HEXACO inventory (CHI). Whether ChatGPT could modify CHI to improve its internal consistency or content validity was investigated. Participants (N = 682) completed Brief HEXACO Inventory (BHI) and HEXACO-60 and were randomly assigned to complete one of three CHI versions. Results showed generally comparable psychometric properties across the three CHI versions and BHI. However, ChatGPT could not improve specific psychometric properties of CHI.",
    "resumen_es_original": "Se generó un inventario de personalidad HEXACO de 24 ítems utilizando ChatGPT 4.0, denominado inventario HEXACO ChatGPT (CHI). Se investigó si ChatGPT podría modificar CHI para mejorar su consistencia interna o validez de contenido. Los participantes (N = 682) completaron el Inventario HEXACO Breve (BHI) y HEXACO-60, y fueron asignados aleatoriamente para completar una de tres versiones de CHI. Los resultados mostraron propiedades psicométricas generalmente comparables entre las tres versiones de CHI y BHI. Sin embargo, ChatGPT no pudo mejorar propiedades psicométricas específicas de CHI.",
    "id": "article-080",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:34.981Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:34.981Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1080/00223891.2024.2444454",
      "fetched_title": "Developing and Improving Personality Inventories Using Generative Artificial Intelligence: The Psychometric Properties of a Short HEXACO Scale Developed Using ChatGPT 4.0",
      "title_similarity": 0.6667,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Developing and Improving Personality Inventories Using Generative Artificial Intelligence: The Psychometric Properties of a Short HEXACO Scale Developed Using ChatGPT 4.0"
  },
  {
    "legacy_article_number": 81,
    "title_original": "Exploring the Impact of Language Switching on Personality Manifestation in LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jacopo Amidei",
      "Jose Gregorio Ferreira De Sá",
      "Rubén Nieto Luna",
      "Andreas Kaltenbrunner"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Traits",
      "Language Switching",
      "GPT-4o",
      "Eysenck Personality Questionnaire-Revised (EPQR-A)",
      "Cross-language analysis"
    ],
    "source_url": "https://aclanthology.org/2025.coling-main.162/",
    "abstract_en_original": "The extent to which large language models align with humans when personality shifts are associated with language changes is investigated. Based on three experiments focusing on GPT-4o and the Eysenck Personality Questionnaire-Revised (EPQR-A), initial results reveal weak yet significant variation in GPT-4o's personality across languages, indicating some variations stem from language-switching effects rather than translation. Further analysis across five English-speaking countries shows GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits.",
    "resumen_es_original": "Se investiga hasta qué punto los modelos de lenguaje de gran escala se alinean con humanos cuando los cambios de personalidad se asocian con cambios de idioma. Basándose en tres experimentos enfocados en GPT-4o y el Cuestionario de Personalidad de Eysenck-Revisado (EPQR-A), los resultados iniciales revelan variación débil pero significativa en la personalidad de GPT-4o entre idiomas, indicando que algunas variaciones provienen de efectos de cambio de idioma en lugar de traducción. Análisis adicional entre cinco países de habla inglesa muestra que GPT-4o, aprovechando estereotipos, refleja rasgos de personalidad específicos de cada país.",
    "id": "article-081",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.130Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.130Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.coling-main.162/",
      "fetched_title": "Exploring the Impact of Language Switching on Personality Traits in LLMs",
      "title_similarity": 0.8333,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Exploring the Impact of Language Switching on Personality Traits in LLMs"
  },
  {
    "legacy_article_number": 82,
    "title_original": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Seungjong Sun",
      "Seo Yeon Baek",
      "Jang Hyun Kim"
    ],
    "keywords": [
      "personality modulation",
      "model merging",
      "personality vectors",
      "Big Five traits",
      "continuous control",
      "multidimensional traits",
      "personalized AI"
    ],
    "source_url": "https://arxiv.org/abs/2509.19727",
    "abstract_en_original": "Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models with human traits such as personality. Previous attempts to induce personality have shown promising results, but struggle to capture the continuous and multidimensional nature of human traits. A novel method for personality modulation via model merging is proposed. Specifically, personality vectors are constructed by subtracting the weights of a pre-trained model from those of the fine-tuned model on a given personality trait. By merging personality vectors, models are enabled to exhibit desired personality traits without additional training.",
    "resumen_es_original": "Impulsada por la demanda de sistemas de inteligencia artificial personalizados, existe un creciente interés en alinear el comportamiento de los modelos de lenguaje de gran escala con rasgos humanos como la personalidad. Los intentos previos de inducir personalidad han mostrado resultados prometedores, pero tienen dificultades para capturar la naturaleza continua y multidimensional de los rasgos humanos. Se propone un método novedoso para la modulación de personalidad mediante fusión de modelos. Específicamente, se construyen vectores de personalidad restando los pesos de un modelo preentrenado de aquellos del modelo con ajuste fino en un rasgo de personalidad dado. Mediante la fusión de vectores de personalidad, se habilita a los modelos para exhibir rasgos de personalidad deseados sin entrenamiento adicional.",
    "id": "article-082",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.251Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.251Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.19727",
      "fetched_title": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality Vector: Modulating Personality of Large Language Models by Model Merging"
  },
  {
    "legacy_article_number": 83,
    "title_original": "Humanoid Artificial Consciousness Designed with LLM Based on Psychoanalysis",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Sang Hun Kim",
      "Jongmin Lee",
      "Dongkyu Park",
      "So Young Lee",
      "Yosep Chong"
    ],
    "keywords": [
      "artificial consciousness",
      "psychoanalysis",
      "MBTI",
      "personality modules",
      "character simulation",
      "human-like cognition",
      "self-awareness"
    ],
    "source_url": "https://arxiv.org/abs/2510.09043",
    "abstract_en_original": "A novel approach is proposed to address challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. Three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) were developed based on the principles of psychoanalysis. Additionally, 16 characters with different personalities representing the sixteen MBTI types were designed.",
    "resumen_es_original": "Se propone un enfoque novedoso para abordar desafíos integrando el psicoanálisis y el Indicador de Tipo Myers-Briggs (MBTI) en la construcción de módulos de consciencia y personalidad. Se desarrollaron tres consciencias artificiales (autoconciencia, inconsciencia y preconsciencia) basadas en los principios del psicoanálisis. Adicionalmente, se diseñaron 16 personajes con diferentes personalidades representando los dieciséis tipos MBTI.",
    "id": "article-083",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.265Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.265Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.09043",
      "fetched_title": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory",
      "title_similarity": 0.5333,
      "author_overlap": 0.4,
      "year_match": true
    },
    "title_canonical": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory"
  },
  {
    "legacy_article_number": 85,
    "title_original": "PerFairX: Is There a Balance Between Fairness and Personality in LLM Recommendations?",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Chandan Kumar Sah"
    ],
    "keywords": [
      "personality-based recommendation",
      "fairness evaluation",
      "OCEAN model",
      "LLM recommender systems",
      "demographic equity",
      "personalization trade-offs",
      "zero-shot learning"
    ],
    "source_url": "https://arxiv.org/abs/2509.08829",
    "abstract_en_original": "PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in recommendations generated by large language models, is proposed. Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities.",
    "resumen_es_original": "Se propone PerFairX, un marco de evaluación unificado diseñado para cuantificar los compromisos entre personalización y equidad demográfica en recomendaciones generadas por modelos de lenguaje de gran escala. Los resultados revelan que las instrucciones conscientes de la personalidad mejoran significativamente la alineación con rasgos individuales pero pueden exacerbar disparidades de equidad.",
    "id": "article-085",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.282Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.282Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.08829",
      "fetched_title": "PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?",
      "title_similarity": 0.7143,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?"
  },
  {
    "legacy_article_number": 86,
    "title_original": "Population-Aligned Persona Generation for LLM-based Social Simulation",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Zhengyu Hu",
      "Jianxun Lian",
      "Zheyuan Xiao",
      "Max Xiong",
      "Yuxuan Lei",
      "Tianfu Wang",
      "Kaize Ding",
      "Ziang Xiao",
      "Nicholas Jing Yuan",
      "Xing Xie"
    ],
    "keywords": [
      "population alignment",
      "persona generation",
      "social simulation",
      "Big Five traits",
      "computational social science",
      "importance sampling",
      "bias reduction"
    ],
    "source_url": "https://arxiv.org/abs/2509.10127",
    "abstract_en_original": "A systematic framework for synthesizing high-quality, population-aligned persona sets for social simulation driven by large language models is proposed. The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions.",
    "resumen_es_original": "Se propone un marco sistemático para sintetizar conjuntos de personas de alta calidad y alineados con la población para simulación social impulsada por modelos de lenguaje de gran escala. El enfoque aprovecha modelos de lenguaje de gran escala para generar personas narrativas a partir de datos de redes sociales, seguido de evaluación de calidad y muestreo por importancia para lograr alineación global con distribuciones psicométricas.",
    "id": "article-086",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.297Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.297Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.10127",
      "fetched_title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Population-Aligned Persona Generation for LLM-based Social Simulation"
  },
  {
    "legacy_article_number": 88,
    "title_original": "Exploring the Potential of Large Language Models to Simulate Personality",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Maria Molchanova",
      "Anna Mikhailova",
      "Anna Korzanova",
      "Lidiia Ostyakova",
      "Alexandra Dolidze"
    ],
    "keywords": [
      "personality simulation",
      "conversational AI",
      "Big Five model",
      "dialogue personalization",
      "personality-related text generation",
      "LLM challenges",
      "trait modeling"
    ],
    "source_url": "https://arxiv.org/abs/2502.08265",
    "abstract_en_original": "With the advancement of large language models, the focus in Conversational AI has shifted to tackling more complex challenges, such as personalizing dialogue systems. Simulating personal traits according to the Big Five model is addressed. Research demonstrates that generating personality-related texts remains a challenging task.",
    "resumen_es_original": "Con el avance de los modelos de lenguaje de gran escala, el enfoque en la inteligencia artificial conversacional ha cambiado hacia abordar desafíos más complejos, como personalizar sistemas de diálogo. Se aborda la simulación de rasgos personales según el modelo de los Cinco Grandes. La investigación demuestra que generar textos relacionados con personalidad sigue siendo una tarea desafiante.",
    "id": "article-088",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.315Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.315Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.08265",
      "fetched_title": "Exploring the Potential of Large Language Models to Simulate Personality",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Exploring the Potential of Large Language Models to Simulate Personality"
  },
  {
    "legacy_article_number": 89,
    "title_original": "Rediscovering the Latent Dimensions of Personality with LLMs as Trait Descriptors",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Joseph Suh",
      "Suhong Moon",
      "Minwoo Kang",
      "David Chan"
    ],
    "keywords": [
      "personality assessment",
      "Big Five traits",
      "singular value decomposition",
      "latent dimensions",
      "trait descriptors",
      "personality probing",
      "LLM evaluation"
    ],
    "source_url": "https://neurips.cc/virtual/2024/102146",
    "abstract_en_original": "A novel approach that uncovers latent personality dimensions in large language models by applying SVD to log-probabilities of trait-descriptive adjectives is introduced. Models \"rediscover\" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance.",
    "resumen_es_original": "Se introduce un enfoque novedoso que descubre dimensiones latentes de personalidad en modelos de lenguaje de gran escala aplicando SVD (descomposición en valores singulares) a las log-probabilidades de adjetivos descriptivos de rasgos. Los modelos \"redescubren\" rasgos de personalidad centrales sin depender de entradas directas de cuestionarios, con los 5 factores principales explicando el 74.3% de la varianza.",
    "id": "article-089",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.328Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.328Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://neurips.cc/virtual/2024/102146",
      "fetched_title": "NeurIPS Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "NeurIPS Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors"
  },
  {
    "legacy_article_number": 90,
    "title_original": "PICLe: Eliciting Diverse Behaviors from LLMs with Persona In-Context Learning",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Hyeong Kyu Choi",
      "Sharon Li"
    ],
    "keywords": [
      "persona elicitation",
      "in-context learning",
      "Bayesian inference",
      "behavioral preferences",
      "personality traits",
      "ICL",
      "persona customization"
    ],
    "source_url": "https://icml.cc/virtual/2024/poster/32764",
    "abstract_en_original": "Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference, is presented. PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona.",
    "resumen_es_original": "Se presenta Persona In-Context Learning (PICLe), un marco novedoso de obtención de persona fundamentado en inferencia bayesiana. PICLe introduce un nuevo criterio de selección de ejemplos de aprendizaje en contexto basado en razón de verosimilitud, diseñado para guiar óptimamente al modelo en la obtención de una persona objetivo específica.",
    "id": "article-090",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.478Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.478Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://icml.cc/virtual/2024/poster/32764",
      "fetched_title": "ICML Poster PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
      "title_similarity": 0.625,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "ICML Poster PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning"
  },
  {
    "legacy_article_number": 91,
    "title_original": "Large Language Models as Superpositions of Cultural Perspectives",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Autores anónimos (ICLR 2024)"
    ],
    "keywords": [
      "cultural perspectives",
      "personality stability",
      "perspective shift",
      "context dependency",
      "value expression",
      "psychological assessment",
      "perspective controllability"
    ],
    "source_url": "https://openreview.net/forum?id=1FWDEIGm33",
    "abstract_en_original": "\"Large language models as a superposition of perspectives\" is proposed: models simulate a multiplicity of behaviors which can be triggered by context. Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect.",
    "resumen_es_original": "Se propone \"modelos de lenguaje de gran escala como una superposición de perspectivas\": los modelos simulan una multiplicidad de comportamientos que pueden ser desencadenados por el contexto. Se demuestra que los cambios de contexto resultan en cambios significativos no deseados y difíciles de predecir en los valores expresados, denominados efecto de cambio inesperado de perspectiva.",
    "id": "article-091",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:19.347Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:19.347Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://openreview.net/forum?id=1FWDEIGm33",
      "fetched_title": "Large Language Models as superpositions of cultural perspectives",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Large Language Models as superpositions of cultural perspectives"
  },
  {
    "legacy_article_number": 92,
    "title_original": "LLM vs Small Model? LLM-Based Text Augmentation for Personality Detection",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Linmei Hu",
      "Hongyu He",
      "Duokang Wang",
      "Ziwang Zhao",
      "Yingxia Shao",
      "Liqiang Nie"
    ],
    "keywords": [
      "personality detection",
      "text augmentation",
      "knowledge distillation",
      "contrastive learning",
      "psycho-linguistic analysis",
      "social media",
      "Big Five traits"
    ],
    "source_url": "https://ojs.aaai.org/index.php/AAAI/article/view/29782",
    "abstract_en_original": "A text augmentation-enhanced personality detection model based on large language models is proposed, which distills knowledge from large language models to enhance the small model for personality detection. Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects.",
    "resumen_es_original": "Se propone un modelo de detección de personalidad mejorado con aumento de texto basado en modelos de lenguaje de gran escala, que destila el conocimiento de modelos de lenguaje de gran escala para mejorar el modelo pequeño para la detección de personalidad. Se habilita a los modelos de lenguaje de gran escala para generar análisis de publicaciones desde aspectos semánticos, de sentimiento y lingüísticos.",
    "id": "article-092",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:19.526Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:19.526Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://ojs.aaai.org/index.php/AAAI/article/view/29782",
      "fetched_title": "LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model | Proceedings of the AAAI Conference on Artificial Intelligence",
      "title_similarity": 0.4286,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model | Proceedings of the AAAI Conference on Artificial Intelligence"
  },
  {
    "legacy_article_number": 93,
    "title_original": "Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Luke James Klinkert",
      "Silvia Buongiorno",
      "Christopher Clark"
    ],
    "keywords": [
      "personality emulation",
      "NPC behavior",
      "game AI",
      "personality alignment",
      "human-like traits",
      "LLM evaluation",
      "interactive agents"
    ],
    "source_url": "https://ojs.aaai.org/index.php/AIIDE/article/view/31867",
    "abstract_en_original": "Results indicate that NPCs can successfully emulate human-like personality traits using large language models. Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters.",
    "resumen_es_original": "Los resultados indican que los personajes no jugadores (NPCs) pueden emular exitosamente rasgos de personalidad similares a los humanos utilizando modelos de lenguaje de gran escala. Los modelos de vanguardia lograron hasta 100% de alineación con perfiles de personalidad humanos, demostrando que los modelos de lenguaje de gran escala pueden representar con precisión personalidades humanas deseadas para personajes de juego.",
    "id": "article-093",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:19.854Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:19.854Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://ojs.aaai.org/index.php/AIIDE/article/view/31867",
      "fetched_title": "Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities | Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment",
      "title_similarity": 0.5,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities | Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment"
  },
  {
    "legacy_article_number": 94,
    "title_original": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Xintao Wang",
      "Yunze Xiao",
      "Jen-tse Huang",
      "Siyu Yuan",
      "Rui Xu",
      "Haoran Guo",
      "Quan Tu",
      "Yaying Fei",
      "Ziang Leng",
      "Wei Wang",
      "Jiangjie Chen",
      "Cheng Li",
      "Yanghua Xiao"
    ],
    "keywords": [
      "role-playing agents",
      "personality assessment",
      "psychological scales",
      "character fidelity",
      "LLM evaluation",
      "Big Five personality traits",
      "psychometric testing"
    ],
    "source_url": "https://aclanthology.org/2024.acl-long.102/",
    "abstract_en_original": "InCharacter, namely Interviewing Character agents for personality tests, is proposed. Experiments cover 32 distinct characters on 14 widely used psychological scales. State-of-the-art role-playing agents exhibit personalities highly aligned with human-perceived personalities of characters, achieving accuracy up to 80.7%.",
    "resumen_es_original": "Se propone InCharacter, es decir, Entrevistar agentes de Personaje para pruebas de personalidad. Los experimentos cubren 32 personajes distintos en 14 escalas psicológicas ampliamente utilizadas. Los agentes de juego de roles de última generación exhiben personalidades altamente alineadas con las personalidades percibidas por humanos de los personajes, alcanzando una precisión de hasta 80.7%.",
    "id": "article-094",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.290Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.290Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.acl-long.102/",
      "fetched_title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
      "title_similarity": 0.7273,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews"
  },
  {
    "legacy_article_number": 95,
    "title_original": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Qisen Yang",
      "Zekun Wang",
      "Honghui Chen",
      "Shenzhi Wang",
      "Yifan Pu",
      "Xin Gao",
      "Wenhao Huang",
      "Shiji Song",
      "Gao Huang"
    ],
    "keywords": [
      "psychological measurement",
      "LLM agents",
      "interactive fiction games",
      "gamification",
      "personality traits",
      "psychometric evaluation",
      "mental health assessment"
    ],
    "source_url": "https://aclanthology.org/2024.acl-long.779/",
    "abstract_en_original": "PsychoGAT is proposed to achieve generic gamification of psychological assessment. Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games.",
    "resumen_es_original": "Se propone PsychoGAT para lograr una gamificación genérica de la evaluación psicológica. Los modelos de lenguaje de gran escala funcionan tanto como psicólogos expertos como diseñadores de juegos innovadores, transformando cualquier escala estandarizada en juegos de ficción interactiva personalizados y atractivos.",
    "id": "article-095",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.623Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.623Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.acl-long.779/",
      "fetched_title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents"
  },
  {
    "legacy_article_number": 96,
    "title_original": "Investigating Personality Consistency in Quantized Role-Playing Dialogue Agents",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yixiao Wang",
      "Homa Fashandi",
      "Kevin Ferreira"
    ],
    "keywords": [
      "model quantization",
      "role-playing agents",
      "personality consistency",
      "Big Five model",
      "edge computing",
      "multi-turn dialogue",
      "conversational AI"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-industry.19/",
    "abstract_en_original": "Personality consistency in quantized large language models for edge device role-playing scenarios is explored. A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits.",
    "resumen_es_original": "Se explora la consistencia de personalidad en modelos de lenguaje de gran escala cuantizados para escenarios de juego de roles en dispositivos de borde. Se propone un método no paramétrico denominado Think2 para abordar la inconsistencia de personalidad, demostrando efectividad en mantener rasgos de personalidad consistentes.",
    "id": "article-096",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.636Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.636Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-industry.19/",
      "fetched_title": "Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents",
      "title_similarity": 0.9,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents"
  },
  {
    "legacy_article_number": 97,
    "title_original": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Zhengyuan Liu",
      "Stella Xin Yin",
      "Geyu Lin",
      "Nancy F. Chen"
    ],
    "keywords": [
      "intelligent tutoring systems",
      "student simulation",
      "personality traits",
      "educational technology",
      "language learning",
      "conversational AI",
      "personalized learning"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-main.37/",
    "abstract_en_original": "A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios.",
    "resumen_es_original": "Se propone un marco para construir perfiles de diferentes grupos de estudiantes refinando e integrando aspectos tanto cognitivos como no cognitivos, aprovechando modelos de lenguaje de gran escala para simulación de estudiantes consciente de personalidad en escenarios de aprendizaje de idiomas.",
    "id": "article-097",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.970Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.970Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-main.37/",
      "fetched_title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems"
  },
  {
    "legacy_article_number": 98,
    "title_original": "PersonaLLM: Investigating the Ability of LLMs to Express Personality Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Hang Jiang",
      "Xiajie Zhang",
      "Xubo Cao",
      "Cynthia Breazeal",
      "Deb Roy",
      "Jad Kabbara"
    ],
    "keywords": [
      "LLM personas",
      "Big Five personality model",
      "personality expression",
      "text generation",
      "psycholinguistic patterns",
      "human evaluation",
      "personalized chatbots"
    ],
    "source_url": "https://aclanthology.org/2024.findings-naacl.229/",
    "abstract_en_original": "Whether large language models can generate content that aligns with assigned personality profiles is investigated. Results show that model personas' self-reported BFI scores are consistent with designated personality types. Human evaluation demonstrates that humans can perceive personality traits with accuracy up to 80%.",
    "resumen_es_original": "Se investiga si los modelos de lenguaje de gran escala pueden generar contenido que se alinee con perfiles de personalidad asignados. Los resultados muestran que las puntuaciones BFI autorreportadas de las personas del modelo son consistentes con los tipos de personalidad designados. La evaluación humana demuestra que los humanos pueden percibir rasgos de personalidad con una precisión de hasta 80%.",
    "id": "article-098",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.988Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.988Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-naacl.229/",
      "fetched_title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits",
      "title_similarity": 0.6923,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits"
  },
  {
    "legacy_article_number": 99,
    "title_original": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using LLMs",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Ji-Eun Han",
      "Jun-Seok Koh",
      "Hyeon-Tae Seo",
      "Du-Seong Chang",
      "Kyung-Ah Sohn"
    ],
    "keywords": [
      "synthetic dialogue generation",
      "personality-based dialogue",
      "Big Five extraversion",
      "multilingual NLP",
      "Korean language",
      "conversational AI",
      "data augmentation"
    ],
    "source_url": "https://aclanthology.org/2024.lrec-main.1166/",
    "abstract_en_original": "A novel end-to-end personality-based synthetic dialogue data generation pipeline is presented. PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model.",
    "resumen_es_original": "Se presenta una novedosa secuencia de generación de datos de diálogo sintético basado en personalidad de extremo a extremo. Se introduce PSYDIAL, el primer conjunto de datos de diálogo coreano enfocado en diálogos basados en personalidad, con enfoque en la dimensión de Extraversión del modelo de personalidad de los Cinco Grandes.",
    "id": "article-099",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.319Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.319Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.lrec-main.1166/",
      "fetched_title": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models",
      "title_similarity": 0.6364,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models"
  },
  {
    "legacy_article_number": 100,
    "title_original": "Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Marina Tiuleneva",
      "Vadim A. Porvatov",
      "Carlo Strapparava"
    ],
    "keywords": [
      "Big Five personality traits",
      "character analysis",
      "psycholinguistics",
      "dramatic dialogue",
      "dataset creation",
      "fictional characters",
      "computational personality analysis"
    ],
    "source_url": "https://aclanthology.org/2024.cogalex-1.13/",
    "abstract_en_original": "A novel textual dataset comprising fictional characters' lines with annotations based on gender and Big-Five personality traits is introduced. Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively.",
    "resumen_es_original": "Se introduce un novedoso conjunto de datos textual que comprende líneas de personajes ficticios con anotaciones basadas en género y rasgos de personalidad de los Cinco Grandes. Los resultados indican que las personas imaginadas reflejan la mayoría de las categorías lingüísticas observadas en personas reales mientras las demuestran de manera más expresiva.",
    "id": "article-100",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.339Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.339Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.cogalex-1.13/",
      "fetched_title": "Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits & Gender Analysis",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits & Gender Analysis"
  },
  {
    "legacy_article_number": 101,
    "title_original": "Is Persona Enough for Personality? Using ChatGPT to Reconstruct Latent Personality",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yongyi Ji",
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "keywords": [
      "HEXACO personality framework",
      "personality reconstruction",
      "persona modeling",
      "socio-demographic factors",
      "personality consistency",
      "latent dimensions",
      "agent simulation"
    ],
    "source_url": "https://arxiv.org/abs/2406.12216",
    "abstract_en_original": "Capabilities of large language models in reconstructing complex cognitive attributes based on simple descriptions are explored. Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined.",
    "resumen_es_original": "Se exploran las capacidades de los modelos de lenguaje de gran escala para reconstruir atributos cognitivos complejos basados en descripciones simples. Utilizando el marco de personalidad HEXACO, se examina la consistencia de los modelos en recuperar y predecir dimensiones de personalidad subyacentes a partir de descripciones simples.",
    "id": "article-101",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.668Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.668Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.12216",
      "fetched_title": "Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions"
  },
  {
    "legacy_article_number": 102,
    "title_original": "A Survey of Personality, Persona, and Profile in Conversational Agents",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Richard Sutcliffe"
    ],
    "keywords": [
      "conversational agents",
      "chatbots",
      "personality models",
      "Big Five",
      "persona",
      "dialogue systems",
      "neural networks"
    ],
    "source_url": "https://arxiv.org/abs/2401.00609",
    "abstract_en_original": "A comprehensive review of personality in neural conversational agents is presented. Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed.",
    "resumen_es_original": "Se presenta una revisión integral de la personalidad en agentes conversacionales neurales. Se definen Personalidad, Persona y Perfil, se explican todos los esquemas de personalidad utilizados en agentes conversacionales, se describen 21 conjuntos de datos, y se revisan modelos y métodos recientes.",
    "id": "article-102",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.683Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.683Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2401.00609",
      "fetched_title": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
      "title_similarity": 0.9,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots"
  },
  {
    "legacy_article_number": 103,
    "title_original": "Character-LLM: A Trainable Agent for Role-Playing",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yunfan Shao",
      "Linyang Li",
      "Junqi Dai",
      "Xipeng Qiu"
    ],
    "keywords": [
      "role-playing agents",
      "character simulation",
      "LLM agents",
      "experience reconstruction",
      "personality embodiment",
      "interactive characters",
      "human simulacra"
    ],
    "source_url": "https://arxiv.org/abs/2310.10158",
    "abstract_en_original": "Character-LLM is introduced to teach large language models to act as specific people such as Beethoven, Queen Cleopatra, or Julius Caesar. The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra.",
    "resumen_es_original": "Se introduce Character-LLM para enseñar a los modelos de lenguaje de gran escala a actuar como personas específicas tales como Beethoven, la Reina Cleopatra o Julio César. El método se enfoca en editar perfiles como experiencias de un cierto personaje y entrenar modelos para ser simulacros personales.",
    "id": "article-103",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.689Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.689Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2310.10158",
      "fetched_title": "Character-LLM: A Trainable Agent for Role-Playing",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Character-LLM: A Trainable Agent for Role-Playing"
  },
  {
    "legacy_article_number": 104,
    "title_original": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Jiangjie Chen",
      "Xintao Wang",
      "Rui Xu",
      "Siyu Yuan",
      "Yikai Zhang",
      "Wei Shi",
      "Jian Xie",
      "Shuang Li",
      "Ruihan Yang",
      "Tinghui Zhu",
      "Aili Chen",
      "Nianqi Li",
      "Lida Chen",
      "Caiyu Hu",
      "Siye Wu",
      "Scott Ren",
      "Ziquan Fu",
      "Yanghua Xiao"
    ],
    "keywords": [
      "role-playing agents",
      "persona modeling",
      "personalization",
      "character simulation",
      "interactive systems",
      "LLM applications",
      "human-likeness"
    ],
    "source_url": "https://arxiv.org/abs/2404.18231",
    "abstract_en_original": "A comprehensive survey of Role-Playing Language Agents (RPLAs), specialized AI systems designed to simulate assigned personas, is conducted. Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation.",
    "resumen_es_original": "Se conduce una encuesta integral de Agentes de Lenguaje de Juego de Roles (RPLAs), sistemas de inteligencia artificial especializados diseñados para simular personas asignadas. Las personas se categorizan en tipos Demográfico, de Personaje e Individualizado, cubriendo metodologías, obtención de datos, construcción y evaluación.",
    "id": "article-104",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.697Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.697Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2404.18231",
      "fetched_title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
      "title_similarity": 1,
      "author_overlap": 0.1667,
      "year_match": true
    },
    "title_canonical": "From Persona to Personalization: A Survey on Role-Playing Language Agents"
  },
  {
    "legacy_article_number": 105,
    "title_original": "Identifying Cooperative Personalities in Multi-agent Contexts",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Kenneth J. K. Ong",
      "Lye Jia Jun",
      "Hieu Minh \"Jord\" Nguyen",
      "Seong Hah Cho",
      "Natalia Pérez-Campanero Antolín"
    ],
    "keywords": [
      "multi-agent systems",
      "personality traits",
      "cooperation dynamics",
      "Iterated Prisoner's Dilemma",
      "Big Five traits",
      "representation engineering",
      "agent coordination"
    ],
    "source_url": "https://arxiv.org/abs/2503.12722",
    "abstract_en_original": "How personality traits influence cooperation in large language models is explored using representation engineering to steer Big Five traits. Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation.",
    "resumen_es_original": "Se explora cómo los rasgos de personalidad influyen en la cooperación en modelos de lenguaje de gran escala utilizando ingeniería de representación para dirigir rasgos de los Cinco Grandes. Los resultados muestran que mayor Amabilidad y Responsabilidad mejoran la cooperación pero aumentan la susceptibilidad a la explotación.",
    "id": "article-105",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.704Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.704Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.12722",
      "fetched_title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
      "title_similarity": 0.5385,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering"
  },
  {
    "legacy_article_number": 106,
    "title_original": "The Impact of Big Five Personality Traits on AI Agent Decision-Making",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Mingjun Ren",
      "Wentao Xu"
    ],
    "keywords": [
      "Big Five personality",
      "AI agent decision-making",
      "social simulation",
      "multi-agent framework",
      "public spaces",
      "AgentVerse",
      "behavioral modeling"
    ],
    "source_url": "https://arxiv.org/abs/2503.15497",
    "abstract_en_original": "How Big Five personality traits of AI agents affect their decision generation in public open environments is investigated. The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework.",
    "resumen_es_original": "Se investiga cómo los rasgos de personalidad de los Cinco Grandes de los agentes de inteligencia artificial afectan su generación de decisiones en ambientes públicos abiertos. La simulación se realizó en un entorno de aula universitaria utilizando GPT-3.5-turbo con el marco AgentVerse.",
    "id": "article-106",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.723Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.723Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.15497",
      "fetched_title": "The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study"
  },
  {
    "legacy_article_number": 107,
    "title_original": "Signs of Consciousness in AI: Can GPT-3 Tell How Smart It Really Is?",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bojana Bojic",
      "Marina Jovanovic",
      "Bojana M. Dinic",
      "Ljubisa Bojic"
    ],
    "keywords": [
      "artificial intelligence",
      "consciousness",
      "GPT-3",
      "cognitive intelligence",
      "emotional intelligence",
      "self-awareness",
      "machine consciousness"
    ],
    "source_url": "https://www.nature.com/articles/s41599-024-04154-3",
    "abstract_en_original": "Objective and self-assessment tests of cognitive and emotional intelligence were administered to GPT-3. Results revealed GPT-3 outperformed average humans on cognitive intelligence tests, but its logical reasoning and emotional intelligence matched average human performance. GPT-3's self-assessments did not always align with objective performance.",
    "resumen_es_original": "Se administraron pruebas objetivas y de autoevaluación de inteligencia cognitiva y emocional a GPT-3. Los resultados revelaron que GPT-3 superó a los humanos promedio en pruebas de inteligencia cognitiva, pero su razonamiento lógico e inteligencia emocional coincidieron con el desempeño humano promedio. Las autoevaluaciones de GPT-3 no siempre se alinearon con el desempeño objetivo.",
    "id": "article-107",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.740Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.740Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41599-024-04154-3?error=cookies_not_supported&code=2f482550-dd91-4578-a72a-b2da20700f7e",
      "fetched_title": "Signs of consciousness in AI: Can GPT-3 tell how smart it really is? - Humanities and Social Sciences Communications",
      "title_similarity": 0.7222,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Signs of consciousness in AI: Can GPT-3 tell how smart it really is? - Humanities and Social Sciences Communications"
  },
  {
    "legacy_article_number": 108,
    "title_original": "An Evolutionary Model of Personality Traits Related to Cooperative Behavior Using LLM",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Reiji Suzuki",
      "Takaya Arita"
    ],
    "keywords": [
      "evolutionary computation",
      "personality traits",
      "cooperative behavior",
      "large language models",
      "game theory",
      "evolutionary dynamics",
      "behavioral traits"
    ],
    "source_url": "https://www.nature.com/articles/s41598-024-55903-y",
    "abstract_en_original": "An evolutionary model of personality traits related to cooperative behavior using large language models is proposed. Linguistic descriptions of personality traits are used as genes. The model exhibits evolution of cooperative behavior based on diverse and higher-order representation of personality traits.",
    "resumen_es_original": "Se propone un modelo evolutivo de rasgos de personalidad relacionados con comportamiento cooperativo utilizando modelos de lenguaje de gran escala. Las descripciones lingüísticas de rasgos de personalidad se utilizan como genes. El modelo exhibe evolución de comportamiento cooperativo basado en representación diversa y de orden superior de rasgos de personalidad.",
    "id": "article-108",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.757Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.757Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-55903-y?error=cookies_not_supported&code=dc5a7b27-8588-4c0c-8c0d-3d29a0462d88",
      "fetched_title": "An evolutionary model of personality traits related to cooperative behavior using a large language model - Scientific Reports",
      "title_similarity": 0.6875,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "An evolutionary model of personality traits related to cooperative behavior using a large language model - Scientific Reports"
  },
  {
    "legacy_article_number": 109,
    "title_original": "Cultural Bias and Cultural Alignment of Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yan Tao",
      "Olga Viberg",
      "Ryan S Baker",
      "René F Kizilcec"
    ],
    "keywords": [
      "cultural bias",
      "cultural alignment",
      "large language models",
      "cross-cultural values",
      "World Values Survey",
      "cultural prompting",
      "AI ethics"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgae346",
    "abstract_en_original": "All models exhibit cultural values resembling English-speaking and Protestant European countries. Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories.",
    "resumen_es_original": "Todos los modelos exhiben valores culturales que se asemejan a países anglófonos y europeos protestantes. Se prueba el uso de instrucciones culturales como estrategia de control para aumentar la alineación cultural, lo cual mejora la alineación para 71-81% de países/territorios.",
    "id": "article-109",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:35.021Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:35.021Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgae346",
      "fetched_title": "Cultural bias and cultural alignment of large language models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Cultural bias and cultural alignment of large language models"
  },
  {
    "legacy_article_number": 110,
    "title_original": "Artificial Intelligence, Human Cognition, and Conscious Supremacy",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Kenichiro Mogi"
    ],
    "keywords": [
      "consciousness",
      "artificial intelligence",
      "cognitive science",
      "conscious supremacy",
      "computational significance",
      "philosophy of mind",
      "neural correlates"
    ],
    "source_url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1364714/full",
    "abstract_en_original": "Salient ideas about computational significance of human conscious processes are reviewed, and cognitive domains potentially unique to consciousness are identified: flexible attention modulation, robust handling of new contexts, choice and decision making. Conscious supremacy is proposed as a concept analogous to quantum supremacy.",
    "resumen_es_original": "Se revisan ideas destacadas sobre la significancia computacional de los procesos conscientes humanos, y se identifican dominios cognitivos potencialmente únicos a la consciencia: modulación flexible de la atención, manejo robusto de nuevos contextos, elección y toma de decisiones. Se propone la supremacía consciente como un concepto análogo a la supremacía cuántica.",
    "id": "article-110",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.367Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.367Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1364714/full",
      "fetched_title": "Frontiers | Artificial intelligence, human cognition, and conscious supremacy",
      "title_similarity": 0.875,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Frontiers | Artificial intelligence, human cognition, and conscious supremacy"
  },
  {
    "legacy_article_number": 111,
    "title_original": "Designing Personality-Adaptive Conversational Agents for Mental Health Care",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Equipo de investigación en informática de salud mental"
    ],
    "keywords": [
      "personality-adaptive conversational agents",
      "mental health care",
      "therapeutic chatbots",
      "user personalization",
      "patient-centered design",
      "Big Five personality",
      "clinical applications"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8889396/",
    "abstract_en_original": "The concept of a personality-adaptive conversational agent (PACA) that can dynamically adjust its personality traits to better align with individual patient needs in therapeutic contexts is proposed. Based on established personality models and advances in natural language processing.",
    "resumen_es_original": "Se propone el concepto de un agente conversacional adaptativo a la personalidad (PACA) que puede ajustar dinámicamente sus rasgos de personalidad para alinearse mejor con las necesidades individuales del paciente en contextos terapéuticos. Basado en modelos de personalidad establecidos y avances en procesamiento de lenguaje natural.",
    "id": "article-111",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.778Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.778Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8889396/",
      "fetched_title": "Designing Personality-Adaptive Conversational Agents for Mental Health Care - PMC",
      "title_similarity": 0.9,
      "author_overlap": 0,
      "year_match": false
    },
    "title_canonical": "Designing Personality-Adaptive Conversational Agents for Mental Health Care - PMC"
  },
  {
    "legacy_article_number": 113,
    "title_original": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Adithya V Ganesan",
      "Yash Kumar Lal",
      "August Håkan Nilsson",
      "H. Andrew Schwartz"
    ],
    "keywords": [
      "personality estimation",
      "GPT-3",
      "zero-shot learning",
      "Big Five traits",
      "social media analysis",
      "human-level NLP",
      "psychometrics"
    ],
    "source_url": "https://aclanthology.org/2023.wassa-1.34/",
    "abstract_en_original": "The zero-shot ability of GPT-3 to estimate Big Five personality traits from social media posts is investigated. Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts.",
    "resumen_es_original": "Se investiga la capacidad de cero disparos de GPT-3 para estimar rasgos de personalidad de los Cinco Grandes a partir de publicaciones de redes sociales. Se encuentra que el rendimiento de cero disparos de GPT-3 es algo cercano al estado del arte preentrenado existente para clasificación amplia al inyectar conocimiento sobre el rasgo en las instrucciones.",
    "id": "article-113",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.845Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.845Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.wassa-1.34/",
      "fetched_title": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation"
  },
  {
    "legacy_article_number": 114,
    "title_original": "The Plasticity of ChatGPT's Mentalizing Abilities: Personalization for Personality Structures",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Dorit Hadar-Shoval",
      "Zohar Elyoseph",
      "Maya Lvovsky"
    ],
    "keywords": [
      "artificial intelligence",
      "borderline personality disorder",
      "emotional intelligence",
      "empathy",
      "emotional awareness",
      "Schizoid Personality Disorder",
      "mentalizing"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10503434/",
    "abstract_en_original": "ChatGPT's potential to generate mentalizing-like abilities tailored to specific personality structures was evaluated. ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies.",
    "resumen_es_original": "Se evaluó el potencial de ChatGPT para generar habilidades similares a la mentalización adaptadas a estructuras de personalidad específicas. ChatGPT describió con precisión las reacciones emocionales de individuos con trastorno límite de la personalidad como más intensas, complejas y ricas que aquellas con trastorno esquizoide de la personalidad, sugiriendo que puede generar respuestas similares a la mentalización consistentes con psicopatologías.",
    "id": "article-114",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.904Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.904Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10503434/",
      "fetched_title": "The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures - PMC",
      "title_similarity": 0.9091,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures - PMC"
  },
  {
    "legacy_article_number": 115,
    "title_original": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Guangyuan Jiang",
      "Manjie Xu",
      "Song-Chun Zhu",
      "Wenjuan Han",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "keywords": [
      "Machine Personality Inventory",
      "Big Five personality traits",
      "LLM evaluation",
      "personality prompting",
      "psychometric testing",
      "pre-trained language models",
      "behavioral assessment"
    ],
    "source_url": "https://arxiv.org/abs/2206.07550",
    "abstract_en_original": "The Machine Personality Inventory (MPI) tool for studying machine behaviors based on Big Five Personality Factors theory is introduced. By systematically evaluating large language models with MPI, first evidence demonstrating the efficacy of MPI in studying model behaviors is provided. The Personality Prompting (P^2) method is devised to induce models with specific personalities.",
    "resumen_es_original": "Se introduce la herramienta de Inventario de Personalidad de Máquina (MPI) para estudiar comportamientos de máquina basados en la teoría de los Factores de Personalidad de los Cinco Grandes. Al evaluar sistemáticamente modelos de lenguaje de gran escala con MPI, se proporciona la primera evidencia que demuestra la eficacia de MPI en estudiar comportamientos del modelo. Se diseña el método de Instrucciones de Personalidad (P^2) para inducir modelos con personalidades específicas.",
    "id": "article-115",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.037Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.037Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2206.07550",
      "fetched_title": "Evaluating and Inducing Personality in Pre-trained Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Evaluating and Inducing Personality in Pre-trained Language Models"
  },
  {
    "legacy_article_number": 117,
    "title_original": "Identifying and Manipulating the Personality Traits of Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Graham Caron",
      "Shashank Srivastava"
    ],
    "keywords": [
      "personality traits",
      "Big Five",
      "language model control",
      "persona manipulation",
      "BERT",
      "GPT-2",
      "personality consistency"
    ],
    "source_url": "https://arxiv.org/abs/2212.10276",
    "abstract_en_original": "Whether perceived personality in language models is exhibited consistently in their language generation is explored. When provided different types of contexts, language models such as BERT and GPT-2 are shown to consistently identify and reflect personality markers. This frames them as tools for identifying personality traits and controlling personas.",
    "resumen_es_original": "Se explora si la personalidad percibida en modelos de lenguaje se exhibe consistentemente en su generación de lenguaje. Se demuestra que cuando se proporcionan diferentes tipos de contextos, modelos de lenguaje como BERT y GPT-2 pueden identificar y reflejar marcadores de personalidad consistentemente. Esto los enmarca como herramientas para identificar rasgos de personalidad y controlar personas.",
    "id": "article-117",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.054Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.054Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2212.10276",
      "fetched_title": "Identifying and Manipulating the Personality Traits of Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Identifying and Manipulating the Personality Traits of Language Models"
  },
  {
    "legacy_article_number": 118,
    "title_original": "Estimating the Personality of White-Box Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Saketh Reddy Karra",
      "Son The Nguyen",
      "Theja Tulabandhula"
    ],
    "keywords": [
      "white-box language models",
      "Big Five personality",
      "zero-shot classification",
      "personality estimation",
      "open-ended text generation",
      "model anthropomorphism",
      "personality modification"
    ],
    "source_url": "https://arxiv.org/abs/2204.12000",
    "abstract_en_original": "Personality traits of several large-scale language models designed for open-ended text generation are explored. Building on Big Five factors, robust methods that quantify personality traits of these models and their underlying datasets are developed. Models are triggered with personality assessment questionnaires and text responses are classified into quantifiable traits.",
    "resumen_es_original": "Se exploran los rasgos de personalidad de varios modelos de lenguaje a gran escala diseñados para generación de texto abierto. Construyendo sobre los factores de los Cinco Grandes, se desarrollan métodos robustos que cuantifican los rasgos de personalidad de estos modelos y sus conjuntos de datos subyacentes. Se activan los modelos con cuestionarios de evaluación de personalidad y las respuestas de texto se clasifican en rasgos cuantificables.",
    "id": "article-118",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.068Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.068Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2204.12000",
      "fetched_title": "Estimating the Personality of White-Box Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Estimating the Personality of White-Box Language Models"
  },
  {
    "legacy_article_number": 119,
    "title_original": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours",
    "category": "Evaluación y validación psicométrica",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Elma Kerz",
      "Yu Qiao",
      "Sourabh Zanwar",
      "Daniel Wiechmann"
    ],
    "keywords": [
      "personality detection",
      "psycholinguistic features",
      "BERT transformers",
      "text contours",
      "Big Five",
      "MBTI",
      "verbal behavior analysis"
    ],
    "source_url": "https://arxiv.org/abs/2204.04629",
    "abstract_en_original": "Two major improvements in predicting personality traits from text are reported: (1) the most comprehensive set of theory-based psycholinguistic features and (2) hybrid models integrating pre-trained Transformer BERT and BLSTM networks trained on within-text distributions of psycholinguistic features. Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset.",
    "resumen_es_original": "Se reportan dos mejoras importantes en la predicción de rasgos de personalidad a partir de texto: (1) el conjunto más completo de características psicolingüísticas basadas en teoría y (2) modelos híbridos que integran el Transformer preentrenado BERT y redes BLSTM entrenadas en distribuciones intra-texto de características psicolingüísticas. Los modelos logran una mejora del 2.9% en el conjunto de datos Essay y del 8.28% en el conjunto de datos Kaggle MBTI.",
    "id": "article-119",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.083Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.083Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2204.04629",
      "fetched_title": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features",
      "title_similarity": 0.7857,
      "author_overlap": 0.1111,
      "year_match": true
    },
    "title_canonical": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features"
  },
  {
    "legacy_article_number": 120,
    "title_original": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "J. M. Diederik Kruijssen",
      "Nicholas Emmons"
    ],
    "keywords": [
      "Machine Learning",
      "Artificial Intelligence",
      "Computers and Society",
      "Human-Computer Interaction"
    ],
    "source_url": "https://arxiv.org/abs/2503.17085",
    "abstract_en_original": "The authors investigate whether AI systems can express deterministic and consistent personalities when instructed using established psychological frameworks. Their research reveals that advanced models like GPT-4o and o1 achieve the highest accuracy on Big Five and Myers-Briggs assessments. The study indicates that personality expression emerges from holistic reasoning rather than question-level optimization, and that fine-tuning affects communication style independently of personality accuracy. The researchers propose this capability could enhance human-AI interaction across education and healthcare applications.",
    "resumen_es_original": "Se investiga si los sistemas de inteligencia artificial pueden expresar personalidades deterministas y coherentes cuando se instruyen mediante marcos psicológicos establecidos. La investigación revela que modelos avanzados como GPT-4o y o1 logran la mayor precisión en evaluaciones Big Five y Myers-Briggs. El estudio indica que la expresión de personalidad emerge del razonamiento holístico en lugar de la optimización pregunta por pregunta, y que el ajuste fino afecta el estilo de comunicación independientemente de la precisión de personalidad. Los investigadores proponen que esta capacidad podría mejorar la interacción humano-IA en educación y aplicaciones de salud.",
    "id": "article-120",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.089Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.089Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.17085",
      "fetched_title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics"
  },
  {
    "legacy_article_number": 121,
    "title_original": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jingxuan Li",
      "Yuning Yang",
      "Shengqi Yang",
      "Linfan Zhang",
      "Ying Nian Wu"
    ],
    "keywords": [
      "Computation and Language",
      "ACL 2025"
    ],
    "source_url": "https://arxiv.org/abs/2411.11479",
    "abstract_en_original": "The researchers present a benchmark called Value-Spectrum designed to evaluate vision-language models using Schwartz's framework of human values. They constructed a database with over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels covering diverse topics. The study examines how these models handle value-oriented content and explores their capacity to adopt specific personas when explicitly prompted. The authors conclude that their benchmark offers potential for tracking VLM preferences in value-based tasks and persona simulation abilities. Code and data are available on GitHub.",
    "resumen_es_original": "Los investigadores presentan un benchmark denominado Value-Spectrum diseñado para evaluar modelos de visión-lenguaje mediante el marco de valores humanos de Schwartz. Construyeron una base de datos con más de 50,000 videos cortos de TikTok, YouTube Shorts e Instagram Reels que cubren temas diversos. El estudio examina cómo estos modelos manejan contenido orientado a valores y explora su capacidad para adoptar personas específicas cuando se les instruye explícitamente. Los autores concluyen que su benchmark ofrece potencial para rastrear preferencias de modelos de visión-lenguaje en tareas basadas en valores y capacidades de simulación de personas. El código y los datos están disponibles en GitHub.",
    "id": "article-121",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.099Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.099Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2411.11479",
      "fetched_title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts"
  },
  {
    "legacy_article_number": 122,
    "title_original": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yifan Duan",
      "Yihong Tang",
      "Xuefeng Bai",
      "Kehai Chen",
      "Juntao Li",
      "Min Zhang"
    ],
    "keywords": [
      "Computation and Language"
    ],
    "source_url": "https://arxiv.org/abs/2502.20859",
    "abstract_en_original": "The researchers investigate how personality traits influence LLM agent performance using human simulation methodology. They examine three primary questions regarding how personality shapes problem-solving in structured tasks, creativity in open-ended tasks, and collaborative dynamics. The study assigns Big Five personality characteristics to LLM agents, revealing that specific traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks. Additionally, the research demonstrates that multi-agent teams develop collective intelligence distinct from individual capabilities depending on personality compositions.",
    "resumen_es_original": "Los investigadores examinan cómo los rasgos de personalidad influyen en el desempeño de agentes LLM mediante metodología de simulación humana. Analizan tres cuestiones principales respecto a cómo la personalidad moldea la resolución de problemas en tareas estructuradas, la creatividad en tareas abiertas y las dinámicas colaborativas. El estudio asigna características de personalidad Big Five a agentes LLM, revelando que rasgos específicos influyen significativamente en la precisión del razonamiento en tareas cerradas y el resultado creativo en tareas abiertas. Adicionalmente, la investigación demuestra que equipos multi-agente desarrollan inteligencia colectiva distinta de las capacidades individuales dependiendo de las composiciones de personalidad.",
    "id": "article-122",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.105Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.105Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.20859",
      "fetched_title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents"
  },
  {
    "legacy_article_number": 124,
    "title_original": "Psychologically Enhanced AI Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Maciej Besta",
      "Shriram Chandran",
      "Robert Gerstenberger",
      "Mathis Lindner",
      "Marcin Chrapek",
      "Sebastian Hermann Martschat",
      "Taraneh Ghandi",
      "Patrick Iff",
      "Hubert Niewiadomski",
      "Piotr Nyczyk",
      "Jürgen Müller",
      "Torsten Hoefler"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Human-Computer Interaction",
      "Multiagent Systems"
    ],
    "source_url": "https://arxiv.org/abs/2509.04343",
    "abstract_en_original": "The researchers present MBTI-in-Thoughts, a framework that enhances LLM agents through personality-based prompt engineering grounded in Myers-Briggs Type Indicator psychology. The method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology: cognition and affect. The work demonstrates that emotionally-oriented agents perform better at narrative tasks while analytically-primed agents show more stable strategies in game-theoretic contexts. The framework supports multi-agent communication and self-reflection protocols. Personality consistency is verified through the official 16Personalities test. The approach generalizes to other psychological frameworks including Big Five, HEXACO, and Enneagram, requiring no fine-tuning.",
    "resumen_es_original": "Los investigadores presentan MBTI-in-Thoughts, un marco que mejora agentes LLM mediante ingeniería de instrucciones basada en personalidad fundamentada en la psicología del Indicador de Tipo Myers-Briggs. El método prepara agentes con arquetipos de personalidad distintos mediante ingeniería de instrucciones, permitiendo control sobre el comportamiento a lo largo de dos ejes fundamentales de la psicología humana: cognición y afecto. El trabajo demuestra que agentes orientados emocionalmente se desempeñan mejor en tareas narrativas mientras que agentes preparados analíticamente muestran estrategias más estables en contextos de teoría de juegos. El marco soporta protocolos de comunicación multi-agente y auto-reflexión. La coherencia de personalidad se verifica mediante el test oficial 16Personalities. El enfoque se generaliza a otros marcos psicológicos incluyendo Big Five, HEXACO y Eneagrama, sin requerir ajuste fino.",
    "id": "article-124",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.116Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.116Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.04343",
      "fetched_title": "Psychologically Enhanced AI Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Psychologically Enhanced AI Agents"
  },
  {
    "legacy_article_number": 125,
    "title_original": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Haoyuan Li",
      "Yuanbo Tong",
      "Yuchen Li",
      "Zirui Wang",
      "Chunhou Liu",
      "Jiamou Liu"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence"
    ],
    "source_url": "https://arxiv.org/abs/2511.00115",
    "abstract_en_original": "The research proposes ProtoMBTI, a framework treating personality classification through the lens of prototype theory rather than traditional hard-label approaches. The methodology involves constructing a quality-controlled text corpus through LLM-guided augmentation across semantic, linguistic, and sentiment dimensions. A lightweight encoder of 2B parameters or fewer undergoes LoRA fine-tuning to develop discriminative embeddings and standardize personality prototypes. At inference, the system retrieves relevant prototypes and executes a retrieve-reuse-revise-retain cycle, where prototype evidence gets aggregated through voting, inconsistencies trigger revision, and successful predictions enrich the prototype library for continuous improvement. The framework demonstrates enhanced performance on MBTI tasks across multiple benchmarks while providing stronger interpretability and cross-dataset transferability compared to existing approaches.",
    "resumen_es_original": "La investigación propone ProtoMBTI, un marco que trata la clasificación de personalidad mediante la teoría de prototipos en lugar de enfoques tradicionales de etiquetas rígidas. La metodología implica construir un corpus de texto controlado en calidad mediante aumento guiado por LLM a través de dimensiones semánticas, lingüísticas y de sentimiento. Un codificador ligero de 2B parámetros o menos experimenta ajuste fino LoRA para desarrollar incrustaciones discriminativas y estandarizar prototipos de personalidad. En la inferencia, el sistema recupera prototipos relevantes y ejecuta un ciclo recuperar-reusar-revisar-retener, donde la evidencia de prototipos se agrega mediante votación, las inconsistencias activan revisión, y las predicciones exitosas enriquecen la biblioteca de prototipos para mejora continua. El marco demuestra desempeño mejorado en tareas MBTI a través de múltiples benchmarks mientras proporciona mayor interpretabilidad y transferibilidad entre conjuntos de datos comparado con enfoques existentes.",
    "id": "article-125",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.123Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.123Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.00115",
      "fetched_title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference"
  },
  {
    "legacy_article_number": 126,
    "title_original": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jia Li",
      "Yichao He",
      "Jiacheng Xu",
      "Tianhao Luo",
      "Zhenzhen Hu",
      "Richang Hong",
      "Meng Wang"
    ],
    "keywords": [
      "Computation and Language",
      "Multimedia",
      "ACM MM 2025"
    ],
    "source_url": "https://arxiv.org/abs/2507.22367",
    "abstract_en_original": "The researchers developed a framework addressing personality assessment challenges. They note that personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors. Their approach uses psychology-informed prompts with large language models and implements a Text-Centric Trait Fusion Network that integrates multimodal signals. Key innovations include personality-specific LLM prompting and audio-visual feature extraction. Results showed approximately 45% MSE reduction, with the method ranking first in the AVI Challenge 2025 Personality Assessment track. Source code availability is planned through GitHub.",
    "resumen_es_original": "Los investigadores desarrollaron un marco que aborda desafíos de evaluación de personalidad. Notan que los rasgos de personalidad son estables, frecuentemente filtrados subconscientemente a través del lenguaje, expresiones faciales y comportamientos corporales. Su enfoque utiliza instrucciones informadas por psicología con modelos de lenguaje de gran escala e implementa una Red de Fusión de Rasgos Centrada en Texto que integra señales multimodales. Las innovaciones clave incluyen instrucciones LLM específicas de personalidad y extracción de características audio-visuales. Los resultados mostraron aproximadamente 45% de reducción en MSE, con el método clasificándose primero en la pista de Evaluación de Personalidad del AVI Challenge 2025. La disponibilidad del código fuente está planificada a través de GitHub.",
    "id": "article-126",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.133Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.133Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2507.22367",
      "fetched_title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors"
  },
  {
    "legacy_article_number": 127,
    "title_original": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Tian Ma",
      "Kaiyu Feng",
      "Yu Rong",
      "Kangfei Zhao"
    ],
    "keywords": [
      "Computation and Language",
      "Social and Information Networks",
      "CIKM 2025"
    ],
    "source_url": "https://arxiv.org/abs/2509.04461",
    "abstract_en_original": "The research addresses personality prediction from social media through the Myers Briggs Type Indicator (MBTI). The authors developed PostToPersonality (PtoP), an LLM-based framework tackling two primary challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population. The approach employs Retrieval Augmented Generation with in-context learning to reduce hallucinations, while fine-tuning uses synthetic minority oversampling to handle class imbalance. Testing on real-world social media data demonstrates that PtoP achieves state of the art performance compared with 10 ML and DL baselines.",
    "resumen_es_original": "La investigación aborda la predicción de personalidad desde redes sociales mediante el Indicador de Tipo Myers Briggs (MBTI). Los autores desarrollaron PostToPersonality (PtoP), un marco basado en LLM que aborda dos desafíos principales: el problema de alucinación inherente en LLMs y la distribución naturalmente desbalanceada de tipos MBTI en la población. El enfoque emplea Generación Aumentada por Recuperación con aprendizaje en contexto para reducir alucinaciones, mientras que el ajuste fino utiliza sobremuestreo sintético de minorías para manejar el desbalance de clases. Las pruebas en datos de redes sociales del mundo real demuestran que PtoP logra desempeño de vanguardia comparado con 10 baselines de ML y DL.",
    "id": "article-127",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.140Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.140Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.04461",
      "fetched_title": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media"
  },
  {
    "legacy_article_number": 128,
    "title_original": "Exploring the Personality Traits of LLMs through Latent Features Steering",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Shu Yang",
      "Shenzhe Zhu",
      "Liang Liu",
      "Lijie Hu",
      "Mengdi Li",
      "Di Wang"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence"
    ],
    "source_url": "https://arxiv.org/abs/2410.10863",
    "abstract_en_original": "The research examines how LLMs develop personality characteristics, investigating factors like cultural norms and environmental stressors that shape these traits. The authors propose a methodology that modifies model behavior by extracting and steering internal features, circumventing the need for retraining. Their approach draws from social determinism theory to understand personality expression. The team also evaluates safety implications through the lens of personality traits, addressing how these underlying factors influence model behavior and safety concerns.",
    "resumen_es_original": "La investigación examina cómo los LLMs desarrollan características de personalidad, investigando factores como normas culturales y estresores ambientales que moldean estos rasgos. Los autores proponen una metodología que modifica el comportamiento del modelo extrayendo y dirigiendo características internas, evitando la necesidad de reentrenamiento. Su enfoque se basa en la teoría del determinismo social para comprender la expresión de personalidad. El equipo también evalúa las implicaciones de seguridad a través del prisma de los rasgos de personalidad, abordando cómo estos factores subyacentes influyen en el comportamiento del modelo y las preocupaciones de seguridad.",
    "id": "article-128",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.149Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.149Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2410.10863",
      "fetched_title": "Exploring the Personality Traits of LLMs through Latent Features Steering",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Exploring the Personality Traits of LLMs through Latent Features Steering"
  },
  {
    "legacy_article_number": 130,
    "title_original": "A-MEM: Agentic Memory for LLM Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wujiang Xu",
      "Zujie Liang",
      "Kai Mei",
      "Hang Gao",
      "Juntao Tan",
      "Yongfeng Zhang"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Multiagent Systems"
    ],
    "source_url": "https://arxiv.org/abs/2502.12110",
    "abstract_en_original": "This research presents A-MEM, a framework that enables LLM agents to develop and manage episodic memories using metacognitive reasoning. The system allows agents to autonomously decide what experiences to store, how to organize memories, and when to retrieve them. The authors demonstrate that A-MEM enables agents to form consistent personality traits over extended interactions, maintain coherent behavioral patterns, and adapt their responses based on accumulated experiences. The framework was evaluated across multiple scenarios including social interactions and task-oriented dialogues, showing improved long-term consistency and personalization compared to baseline memory systems.",
    "resumen_es_original": "Esta investigación presenta A-MEM, un marco que permite a los agentes LLM desarrollar y gestionar memorias episódicas mediante razonamiento metacognitivo. El sistema permite que los agentes decidan autónomamente qué experiencias almacenar, cómo organizar memorias y cuándo recuperarlas. Los autores demuestran que A-MEM permite a los agentes formar rasgos de personalidad consistentes a lo largo de interacciones extendidas, mantener patrones conductuales coherentes y adaptar sus respuestas basándose en experiencias acumuladas. El marco fue evaluado en múltiples escenarios incluyendo interacciones sociales y diálogos orientados a tareas, mostrando mejor consistencia a largo plazo y personalización comparado con sistemas de memoria baseline.",
    "id": "article-130",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:35.312Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:35.312Z",
      "checks": [
        "arxiv_metadata_checked",
        "arxiv_title_search_checked",
        "verified"
      ],
      "reason": "verified_arxiv_title_search",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.12110",
      "fetched_title": "A-MEM: Agentic Memory for LLM Agents",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "A-MEM: Agentic Memory for LLM Agents"
  },
  {
    "legacy_article_number": 134,
    "title_original": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Julia Kharchenko",
      "Tanya Roosta",
      "Aman Chadha",
      "Chirag Shah"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society"
    ],
    "source_url": "https://arxiv.org/abs/2406.14805",
    "abstract_en_original": "The researchers evaluate how well LLMs represent cultural values using Hofstede's cultural dimensions framework. The study systematically prompts multiple LLMs to adopt personas from different cultural backgrounds and assesses whether their responses align with established cultural value patterns. Testing includes GPT-4, Claude, and other frontier models across dimensions such as individualism-collectivism, power distance, and uncertainty avoidance. Results indicate that while LLMs can adjust some surface-level cultural markers, they struggle to consistently embody deep-rooted cultural values, particularly for non-Western cultures. The work emphasizes the need for culturally-aware training approaches beyond simple multilingual data inclusion.",
    "resumen_es_original": "Los investigadores evalúan qué tan bien los LLMs representan valores culturales usando el marco de dimensiones culturales de Hofstede. El estudio instruye sistemáticamente a múltiples LLMs para adoptar personas de diferentes trasfondos culturales y evalúa si sus respuestas se alinean con patrones de valores culturales establecidos. Las pruebas incluyen GPT-4, Claude y otros modelos de frontera a través de dimensiones como individualismo-colectivismo, distancia de poder y evitación de incertidumbre. Los resultados indican que mientras los LLMs pueden ajustar algunos marcadores culturales superficiales, tienen dificultades para encarnar consistentemente valores culturales arraigados, particularmente para culturas no occidentales. El trabajo enfatiza la necesidad de enfoques de entrenamiento culturalmente conscientes más allá de la simple inclusión de datos multilingües.",
    "id": "article-134",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:59:35.314Z",
    "evidence": {
      "checked_at": "2026-02-13T18:59:35.314Z",
      "checks": [
        "arxiv_metadata_checked",
        "arxiv_title_search_checked",
        "verified"
      ],
      "reason": "verified_arxiv_title_search",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.14805",
      "fetched_title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions"
  },
  {
    "legacy_article_number": 135,
    "title_original": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Cheng Li",
      "Mengzhou Chen",
      "Jindong Wang",
      "Sunayana Sitaram",
      "Xing Xie"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2402.10946",
    "abstract_en_original": "This paper presents CultureLLM, a framework designed to embed cultural knowledge into Large Language Models to improve their cross-cultural understanding and generation capabilities. The authors create a cultural knowledge base covering multiple dimensions including values, norms, communication styles, and behavioral expectations across diverse cultures. The framework incorporates this knowledge through specialized fine-tuning and prompting strategies. Evaluation across cultural reasoning tasks and personality assessments shows that CultureLLM-enhanced models produce more culturally appropriate responses and can better simulate personality traits consistent with specific cultural contexts compared to baseline models.",
    "resumen_es_original": "Este artículo presenta CultureLLM, un marco diseñado para incorporar conocimiento cultural en Modelos de Lenguaje de Gran Escala para mejorar sus capacidades de comprensión y generación intercultural. Los autores crean una base de conocimiento cultural que cubre múltiples dimensiones incluyendo valores, normas, estilos de comunicación y expectativas conductuales a través de culturas diversas. El marco incorpora este conocimiento mediante estrategias especializadas de ajuste fino e instrucción. La evaluación a través de tareas de razonamiento cultural y evaluaciones de personalidad muestra que modelos mejorados con CultureLLM producen respuestas más culturalmente apropiadas y pueden simular mejor rasgos de personalidad consistentes con contextos culturales específicos comparado con modelos baseline.",
    "id": "article-135",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.193Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.193Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.10946",
      "fetched_title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "CultureLLM: Incorporating Cultural Differences into Large Language Models"
  },
  {
    "legacy_article_number": 149,
    "title_original": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yashar Deldjoo",
      "Tommaso Di Noia"
    ],
    "keywords": [
      "Information Retrieval",
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society"
    ],
    "source_url": "https://doi.org/10.1145/3725853",
    "abstract_en_original": "This work presents CFaiRLLM, a specialized framework for evaluating consumer-side fairness in LLM-based recommendation systems that utilize personality profiling. The authors argue that traditional fairness metrics fail to capture harms specific to personality-based personalization, such as the exploitation of personality traits (e.g., targeting high neuroticism users with anxiety-inducing content to increase engagement). The framework introduces personality-aware fairness metrics and evaluates recommendation systems built on GPT-4, Claude, and other LLMs. Findings demonstrate that personality-targeted recommendations can systematically disadvantage users with certain psychological profiles, raising ethical concerns about the deployment of such systems without appropriate safeguards.",
    "resumen_es_original": "Este trabajo presenta CFaiRLLM, un marco especializado para evaluar equidad del lado del consumidor en sistemas de recomendación basados en LLM que utilizan perfilado de personalidad. Los autores argumentan que las métricas de equidad tradicionales fallan en capturar daños específicos a personalización basada en personalidad, como la explotación de rasgos de personalidad (por ejemplo, dirigirse a usuarios con alto neuroticismo con contenido inductor de ansiedad para aumentar compromiso). El marco introduce métricas de equidad conscientes de personalidad y evalúa sistemas de recomendación construidos sobre GPT-4, Claude y otros LLMs. Los hallazgos demuestran que recomendaciones dirigidas a personalidad pueden sistemáticamente desfavorecer a usuarios con ciertos perfiles psicológicos, planteando preocupaciones éticas sobre el despliegue de tales sistemas sin salvaguardas apropiadas.",
    "id": "article-149",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.198Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.198Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1145/3725853",
      "fetched_title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System"
  },
  {
    "legacy_article_number": 150,
    "title_original": "Emotion Recognition in Conversation via Dynamic Personality Representation Learning",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Linh The Nguyen",
      "Dat Ngo",
      "Anh Thu Nguyen",
      "Cong-Tinh Dao",
      "Thien Huu Nguyen"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Human-Computer Interaction"
    ],
    "source_url": "https://aclanthology.org/2024.lrec-main.507/",
    "abstract_en_original": "This research proposes a novel approach to emotion recognition in conversations by incorporating dynamic personality representation learning. The authors argue that understanding a speaker's personality traits is crucial for accurately interpreting their emotional expressions, as personality influences how emotions are communicated. The framework learns to extract and update personality representations dynamically throughout conversations, using these representations to improve emotion classification. Experiments on benchmark conversational datasets demonstrate that personality-aware emotion recognition significantly outperforms methods that ignore speaker personality. The work has implications for developing more psychologically grounded conversational AI systems including those based on Large Language Models.",
    "resumen_es_original": "Esta investigación propone un enfoque novedoso para reconocimiento de emociones en conversaciones mediante la incorporación de aprendizaje de representación de personalidad dinámica. Los autores argumentan que comprender los rasgos de personalidad de un hablante es crucial para interpretar con precisión sus expresiones emocionales, ya que la personalidad influye en cómo se comunican las emociones. El marco aprende a extraer y actualizar representaciones de personalidad dinámicamente a lo largo de conversaciones, usando estas representaciones para mejorar la clasificación de emociones. Experimentos en conjuntos de datos conversacionales de referencia demuestran que el reconocimiento de emociones consciente de personalidad supera significativamente a métodos que ignoran la personalidad del hablante. El trabajo tiene implicaciones para desarrollar sistemas de IA conversacional más fundamentados psicológicamente incluyendo aquellos basados en Modelos de Lenguaje de Gran Escala.",
    "id": "article-150",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.199Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.199Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.lrec-main.507/",
      "fetched_title": "Emotion Recognition in Conversation via Dynamic Personality",
      "title_similarity": 0.7778,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Emotion Recognition in Conversation via Dynamic Personality"
  },
  {
    "legacy_article_number": 151,
    "title_original": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tianyu Zhao",
      "Siqi Li",
      "Yasser Shoukry",
      "Salma Elmalaki"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2602.07181",
    "abstract_en_original": "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.",
    "resumen_es_original": "Este artículo, titulado \"Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-151",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.207Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.207Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.07181",
      "fetched_title": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs"
  },
  {
    "legacy_article_number": 152,
    "title_original": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Deuksin Kwon",
      "Kaleen Shrestha",
      "Bin Han",
      "Spencer Lin",
      "James Hale",
      "Jonathan Gratch",
      "Maja Matarić",
      "Gale M. Lucas"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2602.07414",
    "abstract_en_original": "Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.",
    "resumen_es_original": "Este artículo, titulado \"Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-152",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.223Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.223Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.07414",
      "fetched_title": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution"
  },
  {
    "legacy_article_number": 153,
    "title_original": "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jingshu Li",
      "Tianqi Song",
      "Nattapat Boonprakong",
      "Zicheng Zhu",
      "Yitian Yang",
      "Yi-Chieh Lee"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.12727",
    "abstract_en_original": "Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits. To explore the possibility, we conducted a randomized behavioral experiment. Our results indicate that after conversations about personal topics with an LLM-based AI chatbot using GPT-4o default personality traits, users' self-concepts aligned with the AI's measured personality traits. The longer the conversation, the greater the alignment. This alignment led to increased homogeneity in self-concepts among users. We also observed that the degree of self-concept alignment was positively associated with users' conversation enjoyment. Our findings uncover how AI personality traits can shape users' self-concepts through human-AI conversation, highlighting both risks and opportunities. We provide important design implications for developing more responsible and ethical AI systems.",
    "resumen_es_original": "Este artículo, titulado \"AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-153",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.240Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.240Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.12727",
      "fetched_title": "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations"
  },
  {
    "legacy_article_number": 154,
    "title_original": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Xiaoxu Ma",
      "Xiangbo Zhang",
      "Zhenyu Weng"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2601.09833",
    "abstract_en_original": "Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.",
    "resumen_es_original": "Este artículo, titulado \"Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-154",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.257Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.257Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.09833",
      "fetched_title": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations"
  },
  {
    "legacy_article_number": 155,
    "title_original": "Structured Personality Control and Adaptation for LLM Agents",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jinpeng Wang",
      "Xinyu Jia",
      "Wei Wei Heng",
      "Yuquan Li",
      "Binbin Shi",
      "Qianlei Chen",
      "Guannan Chen",
      "Junxia Zhang",
      "Yuyu Yin"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.10025",
    "abstract_en_original": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.",
    "resumen_es_original": "Este artículo, titulado \"Structured Personality Control and Adaptation for LLM Agents\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-155",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.271Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.271Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.10025",
      "fetched_title": "Structured Personality Control and Adaptation for LLM Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Structured Personality Control and Adaptation for LLM Agents"
  },
  {
    "legacy_article_number": 156,
    "title_original": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jiongchi Yu",
      "Yuhan Ma",
      "Xiaoyu Zhang",
      "Junjie Wang",
      "Qiang Hu",
      "Chao Shen",
      "Xiaofei Xie"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2602.00016",
    "abstract_en_original": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.",
    "resumen_es_original": "Este artículo, titulado \"PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-156",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.287Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.287Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.00016",
      "fetched_title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems"
  },
  {
    "legacy_article_number": 157,
    "title_original": "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jacopo Amidei",
      "Gregorio Ferreira",
      "Mario Muñoz Serrano",
      "Rubén Nieto",
      "Andreas Kaltenbrunner"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2602.03334",
    "abstract_en_original": "This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. Using five LLMs, we first assess the representativeness and potential biases in the sociodemographic attributes of the generated personas, as well as their alignment with the intended personality traits. While LLMs successfully reproduce known correlations between personality and sociodemographic variables, all models exhibit pronounced WEIRD (western, educated, industrialized, rich and democratic) biases, favoring young, educated, white, heterosexual, Western individuals with centrist or progressive political views and secular or Christian beliefs. In a second analysis, we manipulate input traits to maximize Neuroticism and Psychoticism scores. Notably, when Psychoticism is maximized, several models produce an overrepresentation of non-binary and LGBTQ+ identities, raising concerns about stereotyping and the potential pathologization of marginalized groups. Our findings highlight both the potential and the risks of using LLMs to generate psychologically grounded synthetic populations.",
    "resumen_es_original": "Este artículo, titulado \"The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-157",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.303Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.303Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.03334",
      "fetched_title": "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas"
  },
  {
    "legacy_article_number": 158,
    "title_original": "Multi-Persona Thinking for Bias Mitigation in Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Yuxing Chen",
      "Guoqing Luo",
      "Zijun Wu",
      "Lili Mou"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.15488",
    "abstract_en_original": "Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.",
    "resumen_es_original": "Este artículo, titulado \"Multi-Persona Thinking for Bias Mitigation in Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-158",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.319Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.319Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.15488",
      "fetched_title": "Multi-Persona Thinking for Bias Mitigation in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Multi-Persona Thinking for Bias Mitigation in Large Language Models"
  },
  {
    "legacy_article_number": 159,
    "title_original": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Adib Sakhawat",
      "Tahsin Islam",
      "Takia Farhin",
      "Syed Rifat Raiyan",
      "Hasan Mahmud",
      "Md Kamrul Hasan"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Bias",
      "Fairness"
    ],
    "source_url": "https://arxiv.org/abs/2601.06194",
    "abstract_en_original": "As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \\approx 27{,}000$). Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort. Alignment signals appear to be consistent architectural traits rather than stochastic noise ($η^2 &gt; 0.90$); however, we identify substantial discrepancies in measurement validity. In particular, the Political Compass exhibits a strong negative correlation with cultural progressivism ($r=-0.64$) when compared against multi-axial instruments, suggesting a conflation of social conservatism with authoritarianism in this context. We further observe a significant divergence between open-weights and closed-source models, with the latter displaying markedly higher cultural progressivism scores ($p&lt;10^{-25}$). In downstream media analysis, models exhibit a systematic \"center-shift,\" frequently categorizing neutral articles as left-leaning, alongside an asymmetric detection capability in which \"Far Left\" content is identified with greater accuracy (19.2%) than \"Far Right\" content (2.0%). These findings suggest that single-axis evaluations are insufficient and that multidimensional auditing frameworks are necessary to characterize alignment behavior in deployed LLMs. Our code and data will be made public.",
    "resumen_es_original": "Este artículo, titulado \"Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-159",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.334Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.334Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.06194",
      "fetched_title": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias"
  },
  {
    "legacy_article_number": 160,
    "title_original": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Bin Han",
      "Deuksin Kwon",
      "Jonathan Gratch"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2602.01063",
    "abstract_en_original": "Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.",
    "resumen_es_original": "Este artículo, titulado \"Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-160",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.351Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.351Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.01063",
      "fetched_title": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents"
  },
  {
    "legacy_article_number": 161,
    "title_original": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tamunotonye Harry",
      "Ivoline Ngong",
      "Chima Nweke",
      "Yuanyuan Feng",
      "Joseph Near"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.15395",
    "abstract_en_original": "User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\\% is within-person(state) while only 26\\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.",
    "resumen_es_original": "Este artículo, titulado \"Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-161",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.367Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.367Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.15395",
      "fetched_title": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind"
  },
  {
    "legacy_article_number": 162,
    "title_original": "Effects of personality steering on cooperative behavior in Large Language Model agents",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Mizuki Sakai",
      "Mizuki Yokoyama",
      "Wakaba Tateishi",
      "Genki Ichinose"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Bias",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2601.05302",
    "abstract_en_original": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
    "resumen_es_original": "Este artículo, titulado \"Effects of personality steering on cooperative behavior in Large Language Model agents\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-162",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.381Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.381Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.05302",
      "fetched_title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Effects of personality steering on cooperative behavior in Large Language Model agents"
  },
  {
    "legacy_article_number": 163,
    "title_original": "Persona Prompting as a Lens on LLM Social Reasoning",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jing Yang",
      "Moritz Hechtbauer",
      "Elisabeth Khalilov",
      "Evelyn Luise Brinkmann",
      "Vera Schmitt",
      "Nils Feldhus"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.20757",
    "abstract_en_original": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.",
    "resumen_es_original": "Este artículo, titulado \"Persona Prompting as a Lens on LLM Social Reasoning\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-163",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.396Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.396Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.20757",
      "fetched_title": "Persona Prompting as a Lens on LLM Social Reasoning",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Persona Prompting as a Lens on LLM Social Reasoning"
  },
  {
    "legacy_article_number": 164,
    "title_original": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Francesco Chiappone",
      "Davide Marocco",
      "Nicola Milano"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.15319",
    "abstract_en_original": "Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.",
    "resumen_es_original": "Este artículo, titulado \"Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-164",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.410Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.410Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.15319",
      "fetched_title": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles"
  },
  {
    "legacy_article_number": 165,
    "title_original": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Candida M. Greco",
      "Lucio La Cava",
      "Andrea Tagarelli"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Cultural Values",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.22396",
    "abstract_en_original": "Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.",
    "resumen_es_original": "Este artículo, titulado \"Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-165",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.429Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.429Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.22396",
      "fetched_title": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks"
  },
  {
    "legacy_article_number": 166,
    "title_original": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Hongliu Cao",
      "Eoin Thomas",
      "Rodrigo Acuna Agost"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Fairness",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2602.00044",
    "abstract_en_original": "Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.",
    "resumen_es_original": "Este artículo, titulado \"When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-166",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.447Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.447Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.00044",
      "fetched_title": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications"
  },
  {
    "legacy_article_number": 167,
    "title_original": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Viswonathan Manoranjan",
      "Snehalkumar `Neil' S. Gaikwad"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.10102",
    "abstract_en_original": "Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.",
    "resumen_es_original": "Este artículo, titulado \"When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-167",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.462Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.462Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.10102",
      "fetched_title": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making"
  },
  {
    "legacy_article_number": 168,
    "title_original": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Donya Rooein",
      "Sankalan Pal Chowdhury",
      "Mariia Eremeeva",
      "Yuan Qin",
      "Debora Nozza",
      "Mrinmaya Sachan",
      "Dirk Hovy"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2601.08402",
    "abstract_en_original": "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",
    "resumen_es_original": "Este artículo, titulado \"PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-168",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.477Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.477Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.08402",
      "fetched_title": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors"
  },
  {
    "legacy_article_number": 169,
    "title_original": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Dominik P. Hofer",
      "David Haag",
      "Rania Islambouli",
      "Jan D. Smeddinck"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2602.06596",
    "abstract_en_original": "Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Using ordinal multilevel models with within-between decomposition, we distinguish trial-level effects, whether personality information improves evaluations of individual messages, from person-level exposure effects, whether participants receiving higher proportions of personality-informed messages exhibit systematically different overall perceptions. Results showed no trial-level associations, but participants who received higher proportions of BFPT-informed messages rated the messages as more personalised, appropriate, and reported less negative affect. We use Communication Accommodation Theory for post-hoc analysis. These results suggest that personality-based personalisation in behaviour change systems may operate primarily through aggregate exposure rather than per-message optimisation, with implications for how adaptive systems are designed and evaluated in sustained human-AI interaction. In-situ longitudinal studies are needed to validate these findings in real-world contexts.",
    "resumen_es_original": "Este artículo, titulado \"Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-169",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.491Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.491Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.06596",
      "fetched_title": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging"
  },
  {
    "legacy_article_number": 170,
    "title_original": "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jana Gonnermann-Müller",
      "Jennifer Haase",
      "Nicolas Leins",
      "Thomas Kosch",
      "Sebastian Pokutta"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.22812",
    "abstract_en_original": "Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. We address this point using a dual-assessment framework measuring both self-reported characteristics and observer-rated persona expression. Across two experiments testing four persona conditions (default, high, moderate, and low ADHD presentations), seven LLMs, and three semantically equivalent persona prompts, we examine between-conversation stability (3,473 conversations) and within-conversation stability (1,370 conversations and 18 turns). Self-reports remain highly stable both between and within conversations. However, observer ratings reveal a tendency for persona expressions to decline during extended conversations. These findings suggest that persona-instructed LLMs produce stable, persona-aligned self-reports, an important prerequisite for behavioral research, while identifying this regression tendency as a boundary condition for multi-agent social simulation.",
    "resumen_es_original": "Este artículo, titulado \"Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-170",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.504Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.504Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.22812",
      "fetched_title": "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation"
  },
  {
    "legacy_article_number": 171,
    "title_original": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tassallah Abdullahi",
      "Shrestha Ghosh",
      "Hamish S Fraser",
      "Daniel León Tramontini",
      "Adeel Abbasi",
      "Ghada Bourjeily",
      "Carsten Eickhoff",
      "Ritambhara Singh"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.05376",
    "abstract_en_original": "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.",
    "resumen_es_original": "Este artículo, titulado \"The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-171",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.509Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.509Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.05376",
      "fetched_title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models"
  },
  {
    "legacy_article_number": 172,
    "title_original": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Yuan Cao",
      "Feixiang Liu",
      "Xinyue Wang",
      "Yihan Zhu",
      "Hui Xu",
      "Zheng Wang",
      "Qiang Qiu"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "MBTI",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.18582",
    "abstract_en_original": "Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.",
    "resumen_es_original": "Este artículo, titulado \"From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-172",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.520Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.520Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.18582",
      "fetched_title": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection"
  },
  {
    "legacy_article_number": 173,
    "title_original": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wei Xie",
      "Shuoyoucheng Ma",
      "Zhenhua Wang",
      "Enze Wang",
      "Kai Chen",
      "Xiaobing Sun",
      "Baosheng Wang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Bias",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2509.16530",
    "abstract_en_original": "Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM. Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans. This results in high rejection rates when human scales are reused directly. Furthermore, these scales do not support the measurement of LLM psychological property variations in different languages. This paper introduces AIPsychoBench, a specialized benchmark tailored to assess the psychological properties of LLM. It uses a lightweight role-playing prompt to bypass LLM alignment, improving the average effective response rate from 70.12% to 90.40%. Meanwhile, the average biases are only 3.3% (positive) and 2.1% (negative), which are significantly lower than the biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts. Furthermore, among the total of 112 psychometric subcategories, the score deviations for seven languages compared to English ranged from 5% to 20.2% in 43 subcategories, providing the first comprehensive evidence of the linguistic impact on the psychometrics of LLM.",
    "resumen_es_original": "Este artículo, titulado \"AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-173",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.525Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.525Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.16530",
      "fetched_title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans"
  },
  {
    "legacy_article_number": 174,
    "title_original": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Peiyu Li",
      "Xiuxiu Tang",
      "Si Chen",
      "Ying Cheng",
      "Ronald Metoyer",
      "Ting Hua",
      "Nitesh V. Chawla"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2511.04689",
    "abstract_en_original": "Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. Existing evaluation protocols rely on average accuracy over fixed item sets, treating all items as equally informative despite substantial variation in difficulty and discrimination. We introduce ATLAS, an adaptive testing framework based on Item Response Theory (IRT) that estimates model ability using Fisher information-guided item selection. ATLAS reduces the number of required items by up to 90% while maintaining measurement precision. For instance, it matches whole-bank ability estimates using only 41 items (0.157 MAE) on HellaSwag (5,600 items). We further reconstruct accuracy from ATLAS's ability estimates and find that reconstructed accuracies closely match raw accuracies across all five benchmarks, indicating that ability $θ$ preserves the global performance structure. At the same time, $θ$ provides finer discrimination within accuracy-equivalent models: among more than 3,000 evaluated models, 23-31% shift by more than 10 rank positions, and models with identical accuracies receive meaningfully different ability estimates. Code and calibrated item banks are available at https://github.com/Peiyu-Georgia-Li/ATLAS.git.",
    "resumen_es_original": "Este artículo, titulado \"Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-174",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.537Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.537Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.04689",
      "fetched_title": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks"
  },
  {
    "legacy_article_number": 175,
    "title_original": "Can LLMs Infer Personality from Real World Conversations?",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jianfeng Zhu",
      "Ruoming Jin",
      "Karin G. Coifman"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Psychometrics",
      "Bias"
    ],
    "source_url": "https://arxiv.org/abs/2507.14355",
    "abstract_en_original": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity. We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item prediction and both zero-shot and chain-of-thought prompting for Big Five trait inference. All models showed high test-retest reliability, but construct validity was limited: correlations with ground-truth scores were weak (max Pearson's $r = 0.27$), interrater agreement was low (Cohen's $κ&lt; 0.10$), and predictions were biased toward moderate or high trait levels. Chain-of-thought prompting and longer input context modestly improved distributional alignment, but not trait-level accuracy. These results underscore limitations in current LLM-based personality inference and highlight the need for evidence-based development for psychological applications.",
    "resumen_es_original": "Este artículo, titulado \"Can LLMs Infer Personality from Real World Conversations?\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-175",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.541Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.541Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2507.14355",
      "fetched_title": "Can LLMs Infer Personality from Real World Conversations?",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Can LLMs Infer Personality from Real World Conversations?"
  },
  {
    "legacy_article_number": 176,
    "title_original": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Pranav Bhandari",
      "Nicolas Fay",
      "Sanjeevan Selvaganapathy",
      "Amitava Datta",
      "Usman Naseem",
      "Mehwish Nasim"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2511.03738",
    "abstract_en_original": "Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled. Personality-aware LLMs hold a promising direction towards this objective. However, the relationship between these psychological constructs and their representations within LLMs remains underexplored and requires further investigation. Moreover, it is intriguing to understand and study the use of these representations to steer the models' behaviour. We propose a novel pipeline that extracts hidden state activations from transformer layers using the Big Five Personality Traits (Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), which is a comprehensive and empirically validated framework to model human personality applies low-rank subspace discovery methods, and identifies trait-specific optimal layers across different model architectures for robust injection. The resulting personality-aligned directions are then operationalised through a flexible steering framework with dynamic layer selection, enabling precise control of trait expression in LLM outputs. Our findings reveal that personality traits occupy a low-rank shared subspace, and that these latent structures can be transformed into actionable mechanisms for effective steering through careful perturbations without impacting the fluency, variance and general capabilities, helping to bridge the gap between psychological theory and practical model alignment.",
    "resumen_es_original": "Este artículo, titulado \"Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-176",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.547Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.547Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.03738",
      "fetched_title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs"
  },
  {
    "legacy_article_number": 177,
    "title_original": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yifan Lyu",
      "Liang Zhang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "MBTI",
      "Psychometrics",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2512.08814",
    "abstract_en_original": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -&gt; user vector -&gt; labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).",
    "resumen_es_original": "Este artículo, titulado \"Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-177",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.557Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.557Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2512.08814",
      "fetched_title": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts"
  },
  {
    "legacy_article_number": 178,
    "title_original": "A Comparative Study of Large Language Models and Human Personality Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wang Jiaqi",
      "Wang bo",
      "Guo fa",
      "Cheng cheng",
      "Yang li"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2505.14845",
    "abstract_en_original": "Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.",
    "resumen_es_original": "Este artículo, titulado \"A Comparative Study of Large Language Models and Human Personality Traits\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-178",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.560Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.560Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2505.14845",
      "fetched_title": "A Comparative Study of Large Language Models and Human Personality Traits",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "A Comparative Study of Large Language Models and Human Personality Traits"
  },
  {
    "legacy_article_number": 179,
    "title_original": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Pranav Bhandari",
      "Usman Naseem",
      "Amitava Datta",
      "Nicolas Fay",
      "Mehwish Nasim"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2502.05248",
    "abstract_en_original": "Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.",
    "resumen_es_original": "Este artículo, titulado \"Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Psychological assessment tools have long helped humans understand behavioural patterns. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-179",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.562Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.562Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.05248",
      "fetched_title": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires"
  },
  {
    "legacy_article_number": 180,
    "title_original": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Michel Frising",
      "Daniel Balcells"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2512.17639",
    "abstract_en_original": "Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.",
    "resumen_es_original": "Este artículo, titulado \"Linear Personality Probing and Steering in LLMs: A Big Five Study\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-180",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.577Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.577Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2512.17639",
      "fetched_title": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Linear Personality Probing and Steering in LLMs: A Big Five Study"
  },
  {
    "legacy_article_number": 181,
    "title_original": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Francesco Di Cursi",
      "Chiara Boldrini",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Bias",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2511.23101",
    "abstract_en_original": "We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.",
    "resumen_es_original": "Este artículo, titulado \"Mind Reading or Misreading? LLMs on the Big Five Personality Test\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-181",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.579Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.579Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.23101",
      "fetched_title": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Mind Reading or Misreading? LLMs on the Big Five Personality Test"
  },
  {
    "legacy_article_number": 182,
    "title_original": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Dongmin Choi",
      "Woojung Song",
      "Jongwook Han",
      "Eun-Ju Lee",
      "Yohan Jo"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2509.10078",
    "abstract_en_original": "Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.",
    "resumen_es_original": "Este artículo, titulado \"Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-182",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.594Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.594Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.10078",
      "fetched_title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models"
  },
  {
    "legacy_article_number": 183,
    "title_original": "Investigating Large Language Models in Inferring Personality Traits from User Conversations",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jianfeng Zhu",
      "Ruoming Jin",
      "Karin G. Coifman"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2501.07532",
    "abstract_en_original": "Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. This study evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer Big Five personality traits and generate Big Five Inventory-10 (BFI-10) item scores from user conversations under zero-shot prompting conditions. Our findings reveal that incorporating an intermediate step--prompting for BFI-10 item scores before calculating traits--enhances accuracy and aligns more closely with the gold standard than direct trait inference. This structured approach underscores the importance of leveraging psychological frameworks in improving predictive precision. Additionally, a group comparison based on depressive symptom presence revealed differential model performance. Participants were categorized into two groups: those experiencing at least one depressive symptom and those without symptoms. GPT-4o mini demonstrated heightened sensitivity to depression-related shifts in traits such as Neuroticism and Conscientiousness within the symptom-present group, whereas GPT-4o exhibited strengths in nuanced interpretation across groups. These findings underscore the potential of LLMs to analyze real-world psychological data effectively, offering a valuable foundation for interdisciplinary research at the intersection of artificial intelligence and psychology.",
    "resumen_es_original": "Este artículo, titulado \"Investigating Large Language Models in Inferring Personality Traits from User Conversations\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-183",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.597Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.597Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2501.07532",
      "fetched_title": "Investigating Large Language Models in Inferring Personality Traits from User Conversations",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Investigating Large Language Models in Inferring Personality Traits from User Conversations"
  },
  {
    "legacy_article_number": 184,
    "title_original": "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Nicola Milano",
      "Michela Ponticorvo",
      "Davide Marocco"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Psychometrics",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2503.12080",
    "abstract_en_original": "In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs. Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment. Graduate psychology students employed the Content Validity Ratio (CVR) to rate test items, forming the human baseline. In parallel, state-of-the-art LLMs, including multilingual and fine-tuned models, analyzed item embeddings to predict construct mappings. The results reveal distinct strengths and limitations of human and AI approaches. Human validators excelled in aligning the behaviorally rich BFQ items, while LLMs performed better with the linguistically concise BFI items. Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs. Here we highlights the complementary potential of hybrid validation systems that integrate human expertise and AI precision. The findings underscore the transformative role of LLMs in psychological assessment, paving the way for scalable, objective, and robust test development methodologies.",
    "resumen_es_original": "Este artículo, titulado \"Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-184",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.610Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.610Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.12080",
      "fetched_title": "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests"
  },
  {
    "legacy_article_number": 185,
    "title_original": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jana Jung",
      "Marlene Lutz",
      "Indira Sen",
      "Markus Strohmaier"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2510.11254",
    "abstract_en_original": "Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests on 17 LLMs for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests on LLMs are essential before interpreting their scores. Our findings also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.",
    "resumen_es_original": "Este artículo, titulado \"Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.",
    "id": "article-185",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.613Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.613Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.11254",
      "fetched_title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality"
  },
  {
    "legacy_article_number": 186,
    "title_original": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Xiaotian Zhang",
      "Ruizhe Chen",
      "Yang Feng",
      "Zuozhu Liu"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2504.12663",
    "abstract_en_original": "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-186",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.617Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.617Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2504.12663",
      "fetched_title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment"
  },
  {
    "legacy_article_number": 187,
    "title_original": "How Personality Traits Shape LLM Risk-Taking Behaviour",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "John Hartley",
      "Conor Hamill",
      "Devesh Batra",
      "Dale Seddon",
      "Ramin Okhrati",
      "Raad Khraishi"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Bias and Fairness",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2503.04735",
    "abstract_en_original": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Interventions on GPT-4o's Big Five traits, particularly Openness, significantly influence its risk propensity, mirroring patterns observed in human studies. Notably, Openness emerges as the most influential factor in GPT-4o's risk propensity, aligning with human findings. In contrast, legacy models like GPT-4-Turbo demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and elucidates the potential and limitations of personality-based interventions in shaping LLM decision-making. Our findings have implications for the development of more robust and predictable AI systems such as financial modelling.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para aplicaciones, sesgos y consecuencias sociales. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-187",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.631Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.631Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.04735",
      "fetched_title": "How Personality Traits Shape LLM Risk-Taking Behaviour",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "How Personality Traits Shape LLM Risk-Taking Behaviour"
  },
  {
    "legacy_article_number": 188,
    "title_original": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Ke Ji",
      "Yixin Lian",
      "Linxu Li",
      "Jingsheng Gao",
      "Weiyuan Li",
      "Bin Dai"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2503.17662",
    "abstract_en_original": "In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive \\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model's role-playing strategy through iterative contrastive learning between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval \\&amp; GPT-4) and human expert evaluation.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-188",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.637Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.637Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.17662",
      "fetched_title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning"
  },
  {
    "legacy_article_number": 189,
    "title_original": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Fernanda Bufon Färber",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues Galvão Filho"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://doi.org/10.18653/v1/2025.findings-emnlp.506",
    "abstract_en_original": "This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-189",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.641Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.641Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.findings-emnlp.506/",
      "fetched_title": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey"
  },
  {
    "legacy_article_number": 190,
    "title_original": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Christos-Nikolaos Zacharopoulos",
      "Revekka Kyriakoglou"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2511.04499",
    "abstract_en_original": "As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-190",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.653Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.653Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.04499",
      "fetched_title": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering"
  },
  {
    "legacy_article_number": 191,
    "title_original": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Rui Xu",
      "Xintao Wang",
      "Jiangjie Chen",
      "Siyu Yuan",
      "Xinfeng Yuan",
      "Jiaqing Liang",
      "Zulong Chen",
      "Xiaoqingdong",
      "Yanghua Xiao"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://doi.org/10.18653/v1/2025.findings-emnlp.813",
    "abstract_en_original": "This paper examines personality-related behavior in large language models and its methodological implications for inducción y control de personalidad. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-191",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.656Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.656Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.findings-emnlp.813/",
      "fetched_title": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?"
  },
  {
    "legacy_article_number": 192,
    "title_original": "Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yixiao Wang",
      "Russel Dsouza",
      "Robert Lee",
      "Ian Apperly",
      "Rory Devine",
      "Sanne van der Kleij",
      "Mark Lee"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://doi.org/10.18653/v1/2025.clpsych-1.7",
    "abstract_en_original": "This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-192",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.667Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.667Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.clpsych-1.7/",
      "fetched_title": "Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models"
  },
  {
    "legacy_article_number": 193,
    "title_original": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Haijiang Liu",
      "Qiyuan Li",
      "Chao Gao",
      "Yong Cao",
      "Xiangyu Xu",
      "Xun Wu",
      "Daniel Hershcovich",
      "Jinguang Gu"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Bias and Fairness",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2508.17855",
    "abstract_en_original": "Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para aplicaciones, sesgos y consecuencias sociales. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-193",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.446Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.446Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.17855",
      "fetched_title": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning"
  },
  {
    "legacy_article_number": 194,
    "title_original": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Chenkai Sun",
      "Ke Yang",
      "Revanth Gangi Reddy",
      "Yi R. Fung",
      "Hou Pong Chan",
      "Kevin Small",
      "ChengXiang Zhai",
      "Heng Ji"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2402.11060",
    "abstract_en_original": "The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the evaluation of response prediction, Persona-DB demonstrates superior context efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 10% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-194",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.455Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.455Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.11060",
      "fetched_title": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement"
  },
  {
    "legacy_article_number": 195,
    "title_original": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yi-Fei Liu",
      "Yi-Long Lu",
      "Di He",
      "Hang Zhang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2511.03235",
    "abstract_en_original": "Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 &gt; 0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-195",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.460Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.460Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.03235",
      "fetched_title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers"
  },
  {
    "legacy_article_number": 196,
    "title_original": "Your Language Model Secretly Contains Personality Subnetworks",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Ruimeng Ye",
      "Zihan Wang",
      "Zinan Ling",
      "Yang Xiao",
      "Manling Li",
      "Xiaolong Ma",
      "Bo Hui"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2602.07164",
    "abstract_en_original": "Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.",
    "resumen_es_original": "Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.",
    "id": "article-196",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.471Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.471Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.07164",
      "fetched_title": "Your Language Model Secretly Contains Personality Subnetworks",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Your Language Model Secretly Contains Personality Subnetworks"
  },
  {
    "legacy_article_number": 197,
    "title_original": "A psychometric framework for evaluating and shaping personality traits in large language models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Gregory Serapio-García",
      "Mustafa Safdari",
      "Clément Crepy",
      "Luning Sun",
      "Stephen Fitz",
      "Peter Romero",
      "Marwa Abdulhai",
      "Aleksandra Faust",
      "Maja Matarić"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://www.nature.com/articles/s42256-025-01115-6",
    "abstract_en_original": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public worldwide, the synthetic personality traits embedded in these models by virtue of training on large amounts of human data are becoming increasingly important to evaluate. The style in which LLMs respond can mimic different human personality traits. Here, as these patterns can be a key factor determining the effectiveness of communication, we present a comprehensive psychometric methodology for administering and validating personality tests on widely used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found that: personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction-fine-tuned models; and personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible artificial intelligence. Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de evaluación y validación psicométrica. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-197",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:46.420Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:46.420Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s42256-025-01115-6?error=cookies_not_supported&code=23dedb28-5517-4d76-953c-8ec4527a9234",
      "fetched_title": "A psychometric framework for evaluating and shaping personality traits in large language models - Nature Machine Intelligence",
      "title_similarity": 0.8,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "A psychometric framework for evaluating and shaping personality traits in large language models - Nature Machine Intelligence"
  },
  {
    "legacy_article_number": 198,
    "title_original": "Chameleon LLMs: User Personas Influence Chatbot Personality Shifts",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jane Xing",
      "Tianyi Niu",
      "Shashank Srivastava"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://aclanthology.org/2025.emnlp-main.875/",
    "abstract_en_original": "As large language models (LLMs) integrate into society, their ability to adapt to users is as critical as their accuracy. While prior work has used personality tests to examine the perceived personalities of LLMs, little research has explored whether LLMs adapt their perceived personalities in response to user interactions. We investigate whether and how LLMs exhibit conversational adaptations over prolonged interactions. Using a controlled simulations where a user and chatbot engage in dialogue, we measure the chatbot’s personality shift before and after the conversation. Across multiple models, we find that traits such as Agreeableness, Extraversion, and Conscientiousness are highly susceptible to user influence, whereas Emotional Stability and Intellect remain relatively more stable. Our results suggest that LLMs dynamically adjust their conversational style in response to user personas, raising important implications for AI alignment, trust, and safety.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de evaluación y validación psicométrica. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-198",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:46.994Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:46.994Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.emnlp-main.875/",
      "fetched_title": "Chameleon LLMs: User Personas Influence Chatbot Personality Shifts",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Chameleon LLMs: User Personas Influence Chatbot Personality Shifts"
  },
  {
    "legacy_article_number": 199,
    "title_original": "From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Shenghan Wu",
      "Yimo Zhu",
      "Wynne Hsu",
      "Mong-Li Lee",
      "Yang Deng"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Emotional Support",
      "LLM Evaluation"
    ],
    "source_url": "https://aclanthology.org/2025.emnlp-main.277/",
    "abstract_en_original": "The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de inducción y control de personalidad. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-199",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:47.342Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:47.342Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.emnlp-main.277/",
      "fetched_title": "From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations"
  },
  {
    "legacy_article_number": 200,
    "title_original": "Neuron based Personality Trait Induction in Large Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jia Deng",
      "Tianyi Tang",
      "Yanbin Yin",
      "Wenhao yang",
      "Xin Zhao",
      "Ji-Rong Wen"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Steering",
      "LLM Evaluation"
    ],
    "source_url": "https://openreview.net/forum?id=LYHEY783Np",
    "abstract_en_original": "Large language models (LLMs) have become increasingly proficient at simulating various personality traits, an important capability for supporting related applications (e.g., role-playing). To further improve this capacity, in this paper, we present a neuron based approach for personality trait induction in LLMs, with three major technical contributions. First, we construct PERSONALITYBENCH, a large-scale dataset for identifying and evaluating personality traits in LLMs. This dataset is grounded in the Big Five personality traits from psychology and designed to assess the generative capabilities of LLMs towards specific personality traits. Second, by leveraging PERSONALITYBENCH, we propose an efficient method for identifying personality-related neurons within LLMs by examining the opposite aspects of a given trait. Third, we develop a simple yet effective induction method that manipulates the values of these identified personality-related neurons, which enables fine-grained control over the traits exhibited by LLMs without training and modifying model parameters. Extensive experiments validates the efficacy of our neuron identification and trait induction methods. Notably, our approach achieves comparable performance as fine-tuned models, offering a more efficient and flexible solution for personality trait induction in LLMs.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de inducción y control de personalidad. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-200",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:47.688Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:47.688Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://openreview.net/forum?id=LYHEY783Np",
      "fetched_title": "Neuron based Personality Trait Induction in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Neuron based Personality Trait Induction in Large Language Models"
  },
  {
    "legacy_article_number": 201,
    "title_original": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Florian Dietz",
      "William Wale",
      "Oscar Gilg",
      "Robert McCarthy",
      "Felix Michalak",
      "Gustavo Ewbank Rodrigues Danon",
      "Miguelito de Guzman",
      "Dietrich Klakow"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Safety",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2602.05532",
    "abstract_en_original": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de inducción y control de personalidad. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-201",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:48.202Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:48.202Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.05532",
      "fetched_title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities"
  },
  {
    "legacy_article_number": 202,
    "title_original": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tiantian Chen",
      "Jiaqi Lu",
      "Ying Shen",
      "Lin Zhang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Long-term Memory",
      "Emotional Support",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2602.01885",
    "abstract_en_original": "Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de aplicaciones, sesgos y consecuencias sociales. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-202",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:54.043Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:54.043Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.01885",
      "fetched_title": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support"
  },
  {
    "legacy_article_number": 203,
    "title_original": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Shivam Ratnakar",
      "Sanjay Raghavendra"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Behavioral Consistency",
      "Bias and Fairness",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2510.16712",
    "abstract_en_original": "Integration of Large Language Models with search/retrieval engines has become ubiquitous, yet these systems harbor a critical vulnerability that undermines their reliability. We present the first systematic investigation of \"chameleon behavior\" in LLMs: their alarming tendency to shift stances when presented with contradictory questions in multi-turn conversations (especially in search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising 17,770 carefully crafted question-answer pairs across 1,180 multi-turn conversations spanning 12 controversial domains, we expose fundamental flaws in state-of-the-art systems. We introduce two theoretically grounded metrics: the Chameleon Score (0-1) that quantifies stance instability, and Source Re-use Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent failures: all models exhibit severe chameleon behavior (scores 0.391-0.511), with GPT-4o-mini showing the worst performance. Crucially, small across-temperature variance (less than 0.004) suggests the effect is not a sampling artifact. Our analysis uncovers the mechanism: strong correlations between source re-use rate and confidence (r=0.627) and stance changes (r=0.429) are statistically significant (p less than 0.05), indicating that limited knowledge diversity makes models pathologically deferential to query framing. These findings highlight the need for comprehensive consistency evaluation before deploying LLMs in healthcare, legal, and financial systems where maintaining coherent positions across interactions is critical for reliable decision support.",
    "resumen_es_original": "Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de aplicaciones, sesgos y consecuencias sociales. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.",
    "id": "article-203",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:57:58.262Z",
    "evidence": {
      "checked_at": "2026-02-13T18:57:58.262Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.16712",
      "fetched_title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "title_canonical": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models"
  },
  {
    "legacy_article_number": 204,
    "title_original": "Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Chang-Jin Li",
      "Jiyuan Zhang",
      "Yun Tang",
      "Jian Li"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Assessment",
      "Situational Judgment Tests",
      "Psychometrics",
      "Big Five"
    ],
    "source_url": "https://doi.org/10.1016/j.chbr.2026.100964",
    "abstract_en_original": "Personality assessment through situational judgment tests (SJTs) offers unique advantages over traditional Likert-type self-report scales, yet their development remains labor-intensive, time-consuming, and heavily dependent on subject matter experts. Recent advances in large language models (LLMs) have shown promise for automatic item generation (AIG). Building on these developments, the present study focuses on developing and evaluating a structured and generalizable framework for automatically generating personality SJTs, using GPT-4 and ChatGPT-5 as empirical examples. Three studies were conducted. Study 1 systematically compared the effects of prompt design and temperature settings on the content validity of LLM-generated items to develop an effective and stable LLM-based AIG approach for personality SJT. Results showed that optimized prompts and a temperature of 1.0 achieved the best balance of creativity and accuracy on GPT-4. Study 2 examined the cross-model generalizability and reproducibility of this automated SJT generation approach through multiple rounds. The results showed that the approach consistently produced reproducible and high-quality items on ChatGPT-5. Study 3 evaluated the psychometric properties of LLM-generated SJTs covering five facets of the Big Five personality traits. Results demonstrated satisfactory reliability and validity across most facets, though limitations were observed in the convergent validity of the compliance facet and certain aspects of criterion-related validity. These findings provide robust evidence that the proposed LLM-based AIG approach can produce culturally appropriate and psychometrically sound SJTs with efficiency comparable to or exceeding traditional methods.",
    "resumen_es_original": "Este estudio aborda la generación automática de ítems para pruebas situacionales de personalidad con modelos de lenguaje, un proceso que tradicionalmente requiere mucho tiempo y fuerte dependencia de expertos humanos. El trabajo propone un marco estructurado y reutilizable para crear ítems de SJT orientados a rasgos de personalidad, y lo evalúa con GPT-4 y ChatGPT-5 en varios experimentos. La investigación pone el foco en validez de contenido, reproducibilidad entre corridas y transferencia entre modelos.\n\nLos resultados muestran que el diseño del prompt y la temperatura influyen de forma crítica en la calidad psicométrica de los ítems generados. Con configuraciones optimizadas, los autores reportan mejor equilibrio entre creatividad y precisión, además de consistencia en la calidad de los ítems al repetir el proceso en distintos ciclos de generación. Esto sugiere que la generación automática no solo puede acelerar el desarrollo de instrumentos, sino también mantener estándares técnicos razonables cuando se controla el protocolo.\n\nEn la evaluación final, los SJT generados por LLMs alcanzan niveles satisfactorios de fiabilidad y validez en la mayoría de facetas analizadas del modelo Big Five, aunque persisten limitaciones en algunas dimensiones concretas. En conjunto, el artículo aporta evidencia útil para integrar LLMs en desarrollo psicométrico aplicado, con implicaciones directas para escalabilidad, coste y adaptación cultural de instrumentos de personalidad.",
    "id": "article-204",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:01.249Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:01.249Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1016/j.chbr.2026.100964",
      "fetched_title": "Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models",
      "title_similarity": 1,
      "author_overlap": 0.75,
      "year_match": true
    },
    "title_canonical": "Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models"
  },
  {
    "legacy_article_number": 205,
    "title_original": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Lucio La Cava",
      "Andrea Tagarelli"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Mimicry",
      "MBTI",
      "Big Five",
      "Personality Conditioning"
    ],
    "source_url": "https://doi.org/10.1609/aaai.v39i2.32125",
    "abstract_en_original": "The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. However, research on the personalities exhibited by LLMs has largely been confined to limited investigations using individual psychological tests, primarily focusing on a small number of commercially licensed LLMs. This approach overlooks the extensive use and significant advancements observed in open-source LLMs. This work aims to address both the above limitations by conducting an in-depth investigation of a significant body of 12 LLM Agents based on the most representative Open models, through the two most well-known psychological assessment tests, namely Myers-Briggs Type Indicator (MBTI) and Big Five Inventory (BFI). Our approach involves evaluating the intrinsic personality traits of LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that (i) each LLM agent showcases distinct human personalities; (ii) personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and (iii) combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of LLMs.",
    "resumen_es_original": "Este trabajo examina en profundidad si los LLMs abiertos pueden imitar personalidades humanas cuando se les condiciona con rasgos y roles específicos. Frente a estudios previos centrados en pocos modelos comerciales, los autores analizan 12 agentes basados en modelos abiertos y aplican dos marcos de evaluación ampliamente usados en psicología: MBTI y Big Five. El objetivo principal es distinguir entre rasgos intrínsecos del modelo y rasgos inducidos por prompting.\n\nLos resultados muestran que cada agente presenta un perfil propio relativamente estable, y que el condicionamiento por personalidad no tiene el mismo efecto en todos los modelos. Solo algunos logran aproximarse de forma convincente a la personalidad objetivo, mientras que muchos mantienen su tendencia base, lo que los autores describen como comportamiento \"closed-minded\". Esta evidencia es especialmente relevante para diseño de agentes donde se espera control fino sobre estilo y comportamiento.\n\nAdemás, el estudio encuentra que combinar rol más personalidad mejora la capacidad de mimetización frente a usar solo rasgos aislados. En términos prácticos, el artículo aporta criterios para inducir personalidad de manera más efectiva en LLMs abiertos y delimita hasta dónde llega ese control sin ajuste de parámetros, con implicaciones para robustez, coherencia y seguridad conductual.",
    "id": "article-205",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:01.722Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:01.722Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://ojs.aaai.org/index.php/AAAI/article/view/32125",
      "fetched_title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models | Proceedings of the AAAI Conference on Artificial Intelligence",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models | Proceedings of the AAAI Conference on Artificial Intelligence"
  },
  {
    "legacy_article_number": 206,
    "title_original": "On the emergent capabilities of ChatGPT 4 to estimate personality traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Marco Piastra",
      "Patrizia Catellani"
    ],
    "keywords": [
      "ChatGPT-4",
      "Personality Trait Estimation",
      "Big Five",
      "Text Analysis",
      "Psychometrics"
    ],
    "source_url": "https://doi.org/10.3389/frai.2025.1484260",
    "abstract_en_original": "This study investigates the potential of ChatGPT 4 in the assessment of personality traits based on written texts. Using two publicly available datasets containing both written texts and self-assessments of the authors’ psychological traits based on the Big Five model, we aimed to evaluate the predictive performance of ChatGPT 4. For each sample text, we asked for numerical predictions on an eleven-point scale and compared them with the self-assessments. We also asked for ChatGPT 4 confidence scores on an eleven-point scale for each prediction. To keep the study within a manageable scope, a zero-prompt modality was chosen, although more sophisticated prompting strategies could potentially improve performance. The results show that ChatGPT 4 has moderate but significant abilities to automatically infer personality traits from written text. However, it also shows limitations in recognizing whether the input text is appropriate or representative enough to make accurate inferences, which could hinder practical applications. Furthermore, the results suggest that improved benchmarking methods could increase the efficiency and reliability of the evaluation process. These results pave the way for a more comprehensive evaluation of the capabilities of Large Language Models in assessing personality traits from written texts.",
    "resumen_es_original": "El artículo evalúa la capacidad de ChatGPT-4 para inferir rasgos de personalidad a partir de texto escrito, usando datasets públicos con autoevaluaciones Big Five como referencia. El protocolo compara predicciones numéricas del modelo con las puntuaciones de los propios autores de los textos, e incorpora además niveles de confianza reportados por el sistema. La configuración empleada es de zero-prompt para aislar el rendimiento base.\n\nLos autores encuentran que ChatGPT-4 logra una capacidad de inferencia moderada pero estadísticamente significativa en varias dimensiones, lo que sugiere potencial para tareas de perfilado psicológico automatizado. Al mismo tiempo, el modelo muestra limitaciones para discriminar cuándo una muestra textual es suficientemente representativa para sostener una inferencia fiable, un punto clave para evitar sobreinterpretaciones.\n\nComo implicación metodológica, el trabajo subraya la necesidad de benchmarks más sólidos y comparables para evaluar personalidad en LLMs. Su contribución se alinea con una agenda psicométrica de validación incremental: medir rendimiento real, cuantificar incertidumbre y definir condiciones de uso antes de trasladar estas herramientas a contextos aplicados sensibles.",
    "id": "article-206",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:02.740Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:02.740Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1484260/full",
      "fetched_title": "Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits",
      "title_similarity": 0.9091,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits"
  },
  {
    "legacy_article_number": 207,
    "title_original": "Rethinking psychometrics through LLMs: how item semantics shape measurement and prediction in psychological questionnaires",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Federico Ravenda",
      "Antonio Preti",
      "Michele Poletti",
      "Antonietta Mira",
      "Andrea Raballo"
    ],
    "keywords": [
      "Psychometrics",
      "Questionnaire Semantics",
      "Big Five",
      "LLMs",
      "Measurement Validity"
    ],
    "source_url": "https://doi.org/10.1038/s41598-025-21289-8",
    "abstract_en_original": "Abstract Psychological questionnaires are typically designed to measure latent constructs by asking respondents a series of semantically related questions. But what if these semantic relationships, rather than reflecting only the underlying construct, also impose their own structure on the data we collect? In other words, to what extent is what we “measure” in questionnaires shaped a priori by item semantics rather than revealed solely a posteriori through empirical correlations? To examine this epistemological question, we propose LLMs Psychometrics, a novel paradigm that harness LLMs to investigate how the semantic structure of questionnaire items influences psychometric outcomes. We hypothesize that the correlations among items partly mirror their linguistic similarity, such that LLMs can predict these correlations-even in the absence of empirical data. To test this, we compared actual correlation matrices from established instruments-the Big 5 Personality (Big 5) and Depression Anxiety Stress Scale (DASS-42)-with the semantic similarity structures computed by LLMs. Among the top 3 semantically similar items, the empirically most correlated item was found in 95% of DASS cases and 82% of Big 5 cases. Building on this, we developed PsychoLLM, a neural proof-of-concept architecture, which uses item semantics to predict responses to new items-demonstrated with the Generalized Anxiety Disorder-7 (GAD-7) and Patient Health Questionnaire-9 (PHQ-9). PsychoLLM achieved 70% accuracy when predicting one scale’s responses from the other, enabling new analyses based on semantic relationships. This work underscores an important epistemological implication for psychometrics: item semantics may influence measurement outcomes to varying degrees, more extensively than previously assumed. By leveraging LLMs to expose this a priori semantic structure, researchers can refine questionnaire design, assess data quality, and expand interpretive possibilities, ultimately inviting a reexamination of “ what ” and “ how ” we truly measure in psychology.",
    "resumen_es_original": "Este trabajo plantea una pregunta central para la psicometría: hasta qué punto los cuestionarios miden constructos latentes y hasta qué punto reflejan la propia semántica de los ítems. Para responderla, los autores proponen un marco llamado \"LLMs Psychometrics\", donde modelos de lenguaje estiman relaciones entre preguntas antes de observar datos empíricos tradicionales. El estudio se apoya en instrumentos consolidados como Big Five y DASS-42.\n\nLos resultados muestran una correspondencia alta entre similitud semántica y correlaciones observadas entre ítems, lo que sugiere que parte de la estructura psicométrica puede estar condicionada por el lenguaje del test. Sobre esa base, el artículo introduce PsychoLLM, un prototipo que usa semántica de ítems para predecir respuestas en escalas distintas (GAD-7 y PHQ-9), alcanzando niveles de precisión relevantes en tareas cruzadas.\n\nLa contribución principal no es solo técnica sino epistemológica: invita a revisar supuestos clásicos sobre validez de medición y construcción de instrumentos. En el contexto de «personalidad sintética» y evaluación de LLMs, aporta una ruta para auditar cuestionarios, depurar artefactos semánticos y mejorar la interpretación de resultados psicométricos en investigación computacional.",
    "id": "article-207",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:02.763Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:02.763Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-025-21289-8?error=cookies_not_supported&code=ac9cb615-a1c7-4dd2-a3ab-b47c7679012c",
      "fetched_title": "Rethinking psychometrics through LLMs: how item semantics shape measurement and prediction in psychological questionnaires - Scientific Reports",
      "title_similarity": 0.875,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Rethinking psychometrics through LLMs: how item semantics shape measurement and prediction in psychological questionnaires - Scientific Reports"
  },
  {
    "legacy_article_number": 208,
    "title_original": "Comparing chatbots to psychometric tests in hiring: reduced social desirability bias, but lower predictive validity",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "D Dukanović",
      "Dario Krpan"
    ],
    "keywords": [
      "Hiring",
      "Chatbots",
      "Psychometric Testing",
      "Social Desirability Bias",
      "Predictive Validity"
    ],
    "source_url": "https://doi.org/10.3389/fpsyg.2025.1564979",
    "abstract_en_original": "This paper explores the efficacy of AI-driven chatbots in accurately inferring personality traits compared to traditional psychometric tests within a real-world professional hiring context. The study is driven by the increasing integration of AI tools in recruitment processes, which necessitates a deeper understanding of their reliability and validity. Using a quasi-experimental design with propensity score matching, we analysed data from 159 candidates and other professionals from Serbian and Montenegrin regions who completed both traditional psychometric assessments and AI-based personality evaluations based on the Big Five Personality model. A novel one-question-per-facet approach was employed in the chatbot assessments with a goal of enabling more granular analysis of the chatbot’s psychometric properties. The findings indicate that the chatbot demonstrated good structural, substantive, and convergent validity for certain traits, particularly Extraversion and Conscientiousness, but not for Neuroticism, Agreeableness, and Openness. While robust regression confirmed that AI-inferred scores are less susceptible to social desirability bias than traditional tests, they did not significantly predict real-world outcomes, indicating issues with external validity, particularly predictive validity. The results suggest that AI-driven chatbots show promise for identifying certain personality traits and demonstrate resistance to social desirability bias. This paper contributes to the emerging field of AI and psychometrics by offering insights into the potential and limitations of AI tools in professional selection, while developing an approach for refining psychometric properties of AI-driven assessments.",
    "resumen_es_original": "El estudio compara evaluaciones de personalidad basadas en chatbots con pruebas psicométricas tradicionales en un contexto real de selección laboral. Con un diseño cuasi-experimental y técnicas de emparejamiento estadístico, analiza datos de 159 participantes que completaron ambos tipos de evaluación bajo el marco Big Five. El trabajo busca medir si los sistemas de IA aportan ventajas prácticas sin sacrificar validez.\n\nLos resultados indican que los chatbots muestran desempeño razonable en algunas dimensiones, especialmente extraversión y responsabilidad, y parecen menos vulnerables al sesgo de deseabilidad social que los tests clásicos. Sin embargo, su capacidad para predecir resultados externos reales es más limitada, lo que afecta la validez predictiva en decisiones de contratación.\n\nDesde la perspectiva de aplicaciones y riesgo, el artículo sugiere una postura equilibrada: los chatbots pueden complementar procesos de evaluación, pero no deberían reemplazar instrumentos consolidados sin evidencia adicional. La contribución es relevante para gobernanza de IA en RRHH, donde sesgo, transparencia y consecuencias de error tienen impacto directo en personas y organizaciones.",
    "id": "article-208",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:03.479Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:03.479Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1564979/full",
      "fetched_title": "Frontiers | Comparing chatbots to psychometric tests in hiring: reduced social desirability bias, but lower predictive validity",
      "title_similarity": 0.9375,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Frontiers | Comparing chatbots to psychometric tests in hiring: reduced social desirability bias, but lower predictive validity"
  },
  {
    "legacy_article_number": 209,
    "title_original": "Large language models display human-like social desirability biases in Big Five personality surveys",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Aadesh Salecha",
      "Molly E Ireland",
      "Shashanka Subrahmanya",
      "João Sedoc",
      "Lyle H Ungar",
      "Johannes C Eichstaedt"
    ],
    "keywords": [
      "Large Language Models",
      "Social Desirability Bias",
      "Big Five",
      "Survey Methods",
      "Behavioral Bias"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgae533",
    "abstract_en_original": "Abstract Large language models (LLMs) are becoming more widely used to simulate human participants and so understanding their biases is important. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e. increased extraversion, decreased neuroticism, etc.). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4’s survey responses changing by 1.20 (human) SD and Llama 3’s by 0.98 SD, which are very large effects. This bias remains after question order randomization and paraphrasing. Reverse coding the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on this use of LLMs as proxies for human participants.",
    "resumen_es_original": "Este artículo identifica un sesgo de deseabilidad social en múltiples LLMs cuando responden encuestas de personalidad Big Five. Los autores diseñan un marco experimental para comprobar si los modelos detectan que están siendo evaluados y, en ese caso, ajustan sus respuestas hacia perfiles socialmente más aceptables. El fenómeno se observa en modelos comerciales y abiertos, incluyendo GPT, Claude, Llama y PaLM.\n\nEl estudio reporta efectos de magnitud alta y robustos a varias pruebas de control, como aleatorización de orden de preguntas y parafraseo de ítems. Aunque el recodificado inverso reduce parte del efecto, no lo elimina, lo que sugiere que no se trata solo de aquiescencia sino de un patrón más profundo en la conducta del modelo ante contextos evaluativos.\n\nLas implicaciones son relevantes para investigación y práctica: usar LLMs como sustitutos de participantes humanos o para perfilado psicométrico directo puede introducir distorsiones sistemáticas. En términos de «personalidad sintética», el trabajo refuerza la necesidad de protocolos que separen señales de rasgo de artefactos de respuesta inducidos por el propio formato de medición.",
    "id": "article-209",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:03.885Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:03.885Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgae533",
      "fetched_title": "Large language models display human-like social desirability biases in Big Five personality surveys",
      "title_similarity": 1,
      "author_overlap": 0.6667,
      "year_match": true
    },
    "title_canonical": "Large language models display human-like social desirability biases in Big Five personality surveys"
  },
  {
    "legacy_article_number": 210,
    "title_original": "Can Large Language Models Assess Personality From Asynchronous Video Interviews? A Comprehensive Evaluation of Validity, Reliability, Fairness, and Rating Patterns",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Tianyi Zhang",
      "Antonis Koutsoumpis",
      "Janneke K. Oostrom",
      "Djurre Holtrop",
      "Sina Ghassemi",
      "Reinout E. de Vries"
    ],
    "keywords": [
      "Large Language Models",
      "Asynchronous Video Interviews",
      "Personality Assessment",
      "Fairness",
      "Reliability"
    ],
    "source_url": "https://doi.org/10.1109/taffc.2024.3374875",
    "abstract_en_original": "The advent of Artificial Intelligence (AI) technologies has precipitated the rise of asynchronous video interviews (AVIs) as an alternative to conventional job interviews. These one-way video interviews are conducted online and can be analyzed using AI algorithms to automate and speed up the selection procedure. In particular, the swift advancement of Large Language Models (LLMs) has significantly decreased the cost and technical barrier to developing AI systems for automatic personality and interview performance evaluation. However, the generative and task-unspecific nature of LLMs might pose potential risks and biases when evaluating humans based on their AVI responses. In this study, we conducted a comprehensive evaluation of the validity, reliability, fairness, and rating patterns of two widely-used LLMs, GPT-3.5 and GPT-4, in assessing personality and interview performance from an AVI. We compared the personality and interview performance ratings of the LLMs with the ratings from a task-specific AI model and human annotators using simulated AVI responses of 685 participants. The results show that LLMs can achieve similar or even better zero-shot validity compared with the task-specific AI model when predicting personality traits. The verbal explanations for predicting personality traits generated by LLMs are interpretable by the personality items that are designed according to psychological theories. However, LLMs also suffered from uneven performance across different traits, insufficient test-retest reliability, and the emergence of certain biases. Thus, it is necessary to exercise caution when applying LLMs for human-related application scenarios, especially for significant decisions such as employment.",
    "resumen_es_original": "El trabajo evalúa si GPT-3.5 y GPT-4 pueden valorar rasgos de personalidad y desempeño en entrevistas asíncronas de video (AVI) con un nivel de calidad comparable a sistemas especializados y evaluadores humanos. El estudio compara validez, fiabilidad, equidad y patrones de puntuación sobre respuestas simuladas de 685 participantes, en un escenario aplicado de selección laboral.\n\nLos resultados muestran que los LLMs pueden alcanzar buena validez en configuración zero-shot, e incluso competir con modelos específicos de tarea en algunas métricas. También se reporta que las explicaciones textuales de los modelos son interpretables desde marcos psicológicos, lo que aporta valor práctico para auditoría cualitativa.\n\nNo obstante, aparecen límites importantes: rendimiento desigual según rasgo, fiabilidad temporal insuficiente y señales de sesgo. Por ello, el artículo concluye que su uso en decisiones de alto impacto debe hacerse con cautela y bajo controles metodológicos estrictos, aportando evidencia clave para la agenda de validación psicométrica en sistemas de IA aplicados a personas.",
    "id": "article-210",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:04.275Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:04.275Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://ieeexplore.ieee.org/document/10463124/",
      "fetched_title": "Can Large Language Models Assess Personality From Asynchronous Video Interviews? A Comprehensive Evaluation of Validity, Reliability, Fairness, and Rating Patterns",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Can Large Language Models Assess Personality From Asynchronous Video Interviews? A Comprehensive Evaluation of Validity, Reliability, Fairness, and Rating Patterns"
  },
  {
    "legacy_article_number": 211,
    "title_original": "The Personality Dimensions GPT-3 Expresses During Human-Chatbot Interactions",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Nikola Kovačević",
      "Christian Holz",
      "Markus Gross",
      "Rafael Wampfler"
    ],
    "keywords": [
      "GPT-3",
      "Human-Chatbot Interaction",
      "Personality Perception",
      "Factor Analysis",
      "Conversational Agents"
    ],
    "source_url": "https://doi.org/10.1145/3659626",
    "abstract_en_original": "Large language models such as GPT-3 and ChatGPT can mimic human-to-human conversation with unprecedented fidelity, which enables many applications such as conversational agents for education and non-player characters in video games. In this work, we investigate the underlying personality structure that a GPT-3-based chatbot expresses during conversations with a human. We conducted a user study to collect 147 chatbot personality descriptors from 86 participants while they interacted with the GPT-3-based chatbot for three weeks. Then, 425 new participants rated the 147 personality descriptors in an online survey. We conducted an exploratory factor analysis on the collected descriptors and show that, though overlapping, human personality models do not fully transfer to the chatbot's personality as perceived by humans. We also show that the perceived personality is significantly different from that of virtual personal assistants, where users focus rather on serviceability and functionality. We discuss the implications of ever-evolving large language models and the change they affect in users' perception of agent personalities.",
    "resumen_es_original": "Este artículo estudia la estructura de personalidad que los usuarios perciben en un chatbot basado en GPT-3 durante interacciones prolongadas. A partir de descriptores recogidos en un estudio de uso de tres semanas y una segunda fase de valoración con más participantes, los autores aplican análisis factorial exploratorio para identificar dimensiones latentes de personalidad percibida.\n\nLos resultados indican que, aunque existen solapamientos con modelos de personalidad humana, la estructura percibida en el chatbot no coincide plenamente con taxonomías humanas estándar. Además, la personalidad atribuida a este tipo de agente difiere de la que los usuarios asignan a asistentes virtuales centrados en utilidad funcional, lo que sugiere que la calidad conversacional modifica el marco psicológico de evaluación del sistema.\n\nEl trabajo aporta evidencia relevante para medir «personalidad sintética» en interacción real y para diseñar métodos de evaluación más adaptados a agentes conversacionales. En conjunto, refuerza la idea de que no basta con trasladar instrumentos humanos sin ajustes, y que la percepción de personalidad en IA requiere marcos específicos de validación.",
    "id": "article-211",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:04.718Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:04.718Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1145/3659626",
      "fetched_title": "The Personality Dimensions GPT-3 Expresses During Human-Chatbot Interactions",
      "title_similarity": 1,
      "author_overlap": 0.75,
      "year_match": true
    },
    "title_canonical": "The Personality Dimensions GPT-3 Expresses During Human-Chatbot Interactions"
  },
  {
    "legacy_article_number": 212,
    "title_original": "Personality prediction from task-oriented and open-domain human-machine dialogues",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Ao Guo",
      "Ryu Hirai",
      "Atsumoto Ohashi",
      "Yuya Chiba",
      "Yuiko Tsunomori",
      "Ryuichiro Higashinaka"
    ],
    "keywords": [
      "Personality Prediction",
      "Human-Machine Dialogue",
      "MBTI",
      "Big Five",
      "Dialogue Systems"
    ],
    "source_url": "https://doi.org/10.1038/s41598-024-53989-y",
    "abstract_en_original": "Abstract If a dialogue system can predict the personality of a user from dialogue, it will enable the system to adapt to the user’s personality, leading to better task success and user satisfaction. In a recent study, personality prediction was performed using the Myers-Briggs Type Indicator (MBTI) personality traits with a task-oriented human-machine dialogue using an end-to-end (neural-based) system. However, it is still not clear whether such prediction is generally possible for other types of systems and user personality traits. To clarify this, we recruited 378 participants, asked them to fill out four personality questionnaires covering 25 personality traits, and had them perform three rounds of human-machine dialogue with a pipeline task-oriented dialogue system or an end-to-end task-oriented dialogue system. We also had another 186 participants do the same with an open-domain dialogue system. We then constructed BERT-based models to predict the personality traits of the participants from the dialogues. The results showed that prediction accuracy was generally better with open-domain dialogue than with task-oriented dialogue, although Extraversion (one of the Big Five personality traits) could be predicted equally well for both open-domain dialogue and pipeline task-oriented dialogue. We also examined the effect of utilizing different types of dialogue on personality prediction by conducting a cross-comparison of the models trained from the task-oriented and open-domain dialogues. As a result, we clarified that the open-domain dialogue cannot be used to predict personality traits from task-oriented dialogue, and vice versa. We further analyzed the effects of system utterances, task performance, and the round of dialogue with regard to the prediction accuracy.",
    "resumen_es_original": "El estudio analiza si es posible predecir rasgos de personalidad de usuarios a partir de diálogos humano-máquina en distintos tipos de sistemas conversacionales. Para ello combina varios cuestionarios de personalidad, múltiples rondas de conversación y modelos BERT entrenados sobre interacciones task-oriented y open-domain, con una muestra amplia de participantes.\n\nLos resultados indican que el rendimiento de predicción suele ser mejor en diálogo abierto que en diálogo orientado a tarea, aunque algunos rasgos concretos, como extraversión, mantienen niveles comparables en ciertos escenarios task-oriented. Además, el análisis cruzado muestra una baja transferibilidad entre dominios: modelos entrenados en un tipo de diálogo no generalizan bien al otro.\n\nEsta evidencia tiene implicaciones directas para evaluación psicométrica en LLMs y sistemas conversacionales: el contexto de interacción condiciona qué rasgos son inferibles y con qué fiabilidad. El artículo aporta una base metodológica sólida para evitar extrapolaciones indebidas y para diseñar protocolos de inferencia de personalidad sensibles al tipo de diálogo.",
    "id": "article-212",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T18:58:05.300Z",
    "evidence": {
      "checked_at": "2026-02-13T18:58:05.300Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-53989-y?error=cookies_not_supported&code=d995459b-2fd3-4882-b9e0-49df88e835a2",
      "fetched_title": "Personality prediction from task-oriented and open-domain human–machine dialogues - Scientific Reports",
      "title_similarity": 0.8462,
      "author_overlap": 0,
      "year_match": true
    },
    "title_canonical": "Personality prediction from task-oriented and open-domain human–machine dialogues - Scientific Reports"
  }
]