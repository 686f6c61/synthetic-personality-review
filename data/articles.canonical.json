[
  {
    "id": "article-001",
    "legacy_article_number": 1,
    "title_original": "Personality Traits in Large Language Models",
    "title_canonical": "Personality Traits in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Greg Serapio-García",
      "Mustafa Safdari",
      "Clément Crepy",
      "Luning Sun",
      "Stephen Fitz",
      "Peter Romero",
      "Marwa Abdulhai",
      "Aleksandra Faust",
      "Maja Matarić"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society",
      "Human-Computer Interaction"
    ],
    "source_url": "https://arxiv.org/abs/2307.00184",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.247Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.247Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2307.00184",
      "fetched_title": "Personality Traits in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language models have revolutionized natural language processing by enabling coherent and contextually relevant text generation. As these models increasingly power conversational agents, understanding and controlling their synthetic personality traits becomes critical. We present a comprehensive psychometrically valid methodology for administering personality tests to LLMs and shaping personality expression in generated text. Applying this method to 18 LLMs reveals that personality measurements are reliable and valid under specific prompting configurations, with stronger evidence in larger instruction-tuned models. The methodology successfully shapes LLM personality along desired dimensions to mimic specific human profiles, with implications for responsible AI deployment.",
    "abstract_en_extended": "This study, \"Personality Traits in Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language models have revolutionized natural language processing by enabling coherent and contextually relevant text generation. It then advances the context by clarifying that As these models increasingly power conversational agents, understanding and controlling their synthetic personality traits becomes critical. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We present a comprehensive psychometrically valid methodology for administering personality tests to LLMs and shaping personality expression in generated text. It also details that Applying this method to 18 LLMs reveals that personality measurements are reliable and valid under specific prompting configurations, with stronger evidence in larger instruction-tuned models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality Traits in Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los modelos de lenguaje de gran escala han transformado el procesamiento del lenguaje natural mediante la generación de texto coherente y contextualmente relevante. Además, contextualiza el aporte al precisar que Dado que estos modelos sustentan agentes conversacionales de uso público masivo, resulta crucial comprender y controlar los rasgos de personalidad sintéticos que exhiben. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se presenta una metodología psicométrica válida y fiable para administrar inventarios de personalidad a estos modelos y modular la expresión de rasgos en el texto generado. También se especifica que La aplicación del método a 18 modelos revela que las mediciones de personalidad resultan fiables y válidas bajo configuraciones específicas de instrucción, con evidencia más robusta en modelos de mayor escala refinados mediante ajuste instruccional. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language models have revolutionized natural language processing by enabling coherent and contextually relevant text generation.",
        "As these models increasingly power conversational agents, understanding and controlling their synthetic personality traits becomes critical.",
        "We present a comprehensive psychometrically valid methodology for administering personality tests to LLMs and shaping personality expression in generated text."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-003",
    "legacy_article_number": 3,
    "title_original": "LLMs Simulate Big Five Personality Traits: Further Evidence",
    "title_canonical": "LLMs Simulate Big Five Personality Traits: Further Evidence",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Aleksandra Sorokovikova",
      "Natalia Fedorova",
      "Sharwin Rezagholi",
      "Ivan P. Yamshchikov"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Big Five personality traits",
      "Large Language Models"
    ],
    "source_url": "https://arxiv.org/abs/2402.01765",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.288Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.288Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.01765",
      "fetched_title": "LLMs Simulate Big Five Personality Traits: Further Evidence",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We present an empirical investigation into Big Five personality trait simulation by Llama2, GPT-4, and Mixtral. The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction.",
    "abstract_en_extended": "This study, \"LLMs Simulate Big Five Personality Traits: Further Evidence\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We present an empirical investigation into Big Five personality trait simulation by Llama2, GPT-4, and Mixtral. It then advances the context by clarifying that The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction. It also details that The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"LLMs Simulate Big Five Personality Traits: Further Evidence\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta una investigación empírica sobre la simulación de rasgos de personalidad Big Five en los modelos Llama2, GPT-4 y Mixtral. Además, contextualiza el aporte al precisar que El estudio analiza los rasgos de personalidad simulados por estos modelos y su estabilidad temporal, contribuyendo a la comprensión de las capacidades de estos sistemas para simular rasgos de personalidad y sus implicaciones para la interacción persona-computadora personalizada. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El estudio analiza los rasgos de personalidad simulados por estos modelos y su estabilidad temporal, contribuyendo a la comprensión de las capacidades de estos sistemas para simular rasgos de personalidad y sus implicaciones para la interacción persona-computadora personalizada. También se especifica que El estudio analiza los rasgos de personalidad simulados por estos modelos y su estabilidad temporal, contribuyendo a la comprensión de las capacidades de estos sistemas para simular rasgos de personalidad y sus implicaciones para la interacción persona-computadora personalizada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We present an empirical investigation into Big Five personality trait simulation by Llama2, GPT-4, and Mixtral.",
        "The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-004",
    "legacy_article_number": 4,
    "title_original": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "title_canonical": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Guangyuan Jiang",
      "Manjie Xu",
      "Song-Chun Zhu",
      "Wenjuan Han",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "keywords": [
      "Language Models",
      "Personality Assessment",
      "Machine Behavior",
      "Psychometric Studies",
      "Big Five",
      "Machine Personality Inventory (MPI)"
    ],
    "source_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.289Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.289Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html",
      "fetched_title": "Evaluating and Inducing Personality in Pre-trained Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Standardized and quantified evaluation of machine behaviors constitutes a fundamental challenge in understanding LLMs. We introduce the Machine Personality Inventory (MPI), a tool grounded in Big Five Personality Factors theory for principled quantitative assessment of language model behaviors. Systematic evaluation with MPI provides evidence of its efficacy in characterizing LLM behaviors. We propose Personality Prompting (P²), a method enabling controlled induction of specific personalities, generating diverse and verifiable behavioral outputs. This work advocates personality assessment as a key indicator for evaluating machine behaviors across downstream applications.",
    "abstract_en_extended": "This study, \"Evaluating and Inducing Personality in Pre-trained Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Standardized and quantified evaluation of machine behaviors constitutes a fundamental challenge in understanding LLMs. It then advances the context by clarifying that We introduce the Machine Personality Inventory (MPI), a tool grounded in Big Five Personality Factors theory for principled quantitative assessment of language model behaviors. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Systematic evaluation with MPI provides evidence of its efficacy in characterizing LLM behaviors. It also details that We propose Personality Prompting (P²), a method enabling controlled induction of specific personalities, generating diverse and verifiable behavioral outputs. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating and Inducing Personality in Pre-trained Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La evaluación estandarizada y cuantificada de comportamientos en sistemas artificiales constituye un desafío fundamental para comprender los modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Se introduce el Inventario de Personalidad de Máquinas (MPI), una herramienta fundamentada en la teoría de los Cinco Grandes Factores de Personalidad para la evaluación cuantitativa y principista de comportamientos en modelos de lenguaje. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La evaluación sistemática mediante MPI proporciona evidencia de su eficacia para caracterizar comportamientos. También se especifica que Se propone el método de Inducción de Personalidad mediante Instrucciones (P²), que permite la inducción controlada de personalidades específicas, generando salidas conductuales diversas y verificables. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Standardized and quantified evaluation of machine behaviors constitutes a fundamental challenge in understanding LLMs.",
        "We introduce the Machine Personality Inventory (MPI), a tool grounded in Big Five Personality Factors theory for principled quantitative assessment of language model behaviors.",
        "Systematic evaluation with MPI provides evidence of its efficacy in characterizing LLM behaviors."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-006",
    "legacy_article_number": 6,
    "title_original": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
    "title_canonical": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Wenkai Li",
      "Jiarui Liu",
      "Andy Liu",
      "Xuhui Zhou",
      "Mona Diab",
      "Maarten Sap"
    ],
    "keywords": [
      "Computation and Language",
      "Personality traits",
      "Human-grounded data",
      "Personality assessment"
    ],
    "source_url": "https://arxiv.org/abs/2410.16491",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.405Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.405Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2410.16491",
      "fetched_title": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Embedding realistic human personality traits in LLMs remains challenging. Previous prompt-based approaches describing desired personality behaviors suffer from realism and validity issues. We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training methods to align LLMs with human personality patterns. Our methods outperform prompting on personality assessments including BFI and IPIP-NEO, with trait correlations more closely matching human data. Experiments reveal models trained toward higher conscientiousness, agreeableness, lower extraversion, and lower neuroticism exhibit superior reasoning performance, consistent with psychological findings on these traits' cognitive impact.",
    "abstract_en_extended": "This study, \"BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Embedding realistic human personality traits in LLMs remains challenging. It then advances the context by clarifying that Previous prompt-based approaches describing desired personality behaviors suffer from realism and validity issues. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. It also details that Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training methods to align LLMs with human personality patterns. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La incorporación de rasgos de personalidad humana realistas en modelos de lenguaje de gran escala constituye un desafío considerable. Además, contextualiza el aporte al precisar que Los enfoques previos basados en instrucciones que describen comportamientos asociados a rasgos deseados adolecen de problemas de realismo y validez. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se introduce BIG5-CHAT, un conjunto de datos a gran escala con 100,000 diálogos que fundamentan los modelos en patrones auténticos de expresión de personalidad humana. También se especifica que Mediante este conjunto de datos se exploran el ajuste fino supervisado y la optimización directa de preferencias como métodos de entrenamiento para alinear los modelos con patrones de personalidad humana. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Embedding realistic human personality traits in LLMs remains challenging.",
        "Previous prompt-based approaches describing desired personality behaviors suffer from realism and validity issues.",
        "We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-007",
    "legacy_article_number": 7,
    "title_original": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data",
    "title_canonical": "BIG5-CHAT: Shaping LLM Personalities Through Training on...",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Wenkai Li",
      "Jiarui Liu",
      "Andy Liu",
      "Xuhui Zhou",
      "Mona Diab",
      "Maarten Sap"
    ],
    "keywords": [
      "Computation and Language",
      "Personality traits",
      "Human-grounded data"
    ],
    "source_url": "https://openreview.net/forum?id=TqwTzLjzGS",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.408Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.408Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://openreview.net/forum?id=TqwTzLjzGS",
      "fetched_title": "BIG5-CHAT: Shaping LLM Personalities Through Training on...",
      "title_similarity": 0.7273,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Embedding realistic human personality traits in LLMs remains challenging. We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. Training-based methods explored here align LLMs with human personality patterns, outperforming prompting on personality assessments. Experiments reveal models trained to exhibit certain traits display superior reasoning performance, consistent with psychological findings.",
    "abstract_en_extended": "This study, \"BIG5-CHAT: Shaping LLM Personalities Through Training on...\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Embedding realistic human personality traits in LLMs remains challenging. It then advances the context by clarifying that We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Training-based methods explored here align LLMs with human personality patterns, outperforming prompting on personality assessments. It also details that Experiments reveal models trained to exhibit certain traits display superior reasoning performance, consistent with psychological findings. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"BIG5-CHAT: Shaping LLM Personalities Through Training on...\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La incorporación de rasgos de personalidad humana realistas en modelos de lenguaje de gran escala constituye un desafío considerable. Además, contextualiza el aporte al precisar que Se introduce BIG5-CHAT, un conjunto de datos a gran escala con 100,000 diálogos que fundamentan los modelos en patrones auténticos de expresión de personalidad humana. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los métodos de entrenamiento explorados alinean los modelos con patrones de personalidad humana, superando la inducción mediante instrucciones en evaluaciones de personalidad. También se especifica que Los experimentos revelan que los modelos entrenados para exhibir ciertos rasgos muestran desempeño superior en razonamiento, coherente con evidencia psicológica. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Embedding realistic human personality traits in LLMs remains challenging.",
        "We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns.",
        "Training-based methods explored here align LLMs with human personality patterns, outperforming prompting on personality assessments."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-008",
    "legacy_article_number": 8,
    "title_original": "On the Reliability of Psychological Scales on Large Language Models",
    "title_canonical": "On the Reliability of Psychological Scales on Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Jen-tse Huang",
      "Wenxiang Jiao",
      "Man Ho Lam",
      "Eric John Li",
      "Wenxuan Wang",
      "Michael Lyu"
    ],
    "keywords": [
      "Large Language Models",
      "Psychological assessment",
      "Personality traits",
      "Big Five Inventory",
      "Personality emulation"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-main.354/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.451Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.451Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-main.354/",
      "fetched_title": "On the Reliability of Psychological Scales on Large Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Determining the reliability of personality assessments applied to LLMs remains a fundamental methodological question. Analysis across 2,500 configuration settings per model reveals various LLMs demonstrate response consistency on the Big Five Inventory, indicating satisfactory reliability levels. The research further explores GPT-3.5's capacity to emulate diverse personality profiles and represent distinct demographic groups through targeted instruction configurations.",
    "abstract_en_extended": "This study, \"On the Reliability of Psychological Scales on Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Determining the reliability of personality assessments applied to LLMs remains a fundamental methodological question. It then advances the context by clarifying that Analysis across 2,500 configuration settings per model reveals various LLMs demonstrate response consistency on the Big Five Inventory, indicating satisfactory reliability levels. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The research further explores GPT-3.5's capacity to emulate diverse personality profiles and represent distinct demographic groups through targeted instruction configurations. It also details that The research further explores GPT-3.5's capacity to emulate diverse personality profiles and represent distinct demographic groups through targeted instruction configurations. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"On the Reliability of Psychological Scales on Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La determinación de la fiabilidad de evaluaciones de personalidad aplicadas a modelos de lenguaje de gran escala constituye una cuestión metodológica fundamental. Además, contextualiza el aporte al precisar que El análisis de 2,500 configuraciones por modelo revela que diversos modelos demuestran coherencia en las respuestas al Inventario Big Five, indicando niveles satisfactorios de fiabilidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La investigación explora además la capacidad de GPT-3.5 para emular perfiles de personalidad diversos y representar grupos demográficos diferenciados mediante configuraciones de instrucciones específicas. También se especifica que La investigación explora además la capacidad de GPT-3.5 para emular perfiles de personalidad diversos y representar grupos demográficos diferenciados mediante configuraciones de instrucciones específicas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Determining the reliability of personality assessments applied to LLMs remains a fundamental methodological question.",
        "Analysis across 2,500 configuration settings per model reveals various LLMs demonstrate response consistency on the Big Five Inventory, indicating satisfactory reliability levels.",
        "The research further explores GPT-3.5's capacity to emulate diverse personality profiles and represent distinct demographic groups through targeted instruction configurations."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-009",
    "legacy_article_number": 9,
    "title_original": "Manipulating the Perceived Personality Traits of Language Models",
    "title_canonical": "Manipulating the Perceived Personality Traits of Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Graham Caron",
      "Shashank Srivastava"
    ],
    "keywords": [
      "Personality traits",
      "Big Five personality model",
      "Language models",
      "Dialog systems",
      "Computational psychology"
    ],
    "source_url": "https://aclanthology.org/2023.findings-emnlp.156/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:12.807Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:12.807Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.findings-emnlp.156/",
      "fetched_title": "Manipulating the Perceived Personality Traits of Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "We explore whether LLM-generated text exhibits consistency in perceived Big Five personality traits. When exposed to diverse contexts including personality descriptions and diagnostic questionnaires, language models consistently identify and reflect personality markers. This demonstrates predictable malleability, with correlations reaching 0.84 between intended and realized personality shifts.",
    "abstract_en_extended": "This study, \"Manipulating the Perceived Personality Traits of Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We explore whether LLM-generated text exhibits consistency in perceived Big Five personality traits. It then advances the context by clarifying that When exposed to diverse contexts including personality descriptions and diagnostic questionnaires, language models consistently identify and reflect personality markers. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This demonstrates predictable malleability, with correlations reaching 0.84 between intended and realized personality shifts. It also details that This demonstrates predictable malleability, with correlations reaching 0.84 between intended and realized personality shifts. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Manipulating the Perceived Personality Traits of Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora si el texto generado por modelos de lenguaje de gran escala exhibe coherencia en los rasgos de personalidad Big Five percibidos. Además, contextualiza el aporte al precisar que Cuando se exponen a contextos diversos que incluyen descripciones de personalidad y cuestionarios diagnósticos, los modelos de lenguaje identifican y reflejan de forma coherente marcadores de personalidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Esto demuestra maleabilidad predecible, con correlaciones que alcanzan 0.84 entre cambios de personalidad pretendidos y realizados. También se especifica que Esto demuestra maleabilidad predecible, con correlaciones que alcanzan 0.84 entre cambios de personalidad pretendidos y realizados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We explore whether LLM-generated text exhibits consistency in perceived Big Five personality traits.",
        "When exposed to diverse contexts including personality descriptions and diagnostic questionnaires, language models consistently identify and reflect personality markers.",
        "This demonstrates predictable malleability, with correlations reaching 0.84 between intended and realized personality shifts."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-010",
    "legacy_article_number": 10,
    "title_original": "Who is GPT-3? An Exploration of Personality, Values and Demographics",
    "title_canonical": "Who is GPT-3? An exploration of personality, values and demographics",
    "category": "Evaluación y validación psicométrica",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Marilù Miotto",
      "Nicola Rossberg",
      "Bennett Kleinberg"
    ],
    "keywords": [
      "Language models",
      "GPT-3",
      "Psychological assessment",
      "Personality traits",
      "Computational social science"
    ],
    "source_url": "https://aclanthology.org/2022.nlpcss-1.24/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.057Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.057Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2022.nlpcss-1.24/",
      "fetched_title": "Who is GPT-3? An exploration of personality, values and demographics",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "We administer two validated psychometric instruments to GPT-3 for assessing personality, values, and self-reported demographics. Results demonstrate GPT-3 scores comparably to human samples regarding personality and values when provided with response memory. This constitutes the first empirical psychological assessment of the GPT-3 model.",
    "abstract_en_extended": "This study, \"Who is GPT-3? An exploration of personality, values and demographics\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We administer two validated psychometric instruments to GPT-3 for assessing personality, values, and self-reported demographics. It then advances the context by clarifying that Results demonstrate GPT-3 scores comparably to human samples regarding personality and values when provided with response memory. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This constitutes the first empirical psychological assessment of the GPT-3 model. It also details that This constitutes the first empirical psychological assessment of the GPT-3 model. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Who is GPT-3? An exploration of personality, values and demographics\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se administran dos instrumentos psicométricos validados a GPT-3 para evaluar personalidad, valores y demografía autorreportada. Además, contextualiza el aporte al precisar que Los resultados demuestran que GPT-3 puntúa de forma comparable a muestras humanas respecto a personalidad y valores cuando se le proporciona memoria de respuestas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Esto constituye la primera evaluación psicológica empírica del modelo GPT-3. También se especifica que Esto constituye la primera evaluación psicológica empírica del modelo GPT-3. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We administer two validated psychometric instruments to GPT-3 for assessing personality, values, and self-reported demographics.",
        "Results demonstrate GPT-3 scores comparably to human samples regarding personality and values when provided with response memory.",
        "This constitutes the first empirical psychological assessment of the GPT-3 model."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-012",
    "legacy_article_number": 12,
    "title_original": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories",
    "title_canonical": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PMC",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Max Pellert",
      "Clemens M Lechner",
      "Claudia Wagner",
      "Beatrice Rammstedt",
      "Markus Strohmaier"
    ],
    "keywords": [
      "Artificial intelligence",
      "Psychometrics",
      "Natural language processing",
      "Personality",
      "Values",
      "Moral foundations",
      "Gender diversity beliefs"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11373167/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.170Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.170Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11373167/",
      "fetched_title": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PMC",
      "title_similarity": 0.9286,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. These models inadvertently acquire psychological characteristics from vast training corpora. Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments.",
    "abstract_en_extended": "This study, \"AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PMC\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. It then advances the context by clarifying that These models inadvertently acquire psychological characteristics from vast training corpora. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. It also details that We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PMC\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los inventarios psicométricos estándar pueden reutilizarse como instrumentos diagnósticos para evaluar rasgos psicológicos en modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Estos modelos adquieren de forma inadvertida características psicológicas desde los amplios corpus de entrenamiento. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Obtener respuestas de los modelos a inventarios psicométricos permite a los investigadores caracterizar sus rasgos latentes. También se especifica que Se demuestran metodologías de clasificación sin entrenamiento previo a través de múltiples modelos e instrumentos psicométricos establecidos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs.",
        "These models inadvertently acquire psychological characteristics from vast training corpora.",
        "Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-013",
    "legacy_article_number": 13,
    "title_original": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories",
    "title_canonical": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PubMed",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Max Pellert",
      "Clemens M Lechner",
      "Claudia Wagner",
      "Beatrice Rammstedt",
      "Markus Strohmaier"
    ],
    "keywords": [
      "Artificial intelligence",
      "Psychometrics",
      "Natural language processing",
      "Personality",
      "Values"
    ],
    "source_url": "https://pubmed.ncbi.nlm.nih.gov/38165766/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.189Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.189Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pubmed.ncbi.nlm.nih.gov/38165766/",
      "fetched_title": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PubMed",
      "title_similarity": 0.9286,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. These models inadvertently acquire psychological characteristics from vast training corpora. Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments.",
    "abstract_en_extended": "This study, \"AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PubMed\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. It then advances the context by clarifying that These models inadvertently acquire psychological characteristics from vast training corpora. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. It also details that We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories - PubMed\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los inventarios psicométricos estándar pueden reutilizarse como instrumentos diagnósticos para evaluar rasgos psicológicos en modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Estos modelos adquieren de forma inadvertida características psicológicas desde los amplios corpus de entrenamiento. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Obtener respuestas de los modelos a inventarios psicométricos permite a los investigadores caracterizar sus rasgos latentes. También se especifica que Se demuestran metodologías de clasificación sin entrenamiento previo a través de múltiples modelos e instrumentos psicométricos establecidos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs.",
        "These models inadvertently acquire psychological characteristics from vast training corpora.",
        "Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-014",
    "legacy_article_number": 14,
    "title_original": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset Designed for LLMs with Psychometrics",
    "title_canonical": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Seungbeen Lee",
      "Seungwon Lim",
      "Seungju Han",
      "Giyeong Oh",
      "Hyungjoo Chae",
      "Jiwan Chung",
      "Minju Kim",
      "Beong-woo Kwak",
      "Yeonsoo Lee",
      "Dongha Lee",
      "Jinyoung Yeo",
      "Youngjae Yu"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Assessment",
      "Psychometrics"
    ],
    "source_url": "https://arxiv.org/abs/2406.14703",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.408Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.408Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.14703",
      "fetched_title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We introduce TRAIT, a benchmark comprising 8,000 multiple-choice questions for assessing LLM personality. TRAIT builds upon psychometrically validated questionnaires enhanced with the ATOMIC-10X knowledge graph. Results reveal LLMs exhibit distinct and consistent personalities strongly influenced by training data. Current instruction techniques demonstrate limited effectiveness in eliciting specific traits.",
    "abstract_en_extended": "This study, \"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We introduce TRAIT, a benchmark comprising 8,000 multiple-choice questions for assessing LLM personality. It then advances the context by clarifying that TRAIT builds upon psychometrically validated questionnaires enhanced with the ATOMIC-10X knowledge graph. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results reveal LLMs exhibit distinct and consistent personalities strongly influenced by training data. It also details that Current instruction techniques demonstrate limited effectiveness in eliciting specific traits. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce TRAIT, una batería de evaluación que comprende 8,000 preguntas de opción múltiple para evaluar personalidad en modelos de lenguaje. Además, contextualiza el aporte al precisar que TRAIT se construye sobre cuestionarios psicométricamente validados mejorados con el grafo de conocimiento ATOMIC-10X. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados revelan que los modelos exhiben personalidades diferenciadas y coherentes, fuertemente influenciadas por los datos de entrenamiento. También se especifica que Las técnicas de instrucción actuales demuestran efectividad limitada para elicitar rasgos específicos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We introduce TRAIT, a benchmark comprising 8,000 multiple-choice questions for assessing LLM personality.",
        "TRAIT builds upon psychometrically validated questionnaires enhanced with the ATOMIC-10X knowledge graph.",
        "Results reveal LLMs exhibit distinct and consistent personalities strongly influenced by training data."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-015",
    "legacy_article_number": 15,
    "title_original": "Evaluating the Alignment of LLMs on Personality Inference from Real-World Interview Data",
    "title_canonical": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jianfeng Zhu",
      "Julina Maharjan",
      "Xinyu Li",
      "Karin G. Coifman",
      "Ruoming Jin"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "Personality Inference",
      "Big Five Personality Traits",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2509.13244",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.412Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.412Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.13244",
      "fetched_title": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data",
      "title_similarity": 0.7143,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We introduce a benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. Systematic evaluation examines LLM performance across zero-shot instruction, fine-tuning, and regression methodologies. Results demonstrate all correlations between model predictions and ground-truth personality traits remain below 0.26, evidencing limited alignment of current LLMs with validated psychological constructs.",
    "abstract_en_extended": "This study, \"Evaluating LLM Alignment on Personality Inference from Real-World Interview Data\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We introduce a benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. It then advances the context by clarifying that Systematic evaluation examines LLM performance across zero-shot instruction, fine-tuning, and regression methodologies. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results demonstrate all correlations between model predictions and ground-truth personality traits remain below 0.26, evidencing limited alignment of current LLMs with validated psychological constructs. It also details that Results demonstrate all correlations between model predictions and ground-truth personality traits remain below 0.26, evidencing limited alignment of current LLMs with validated psychological constructs. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating LLM Alignment on Personality Inference from Real-World Interview Data\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce una batería de evaluación que comprende transcripciones de entrevistas semiestructuradas pareadas con puntuaciones validadas de rasgos Big Five continuos. Además, contextualiza el aporte al precisar que La evaluación sistemática examina el desempeño de modelos a través de instrucción sin entrenamiento previo, ajuste fino y metodologías de regresión. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados demuestran que todas las correlaciones entre predicciones del modelo y rasgos de personalidad reales permanecen por debajo de 0.26, evidenciando alineamiento limitado de los modelos actuales con constructos psicológicos validados. También se especifica que Los resultados demuestran que todas las correlaciones entre predicciones del modelo y rasgos de personalidad reales permanecen por debajo de 0.26, evidenciando alineamiento limitado de los modelos actuales con constructos psicológicos validados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We introduce a benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores.",
        "Systematic evaluation examines LLM performance across zero-shot instruction, fine-tuning, and regression methodologies.",
        "Results demonstrate all correlations between model predictions and ground-truth personality traits remain below 0.26, evidencing limited alignment of current LLMs with validated psychological constructs."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-016",
    "legacy_article_number": 16,
    "title_original": "Evaluating the Capability of Large Language Models in Emulating Personality",
    "title_canonical": "Evaluating the ability of large language models to emulate personality - Scientific Reports",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yilei Wang",
      "Jiabao Zhao",
      "Derek Siyuan Ong",
      "Xuguang Xu",
      "Lun Hong"
    ],
    "keywords": [
      "Large Language Models",
      "Personality emulation",
      "GPT-4",
      "Big Five personality profiles",
      "Role-playing"
    ],
    "source_url": "https://www.nature.com/articles/s41598-024-84109-5?error=cookies_not_supported&code=00ececb8-c902-4dd7-9fb2-68e39ceeac8f",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.424Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.424Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-84109-5?error=cookies_not_supported&code=00ececb8-c902-4dd7-9fb2-68e39ceeac8f",
      "fetched_title": "Evaluating the ability of large language models to emulate personality - Scientific Reports",
      "title_similarity": 0.4667,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "We present simulation studies evaluating GPT-4's capacity for role-playing individuals with diverse Big Five personality profiles. Emulated personality responses demonstrate superior internal consistency and more distinct factorial organization compared with human participants. These emulated scores exhibit remarkably high convergent validity with human self-reported personality assessments.",
    "abstract_en_extended": "This study, \"Evaluating the ability of large language models to emulate personality - Scientific Reports\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We present simulation studies evaluating GPT-4's capacity for role-playing individuals with diverse Big Five personality profiles. It then advances the context by clarifying that Emulated personality responses demonstrate superior internal consistency and more distinct factorial organization compared with human participants. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that These emulated scores exhibit remarkably high convergent validity with human self-reported personality assessments. It also details that These emulated scores exhibit remarkably high convergent validity with human self-reported personality assessments. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating the ability of large language models to emulate personality - Scientific Reports\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presentan estudios de simulación que evalúan la capacidad de GPT-4 para emular individuos con perfiles de personalidad Big Five diversos. Además, contextualiza el aporte al precisar que Las respuestas de personalidad emuladas demuestran coherencia interna superior y organización factorial más diferenciada en comparación con participantes humanos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Estas puntuaciones emuladas exhiben validez convergente notablemente elevada con evaluaciones de personalidad autorreportadas por humanos. También se especifica que Estas puntuaciones emuladas exhiben validez convergente notablemente elevada con evaluaciones de personalidad autorreportadas por humanos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We present simulation studies evaluating GPT-4's capacity for role-playing individuals with diverse Big Five personality profiles.",
        "Emulated personality responses demonstrate superior internal consistency and more distinct factorial organization compared with human participants.",
        "These emulated scores exhibit remarkably high convergent validity with human self-reported personality assessments."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-017",
    "legacy_article_number": 17,
    "title_original": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
    "title_canonical": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jivnesh Sandhan",
      "Fei Cheng",
      "Tushar Sandhan",
      "Yugo Murawaki"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "Personality Evaluation",
      "Context-Aware Analysis"
    ],
    "source_url": "https://arxiv.org/abs/2508.20385",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.427Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.427Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.20385",
      "fetched_title": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. Experiments across 7 LLMs reveal conversational history enhances response consistency through in-context learning while simultaneously inducing personality shifts. We introduce novel metrics quantifying LLM response consistency, a fundamental behavioral trait.",
    "abstract_en_extended": "This study, \"CAPE: Context-Aware Personality Evaluation Framework for Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. It then advances the context by clarifying that Experiments across 7 LLMs reveal conversational history enhances response consistency through in-context learning while simultaneously inducing personality shifts. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We introduce novel metrics quantifying LLM response consistency, a fundamental behavioral trait. It also details that We introduce novel metrics quantifying LLM response consistency, a fundamental behavioral trait. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"CAPE: Context-Aware Personality Evaluation Framework for Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone el primer marco de Evaluación de Personalidad Sensible al Contexto (CAPE) para modelos de lenguaje, incorporando interacciones conversacionales previas. Además, contextualiza el aporte al precisar que Los experimentos a través de 7 modelos revelan que el historial conversacional incrementa la coherencia de respuestas mediante aprendizaje en contexto mientras induce simultáneamente desplazamientos de personalidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se introducen métricas novedosas que cuantifican la coherencia de respuestas, un rasgo conductual fundamental. También se especifica que Se introducen métricas novedosas que cuantifican la coherencia de respuestas, un rasgo conductual fundamental. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions.",
        "Experiments across 7 LLMs reveal conversational history enhances response consistency through in-context learning while simultaneously inducing personality shifts.",
        "We introduce novel metrics quantifying LLM response consistency, a fundamental behavioral trait."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-018",
    "legacy_article_number": 18,
    "title_original": "Scaling Personality Control in LLMs with Big Five Scaling Prompts",
    "title_canonical": "Scaling Personality Control in LLMs with Big Five Scaler Prompts",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Gunhee Cho",
      "Yun-Gyung Cheong"
    ],
    "keywords": [
      "Computation and Language",
      "Multiagent Systems",
      "Big Five personality traits",
      "Prompt engineering"
    ],
    "source_url": "https://arxiv.org/abs/2508.06149",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.443Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.443Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.06149",
      "fetched_title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts",
      "title_similarity": 0.9,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We present Big5-Scaler, an instruction-based framework for conditioning LLMs with controllable Big Five personality traits. Embedding numeric trait values into natural language instructions enables fine-grained personality control without additional training. Results demonstrate consistent induction of distinguishable personality traits across models, with performance varying by instruction type and scale.",
    "abstract_en_extended": "This study, \"Scaling Personality Control in LLMs with Big Five Scaler Prompts\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We present Big5-Scaler, an instruction-based framework for conditioning LLMs with controllable Big Five personality traits. It then advances the context by clarifying that Embedding numeric trait values into natural language instructions enables fine-grained personality control without additional training. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results demonstrate consistent induction of distinguishable personality traits across models, with performance varying by instruction type and scale. It also details that Results demonstrate consistent induction of distinguishable personality traits across models, with performance varying by instruction type and scale. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Scaling Personality Control in LLMs with Big Five Scaler Prompts\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta Big5-Scaler, un marco basado en instrucciones para condicionar modelos de lenguaje con rasgos de personalidad Big Five controlables. Además, contextualiza el aporte al precisar que La incrustación de valores numéricos de rasgos en instrucciones de lenguaje natural permite control de personalidad de grano fino sin entrenamiento adicional. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados demuestran inducción coherente de rasgos de personalidad diferenciables a través de modelos, con desempeño variable según tipo e intensidad de instrucción. También se especifica que Los resultados demuestran inducción coherente de rasgos de personalidad diferenciables a través de modelos, con desempeño variable según tipo e intensidad de instrucción. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We present Big5-Scaler, an instruction-based framework for conditioning LLMs with controllable Big Five personality traits.",
        "Embedding numeric trait values into natural language instructions enables fine-grained personality control without additional training.",
        "Results demonstrate consistent induction of distinguishable personality traits across models, with performance varying by instruction type and scale."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-019",
    "legacy_article_number": 19,
    "title_original": "Predicting Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
    "title_canonical": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yang Yan",
      "Lizhi Ma",
      "Anqi Li",
      "Jingsong Ma",
      "Zhenzhong Lan"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computational Psychometrics",
      "Personality Trait Prediction"
    ],
    "source_url": "https://arxiv.org/abs/2406.17287",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.457Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.457Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.17287",
      "fetched_title": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
      "title_similarity": 0.9286,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We examine whether LLMs can predict Big Five personality traits directly from counseling dialogues. The framework applies role-playing and questionnaire-based instruction to condition LLMs on counseling sessions. Evaluation across 853 real-world sessions reveals significant correlation between LLM-predicted and actual Big Five traits. Fine-tuned Llama3-8B achieves 130.95% improvement, surpassing Qwen1.5-110B by 36.94%.",
    "abstract_en_extended": "This study, \"Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We examine whether LLMs can predict Big Five personality traits directly from counseling dialogues. It then advances the context by clarifying that The framework applies role-playing and questionnaire-based instruction to condition LLMs on counseling sessions. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Evaluation across 853 real-world sessions reveals significant correlation between LLM-predicted and actual Big Five traits. It also details that Fine-tuned Llama3-8B achieves 130.95% improvement, surpassing Qwen1.5-110B by 36.94%. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se examina si los modelos de lenguaje pueden predecir rasgos de personalidad Big Five directamente desde diálogos de orientación psicológica. Además, contextualiza el aporte al precisar que El marco aplica simulación de roles e instrucciones basadas en cuestionarios para condicionar los modelos sobre sesiones de orientación. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La evaluación a través de 853 sesiones reales revela correlación significativa entre rasgos Big Five predichos por el modelo y reales. También se especifica que Llama3-8B con ajuste fino logra mejora del 130.95%, superando a Qwen1.5-110B en 36.94%. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We examine whether LLMs can predict Big Five personality traits directly from counseling dialogues.",
        "The framework applies role-playing and questionnaire-based instruction to condition LLMs on counseling sessions.",
        "Evaluation across 853 real-world sessions reveals significant correlation between LLM-predicted and actual Big Five traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-020",
    "legacy_article_number": 20,
    "title_original": "A Framework for the Early Phases of Personality Test Development Using Large Language Models and Artificial Personas",
    "title_canonical": "A framework for the initial phases of personality test development using large language models and artificial personas",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Patrick M. Markey",
      "Hanna Campbell",
      "Samantha Goldman"
    ],
    "keywords": [
      "Large Language Models",
      "Personality test development",
      "Artificial personas",
      "Psychometrics",
      "Five-Factor Model",
      "Self-esteem"
    ],
    "source_url": "https://doi.org/10.1016/j.jrp.2025.104647",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.473Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.473Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1016/j.jrp.2025.104647",
      "fetched_title": "A framework for the initial phases of personality test development using large language models and artificial personas",
      "title_similarity": 0.8824,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We explore LLM applications in early-phase personality test construction, presenting a methodology for efficient assessment of item relevance to psychological constructs. Study 1 employed artificial personas for evaluating personality test items; Study 2 validated resulting scales with 449 human participants. AI-generated scales demonstrated satisfactory internal consistency and robust correlations with established psychometric instruments.",
    "abstract_en_extended": "This study, \"A framework for the initial phases of personality test development using large language models and artificial personas\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We explore LLM applications in early-phase personality test construction, presenting a methodology for efficient assessment of item relevance to psychological constructs. It then advances the context by clarifying that Study 1 employed artificial personas for evaluating personality test items; Study 2 validated resulting scales with 449 human participants. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that AI-generated scales demonstrated satisfactory internal consistency and robust correlations with established psychometric instruments. It also details that AI-generated scales demonstrated satisfactory internal consistency and robust correlations with established psychometric instruments. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"A framework for the initial phases of personality test development using large language models and artificial personas\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se exploran aplicaciones de modelos de lenguaje en la construcción de inventarios de personalidad en fases iniciales, presentando una metodología para evaluación eficiente de relevancia de ítems a constructos psicológicos. Además, contextualiza el aporte al precisar que El Estudio 1 empleó personas artificiales para evaluar ítems de inventarios de personalidad; el Estudio 2 validó las escalas resultantes con 449 participantes humanos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las escalas generadas por IA demostraron coherencia interna satisfactoria y correlaciones robustas con instrumentos psicométricos establecidos. También se especifica que Las escalas generadas por IA demostraron coherencia interna satisfactoria y correlaciones robustas con instrumentos psicométricos establecidos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We explore LLM applications in early-phase personality test construction, presenting a methodology for efficient assessment of item relevance to psychological constructs.",
        "Study 1 employed artificial personas for evaluating personality test items; Study 2 validated resulting scales with 449 human participants.",
        "AI-generated scales demonstrated satisfactory internal consistency and robust correlations with established psychometric instruments."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-021",
    "legacy_article_number": 21,
    "title_original": "On the Emergent Capabilities of ChatGPT 4 to Estimate Personality Traits",
    "title_canonical": "Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Marco Piastra",
      "Patrizia Catellani"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Personality Traits",
      "Large Language Models",
      "Big Five",
      "Text Analysis",
      "ChatGPT 4"
    ],
    "source_url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1484260/full",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:13.478Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:13.478Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1484260/full",
      "fetched_title": "Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits",
      "title_similarity": 0.9091,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "We investigate ChatGPT-4's potential for assessing personality traits from written texts. Using two datasets containing texts and Big Five-based self-assessments, we evaluate ChatGPT-4's predictive performance. Results demonstrate moderate yet significant capacities for automatically inferring personality traits from written text, though with limitations in recognizing input text appropriateness for accurate inference.",
    "abstract_en_extended": "This study, \"Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We investigate ChatGPT-4's potential for assessing personality traits from written texts. It then advances the context by clarifying that Using two datasets containing texts and Big Five-based self-assessments, we evaluate ChatGPT-4's predictive performance. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results demonstrate moderate yet significant capacities for automatically inferring personality traits from written text, though with limitations in recognizing input text appropriateness for accurate inference. It also details that Results demonstrate moderate yet significant capacities for automatically inferring personality traits from written text, though with limitations in recognizing input text appropriateness for accurate inference. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Frontiers | On the emergent capabilities of ChatGPT 4 to estimate personality traits\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga el potencial de ChatGPT-4 para evaluar rasgos de personalidad desde textos escritos. Además, contextualiza el aporte al precisar que Utilizando dos conjuntos de datos que contienen textos y autoevaluaciones basadas en Big Five, se evalúa el desempeño predictivo de ChatGPT-4. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados demuestran capacidades moderadas pero significativas para inferir automáticamente rasgos de personalidad desde texto escrito, aunque con limitaciones para reconocer la adecuación del texto de entrada para inferencia precisa. También se especifica que Los resultados demuestran capacidades moderadas pero significativas para inferir automáticamente rasgos de personalidad desde texto escrito, aunque con limitaciones para reconocer la adecuación del texto de entrada para inferencia precisa. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We investigate ChatGPT-4's potential for assessing personality traits from written texts.",
        "Using two datasets containing texts and Big Five-based self-assessments, we evaluate ChatGPT-4's predictive performance.",
        "Results demonstrate moderate yet significant capacities for automatically inferring personality traits from written text, though with limitations in recognizing input text appropriateness for accurate inference."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-022",
    "legacy_article_number": 22,
    "title_original": "Personality as a Probe for LLM Evaluation: Method Tradeoffs and Aftereffects",
    "title_canonical": "Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Gunmay Handa",
      "Zekun Wu",
      "Adriano Koshiyama",
      "Philip Treleaven"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "Personality Manipulation",
      "Big Five Traits",
      "Machine Learning Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2509.04794",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.012Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.012Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.04794",
      "fetched_title": "Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects",
      "title_similarity": 0.5714,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We present a systematic study of personality control via Big Five traits, comparing in-context learning, parameter-efficient fine-tuning, and mechanistic steering. Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness.",
    "abstract_en_extended": "This study, \"Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We present a systematic study of personality control via Big Five traits, comparing in-context learning, parameter-efficient fine-tuning, and mechanistic steering. It then advances the context by clarifying that Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness. It also details that Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta un estudio sistemático de control de personalidad mediante rasgos Big Five, comparando aprendizaje en contexto, ajuste fino eficiente en parámetros y dirección mecanicista. Además, contextualiza el aporte al precisar que Emergen compensaciones claras: el aprendizaje en contexto logra alineamiento robusto con degradación mínima de capacidades; el ajuste fino eficiente en parámetros proporciona alineamiento máximo a expensas del desempeño en tareas; la dirección mecanicista ofrece control ligero en tiempo de ejecución con efectividad competitiva. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Emergen compensaciones claras: el aprendizaje en contexto logra alineamiento robusto con degradación mínima de capacidades; el ajuste fino eficiente en parámetros proporciona alineamiento máximo a expensas del desempeño en tareas; la dirección mecanicista ofrece control ligero en tiempo de ejecución con efectividad competitiva. También se especifica que Emergen compensaciones claras: el aprendizaje en contexto logra alineamiento robusto con degradación mínima de capacidades; el ajuste fino eficiente en parámetros proporciona alineamiento máximo a expensas del desempeño en tareas; la dirección mecanicista ofrece control ligero en tiempo de ejecución con efectividad competitiva. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We present a systematic study of personality control via Big Five traits, comparing in-context learning, parameter-efficient fine-tuning, and mechanistic steering.",
        "Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-024",
    "legacy_article_number": 24,
    "title_original": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models",
    "title_canonical": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Keyu Pan",
      "Yawen Zeng"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Assessment",
      "Myers-Briggs Type Indicator (MBTI)",
      "Prompt Engineering",
      "Artificial Intelligence"
    ],
    "source_url": "https://arxiv.org/abs/2307.16180",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.027Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.027Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2307.16180",
      "fetched_title": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We investigate MBTI feasibility as an evaluation metric for LLMs. Extensive experiments explore personality types across different LLMs, personality type modification through instruction engineering, and training data effects on model personality. Although MBTI lacks psychometric rigor, it can reflect similarity between LLM and human personality patterns.",
    "abstract_en_extended": "This study, \"Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We investigate MBTI feasibility as an evaluation metric for LLMs. It then advances the context by clarifying that Extensive experiments explore personality types across different LLMs, personality type modification through instruction engineering, and training data effects on model personality. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Although MBTI lacks psychometric rigor, it can reflect similarity between LLM and human personality patterns. It also details that Although MBTI lacks psychometric rigor, it can reflect similarity between LLM and human personality patterns. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga la viabilidad de MBTI como métrica de evaluación para modelos de lenguaje. Además, contextualiza el aporte al precisar que Los experimentos extensivos exploran tipos de personalidad a través de diferentes modelos, modificación de tipos de personalidad mediante ingeniería de instrucciones, y efectos de datos de entrenamiento sobre personalidad del modelo. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aunque MBTI carece de rigor psicométrico, puede reflejar similitud entre patrones de personalidad de modelos y humanos. También se especifica que Aunque MBTI carece de rigor psicométrico, puede reflejar similitud entre patrones de personalidad de modelos y humanos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We investigate MBTI feasibility as an evaluation metric for LLMs.",
        "Extensive experiments explore personality types across different LLMs, personality type modification through instruction engineering, and training data effects on model personality.",
        "Although MBTI lacks psychometric rigor, it can reflect similarity between LLM and human personality patterns."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-026",
    "legacy_article_number": 26,
    "title_original": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
    "title_canonical": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Haocong Rao",
      "Cyril Leung",
      "Chunyan Miao"
    ],
    "keywords": [
      "Large Language Models",
      "ChatGPT",
      "Personality Assessment",
      "Myers-Briggs Type Indicator (MBTI)",
      "AI Psychology"
    ],
    "source_url": "https://aclanthology.org/2023.findings-emnlp.84/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.043Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.043Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.findings-emnlp.84/",
      "fetched_title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "We present a generic evaluation framework enabling LLMs to assess human personalities through MBTI. The framework devises unbiased instructions, enables flexible queries across subjects, and reformulates questions for enhanced response clarity. Experiments reveal ChatGPT's capacity for personality assessment with more consistent and equitable evaluations, despite lower robustness to instruction biases compared with InstructGPT.",
    "abstract_en_extended": "This study, \"Can ChatGPT Assess Human Personalities? A General Evaluation Framework\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We present a generic evaluation framework enabling LLMs to assess human personalities through MBTI. It then advances the context by clarifying that The framework devises unbiased instructions, enables flexible queries across subjects, and reformulates questions for enhanced response clarity. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Experiments reveal ChatGPT's capacity for personality assessment with more consistent and equitable evaluations, despite lower robustness to instruction biases compared with InstructGPT. It also details that Experiments reveal ChatGPT's capacity for personality assessment with more consistent and equitable evaluations, despite lower robustness to instruction biases compared with InstructGPT. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can ChatGPT Assess Human Personalities? A General Evaluation Framework\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta un marco de evaluación genérico que permite a modelos de lenguaje evaluar personalidades humanas mediante MBTI. Además, contextualiza el aporte al precisar que El marco diseña instrucciones imparciales, habilita consultas flexibles entre sujetos y reformula preguntas para mayor claridad de respuestas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los experimentos revelan la capacidad de ChatGPT para evaluación de personalidad con evaluaciones más coherentes y equitativas, pese a menor robustez ante sesgos de instrucciones en comparación con InstructGPT. También se especifica que Los experimentos revelan la capacidad de ChatGPT para evaluación de personalidad con evaluaciones más coherentes y equitativas, pese a menor robustez ante sesgos de instrucciones en comparación con InstructGPT. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We present a generic evaluation framework enabling LLMs to assess human personalities through MBTI.",
        "The framework devises unbiased instructions, enables flexible queries across subjects, and reformulates questions for enhanced response clarity.",
        "Experiments reveal ChatGPT's capacity for personality assessment with more consistent and equitable evaluations, despite lower robustness to instruction biases compared with InstructGPT."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-027",
    "legacy_article_number": 27,
    "title_original": "Machine Mindset: An MBTI Exploration of Large Language Models",
    "title_canonical": "Machine Mindset: An MBTI Exploration of Large Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Jiaxi Cui",
      "Liuzhenghao Lv",
      "Jing Wen",
      "Rongsheng Wang",
      "Jing Tang",
      "YongHong Tian",
      "Li Yuan"
    ],
    "keywords": [
      "Computation and Language",
      "Large Language Models",
      "MBTI Personality Traits",
      "Artificial Intelligence",
      "Personalized AI"
    ],
    "source_url": "https://arxiv.org/abs/2312.12999",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.067Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.067Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2312.12999",
      "fetched_title": "Machine Mindset: An MBTI Exploration of Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We present a novel methodology for integrating MBTI personality traits into LLMs, addressing personality consistency challenges. Machine Mindset employs two-phase fine-tuning and Direct Preference Optimization to embed MBTI traits. The approach ensures models internalize these traits, yielding stable and consistent personality profiles. We demonstrate effectiveness across multiple domains and release the model as open source.",
    "abstract_en_extended": "This study, \"Machine Mindset: An MBTI Exploration of Large Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We present a novel methodology for integrating MBTI personality traits into LLMs, addressing personality consistency challenges. It then advances the context by clarifying that Machine Mindset employs two-phase fine-tuning and Direct Preference Optimization to embed MBTI traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The approach ensures models internalize these traits, yielding stable and consistent personality profiles. It also details that We demonstrate effectiveness across multiple domains and release the model as open source. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Machine Mindset: An MBTI Exploration of Large Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta una metodología novedosa para integrar rasgos de personalidad MBTI en modelos de lenguaje, abordando desafíos de coherencia de personalidad. Además, contextualiza el aporte al precisar que Machine Mindset emplea ajuste fino bifásico y Optimización Directa de Preferencias para incrustar rasgos MBTI. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El enfoque asegura que los modelos internalicen estos rasgos, produciendo perfiles de personalidad estables y coherentes. También se especifica que Se demuestra efectividad a través de múltiples dominios y se libera el modelo como código abierto. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We present a novel methodology for integrating MBTI personality traits into LLMs, addressing personality consistency challenges.",
        "Machine Mindset employs two-phase fine-tuning and Direct Preference Optimization to embed MBTI traits.",
        "The approach ensures models internalize these traits, yielding stable and consistent personality profiles."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-028",
    "legacy_article_number": 28,
    "title_original": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
    "title_canonical": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Xiaoyang Song",
      "Yuta Adachi",
      "Jessie Feng",
      "Mouwei Lin",
      "Linhao Yu",
      "Frank Li",
      "Akshat Gupta",
      "Gopala Anumanchipalli",
      "Simerjot Kaur"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Assessment",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2402.14805",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.083Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.083Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.14805",
      "fetched_title": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We investigate LLM personalities through external evaluation methodology. Rather than prompting with multiple-choice questions, personalities are assessed by analyzing open-ended situational responses via external ML models. Results demonstrate LLMs exhibit divergent personalities when generating posts versus comments, while humans maintain consistent profiles, evidencing fundamental differences in personality manifestation between LLMs and humans.",
    "abstract_en_extended": "This study, \"Identifying Multiple Personalities in Large Language Models with External Evaluation\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We investigate LLM personalities through external evaluation methodology. It then advances the context by clarifying that Rather than prompting with multiple-choice questions, personalities are assessed by analyzing open-ended situational responses via external ML models. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results demonstrate LLMs exhibit divergent personalities when generating posts versus comments, while humans maintain consistent profiles, evidencing fundamental differences in personality manifestation between LLMs and humans. It also details that Results demonstrate LLMs exhibit divergent personalities when generating posts versus comments, while humans maintain consistent profiles, evidencing fundamental differences in personality manifestation between LLMs and humans. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Identifying Multiple Personalities in Large Language Models with External Evaluation\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investigan personalidades en modelos de lenguaje mediante metodología de evaluación externa. Además, contextualiza el aporte al precisar que En lugar de emplear preguntas de opción múltiple, las personalidades se evalúan analizando respuestas situacionales abiertas mediante modelos de aprendizaje automático externos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados demuestran que los modelos exhiben personalidades divergentes al generar publicaciones versus comentarios, mientras que los humanos mantienen perfiles coherentes, evidenciando diferencias fundamentales en manifestación de personalidad entre modelos y humanos. También se especifica que Los resultados demuestran que los modelos exhiben personalidades divergentes al generar publicaciones versus comentarios, mientras que los humanos mantienen perfiles coherentes, evidenciando diferencias fundamentales en manifestación de personalidad entre modelos y humanos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We investigate LLM personalities through external evaluation methodology.",
        "Rather than prompting with multiple-choice questions, personalities are assessed by analyzing open-ended situational responses via external ML models.",
        "Results demonstrate LLMs exhibit divergent personalities when generating posts versus comments, while humans maintain consistent profiles, evidencing fundamental differences in personality manifestation between LLMs and humans."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-029",
    "legacy_article_number": 29,
    "title_original": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits",
    "title_canonical": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bohan Li",
      "Jiannan Guan",
      "Longxu Dou",
      "Yunlong Feng",
      "Dingzirui Wang",
      "Yang Xu",
      "Enbo Wang",
      "Qiguang Chen",
      "Bichen Wang",
      "Xiao Xu",
      "Yimeng Zhang",
      "Libo Qin",
      "Yanyan Zhao",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "keywords": [
      "Computation and Language",
      "Computers and Society",
      "MBTI Personality Detection",
      "Large Language Models",
      "Personality Traits"
    ],
    "source_url": "https://arxiv.org/abs/2412.12510",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.106Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.106Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2412.12510",
      "fetched_title": "Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We optimize MBTI personality detection by constructing MBTIBench, the first manually annotated high-quality dataset with soft labels. The dataset effectively resolves incorrect labeling issues affecting 29.58% of data and estimates soft labels through polarity tendency derivation. Experimental results identify polarized predictions and LLM biases as critical directions for future investigation.",
    "abstract_en_extended": "This study, \"Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We optimize MBTI personality detection by constructing MBTIBench, the first manually annotated high-quality dataset with soft labels. It then advances the context by clarifying that The dataset effectively resolves incorrect labeling issues affecting 29.58% of data and estimates soft labels through polarity tendency derivation. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Experimental results identify polarized predictions and LLM biases as critical directions for future investigation. It also details that Experimental results identify polarized predictions and LLM biases as critical directions for future investigation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se optimiza la detección de personalidad MBTI construyendo MBTIBench, el primer conjunto de datos de alta calidad anotado manualmente con etiquetas suaves. Además, contextualiza el aporte al precisar que El conjunto de datos resuelve efectivamente problemas de etiquetado incorrecto que afectan al 29.58% de datos y estima etiquetas suaves mediante derivación de tendencia de polaridad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados experimentales identifican predicciones polarizadas y sesgos en modelos como direcciones críticas para investigación futura. También se especifica que Los resultados experimentales identifican predicciones polarizadas y sesgos en modelos como direcciones críticas para investigación futura. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We optimize MBTI personality detection by constructing MBTIBench, the first manually annotated high-quality dataset with soft labels.",
        "The dataset effectively resolves incorrect labeling issues affecting 29.58% of data and estimates soft labels through polarity tendency derivation.",
        "Experimental results identify polarized predictions and LLM biases as critical directions for future investigation."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-030",
    "legacy_article_number": 30,
    "title_original": "Evaluating the Psychological Safety of Large Language Models",
    "title_canonical": "Evaluating Psychological Safety of Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Xingxuan Li",
      "Yutong Li",
      "Lin Qiu",
      "Shafiq Joty",
      "Lidong Bing"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society",
      "Psychological safety",
      "Personality tests",
      "Well-being assessments"
    ],
    "source_url": "https://arxiv.org/abs/2212.10529",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.122Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.122Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2212.10529",
      "fetched_title": "Evaluating Psychological Safety of Large Language Models",
      "title_similarity": 0.875,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We design unbiased instructions to systematically evaluate psychological safety in LLMs. Five LLMs underwent testing via Short Dark Triad and Big Five Inventory. All models score above human averages on SD-3, suggesting relatively darker personality patterns. We recommend systematic psychological metric application to further evaluate and enhance LLM safety.",
    "abstract_en_extended": "This study, \"Evaluating Psychological Safety of Large Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We design unbiased instructions to systematically evaluate psychological safety in LLMs. It then advances the context by clarifying that Five LLMs underwent testing via Short Dark Triad and Big Five Inventory. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that All models score above human averages on SD-3, suggesting relatively darker personality patterns. It also details that We recommend systematic psychological metric application to further evaluate and enhance LLM safety. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating Psychological Safety of Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se diseñan instrucciones imparciales para evaluar sistemáticamente seguridad psicológica en modelos de lenguaje. Además, contextualiza el aporte al precisar que Cinco modelos fueron evaluados mediante Short Dark Triad e Inventario Big Five. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Todos los modelos puntúan por encima de promedios humanos en SD-3, sugiriendo patrones de personalidad relativamente más oscuros. También se especifica que Se recomienda aplicación sistemática de métricas psicológicas para evaluar y mejorar la seguridad de los modelos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We design unbiased instructions to systematically evaluate psychological safety in LLMs.",
        "Five LLMs underwent testing via Short Dark Triad and Big Five Inventory.",
        "All models score above human averages on SD-3, suggesting relatively darker personality patterns."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-031",
    "legacy_article_number": 31,
    "title_original": "Personality Testing of Large Language Models: Limited Temporal Stability But Highlighted Social Desirability",
    "title_canonical": "Personality testing of large language models: limited temporal stability, but highlighted prosociality",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bojana Bodroža",
      "Bojana M. Dinić",
      "Ljubiša Bojić"
    ],
    "keywords": [
      "Large Language Models",
      "Personality testing",
      "Temporal stability",
      "Prosociality",
      "Psychometric assessment"
    ],
    "source_url": "https://doi.org/10.1098/rsos.240180",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.139Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.139Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1098/rsos.240180",
      "fetched_title": "Personality testing of large language models: limited temporal stability, but highlighted prosociality",
      "title_similarity": 0.7857,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality testing across seven LLMs was investigated with focus on temporal stability. Models demonstrated varying inter-rater agreement levels across short timeframes. Models including Llama3 and GPT-4o exhibited higher consistency. Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism. Temporal stability proves crucial for AI systems given their expanding societal influence.",
    "abstract_en_extended": "This study, \"Personality testing of large language models: limited temporal stability, but highlighted prosociality\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality testing across seven LLMs was investigated with focus on temporal stability. It then advances the context by clarifying that Models demonstrated varying inter-rater agreement levels across short timeframes. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Models including Llama3 and GPT-4o exhibited higher consistency. It also details that Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality testing of large language models: limited temporal stability, but highlighted prosociality\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investigó la evaluación de personalidad a través de siete modelos de lenguaje con enfoque en estabilidad temporal. Además, contextualiza el aporte al precisar que Los modelos demostraron niveles variables de acuerdo interevaluador a través de períodos breves. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Modelos como Llama3 y GPT-4o exhibieron mayor coherencia. También se especifica que Los modelos mostraron perfiles socialmente deseables caracterizados por amabilidad y responsabilidad elevadas junto con maquiavelismo reducido. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality testing across seven LLMs was investigated with focus on temporal stability.",
        "Models demonstrated varying inter-rater agreement levels across short timeframes.",
        "Models including Llama3 and GPT-4o exhibited higher consistency."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-033",
    "legacy_article_number": 33,
    "title_original": "Personality Testing of Large Language Models: Limited Temporal Stability But Highlighted Social Desirability",
    "title_canonical": "Personality testing of large language models: limited temporal stability, but highlighted prosociality - PMC",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bojana Bodroža",
      "Bojana M. Dinić",
      "Ljubiša Bojić"
    ],
    "keywords": [
      "Large Language Models",
      "Personality testing",
      "Temporal stability",
      "Prosociality"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11461045/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.390Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.390Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11461045/",
      "fetched_title": "Personality testing of large language models: limited temporal stability, but highlighted prosociality - PMC",
      "title_similarity": 0.7333,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Personality testing across seven LLMs was investigated with focus on temporal stability. Models demonstrated varying inter-rater agreement levels across short timeframes. Models including Llama3 and GPT-4o exhibited higher consistency. Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism.",
    "abstract_en_extended": "This study, \"Personality testing of large language models: limited temporal stability, but highlighted prosociality - PMC\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality testing across seven LLMs was investigated with focus on temporal stability. It then advances the context by clarifying that Models demonstrated varying inter-rater agreement levels across short timeframes. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Models including Llama3 and GPT-4o exhibited higher consistency. It also details that Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality testing of large language models: limited temporal stability, but highlighted prosociality - PMC\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investigó la evaluación de personalidad a través de siete modelos de lenguaje con enfoque en estabilidad temporal. Además, contextualiza el aporte al precisar que Los modelos demostraron niveles variables de acuerdo interevaluador a través de períodos breves. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Modelos como Llama3 y GPT-4o exhibieron mayor coherencia. También se especifica que Los modelos mostraron perfiles socialmente deseables caracterizados por amabilidad y responsabilidad elevadas junto con maquiavelismo reducido. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality testing across seven LLMs was investigated with focus on temporal stability.",
        "Models demonstrated varying inter-rater agreement levels across short timeframes.",
        "Models including Llama3 and GPT-4o exhibited higher consistency."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-034",
    "legacy_article_number": 34,
    "title_original": "Applying Psychometrics to Simulated Populations of Large Language Models: Recreating the HEXACO Personality Inventory Experiment",
    "title_canonical": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Sarah Mercer",
      "Daniel P. Martin",
      "Phil Swatton"
    ],
    "keywords": [
      "Computation and Language",
      "Machine Learning",
      "Generative Agents",
      "Personality Inventory",
      "Psychometrics",
      "HEXACO"
    ],
    "source_url": "https://arxiv.org/abs/2508.00742",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.605Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.605Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.00742",
      "fetched_title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
      "title_similarity": 0.6842,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We explore validity of persona-based agents for representing human populations by recreating the HEXACO personality inventory experiment. Results reveal coherent personality structures recoverable from agent responses, demonstrating partial alignment with the HEXACO framework. Derived personality dimensions prove consistent and reliable within GPT-4 when paired with curated populations. Cross-model analysis reveals variability suggesting model-specific biases and limitations.",
    "abstract_en_extended": "This study, \"Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We explore validity of persona-based agents for representing human populations by recreating the HEXACO personality inventory experiment. It then advances the context by clarifying that Results reveal coherent personality structures recoverable from agent responses, demonstrating partial alignment with the HEXACO framework. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Derived personality dimensions prove consistent and reliable within GPT-4 when paired with curated populations. It also details that Cross-model analysis reveals variability suggesting model-specific biases and limitations. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora la validez de agentes basados en personas para representar poblaciones humanas recreando el experimento del inventario de personalidad HEXACO. Además, contextualiza el aporte al precisar que Los resultados revelan estructuras de personalidad coherentes recuperables desde respuestas de agentes, demostrando alineamiento parcial con el marco HEXACO. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las dimensiones de personalidad derivadas resultan coherentes y fiables dentro de GPT-4 cuando se emparejan con poblaciones curadas. También se especifica que El análisis entre modelos revela variabilidad que sugiere sesgos y limitaciones específicos del modelo. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We explore validity of persona-based agents for representing human populations by recreating the HEXACO personality inventory experiment.",
        "Results reveal coherent personality structures recoverable from agent responses, demonstrating partial alignment with the HEXACO framework.",
        "Derived personality dimensions prove consistent and reliable within GPT-4 when paired with curated populations."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-035",
    "legacy_article_number": 35,
    "title_original": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "title_canonical": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Traits",
      "Bias",
      "Toxicity"
    ],
    "source_url": "https://arxiv.org/abs/2502.12566",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.609Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.609Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.12566",
      "fetched_title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality trait assignment influences toxicity and bias levels in large language model outputs. Using the HEXACO personality framework, experimentally validated prompts were designed to evaluate three models across toxicity and bias benchmarks. All models exhibited sensitivity to HEXACO traits with consistent variations in bias, negative sentiment, and toxicity levels. Adjustment of personality trait intensities effectively reduces bias and toxicity in model outputs.",
    "abstract_en_extended": "This study, \"Exploring the Impact of Personality Traits on LLM Bias and Toxicity\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality trait assignment influences toxicity and bias levels in large language model outputs. It then advances the context by clarifying that Using the HEXACO personality framework, experimentally validated prompts were designed to evaluate three models across toxicity and bias benchmarks. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that All models exhibited sensitivity to HEXACO traits with consistent variations in bias, negative sentiment, and toxicity levels. It also details that Adjustment of personality trait intensities effectively reduces bias and toxicity in model outputs. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Exploring the Impact of Personality Traits on LLM Bias and Toxicity\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La asignación de rasgos de personalidad influye en los niveles de toxicidad y sesgo de los modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Mediante el marco de personalidad HEXACO, se diseñaron instrucciones experimentalmente validadas para evaluar tres modelos a través de benchmarks de toxicidad y sesgo. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Todos los modelos exhibieron sensibilidad a los rasgos HEXACO con variaciones consistentes en sesgo, sentimiento negativo y niveles de toxicidad. También se especifica que El ajuste de las intensidades de rasgos de personalidad reduce efectivamente el sesgo y la toxicidad en las salidas del modelo. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality trait assignment influences toxicity and bias levels in large language model outputs.",
        "Using the HEXACO personality framework, experimentally validated prompts were designed to evaluate three models across toxicity and bias benchmarks.",
        "All models exhibited sensitivity to HEXACO traits with consistent variations in bias, negative sentiment, and toxicity levels."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-036",
    "legacy_article_number": 36,
    "title_original": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control",
    "title_canonical": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Adithya Chittem",
      "Aishna Shrivastava",
      "Sai Tarun Pendela",
      "Jagat Sesh Challa",
      "Dhruv Kumar"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Human-Computer Interaction"
    ],
    "source_url": "https://arxiv.org/abs/2506.20993",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.620Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.620Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2506.20993",
      "fetched_title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality modeling is extended from the Big Five to the 16PF model, enabling expressive control over sixteen distinct traits. The Specific Attribute Control (SAC) framework evaluates and dynamically induces trait intensity in large language models using adjective-based semantic anchoring. Modeling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression. Changes in target trait intensity systematically influence closely related traits in psychologically coherent directions.",
    "abstract_en_extended": "This study, \"SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality modeling is extended from the Big Five to the 16PF model, enabling expressive control over sixteen distinct traits. It then advances the context by clarifying that The Specific Attribute Control (SAC) framework evaluates and dynamically induces trait intensity in large language models using adjective-based semantic anchoring. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Modeling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression. It also details that Changes in target trait intensity systematically influence closely related traits in psychologically coherent directions. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se extiende el modelado de personalidad desde los Cinco Grandes al modelo 16PF, permitiendo control expresivo sobre dieciséis rasgos distintos. Además, contextualiza el aporte al precisar que El marco de Control de Atributos Específicos (SAC) evalúa e induce dinámicamente la intensidad de rasgos en modelos de lenguaje de gran escala mediante anclaje semántico basado en adjetivos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El modelado de la intensidad como espectro continuo produce expresión de personalidad sustancialmente más consistente y controlable. También se especifica que Los cambios en la intensidad del rasgo objetivo influyen sistemáticamente en rasgos estrechamente relacionados en direcciones psicológicamente coherentes. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality modeling is extended from the Big Five to the 16PF model, enabling expressive control over sixteen distinct traits.",
        "The Specific Attribute Control (SAC) framework evaluates and dynamically induces trait intensity in large language models using adjective-based semantic anchoring.",
        "Modeling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-037",
    "legacy_article_number": 37,
    "title_original": "Moral Foundations of Large Language Models",
    "title_canonical": "Moral Foundations of Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Marwa Abdulhai",
      "Gregory Serapio-Garcia",
      "Clément Crepy",
      "Daria Valter",
      "John Canny",
      "Natasha Jaques"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Moral Foundations Theory"
    ],
    "source_url": "https://arxiv.org/abs/2310.15337",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.625Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.625Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2310.15337",
      "fetched_title": "Moral Foundations of Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Moral Foundations Theory is applied to analyze whether popular large language models have acquired bias toward particular moral values. Models exhibit specific moral foundations correlating with human moral foundations and political affiliations. Consistency of these biases is measured, revealing instructions can be adversarially selected to induce models to exhibit particular moral foundations, affecting downstream task behavior.",
    "abstract_en_extended": "This study, \"Moral Foundations of Large Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Moral Foundations Theory is applied to analyze whether popular large language models have acquired bias toward particular moral values. It then advances the context by clarifying that Models exhibit specific moral foundations correlating with human moral foundations and political affiliations. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Consistency of these biases is measured, revealing instructions can be adversarially selected to induce models to exhibit particular moral foundations, affecting downstream task behavior. It also details that Consistency of these biases is measured, revealing instructions can be adversarially selected to induce models to exhibit particular moral foundations, affecting downstream task behavior. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Moral Foundations of Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se aplica la Teoría de Fundamentos Morales para analizar si los modelos de lenguaje de gran escala populares han adquirido sesgo hacia valores morales particulares. Además, contextualiza el aporte al precisar que Los modelos exhiben fundamentos morales específicos que correlacionan con fundamentos morales humanos y afiliaciones políticas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se mide la coherencia de estos sesgos, revelando que las instrucciones pueden seleccionarse adversarialmente para inducir a los modelos a exhibir fundamentos morales particulares, afectando el comportamiento en tareas posteriores. También se especifica que Se mide la coherencia de estos sesgos, revelando que las instrucciones pueden seleccionarse adversarialmente para inducir a los modelos a exhibir fundamentos morales particulares, afectando el comportamiento en tareas posteriores. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Moral Foundations Theory is applied to analyze whether popular large language models have acquired bias toward particular moral values.",
        "Models exhibit specific moral foundations correlating with human moral foundations and political affiliations.",
        "Consistency of these biases is measured, revealing instructions can be adversarially selected to induce models to exhibit particular moral foundations, affecting downstream task behavior."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-038",
    "legacy_article_number": 38,
    "title_original": "Questioning the Validity of Personality Tests for Large Language Models",
    "title_canonical": "Challenging the Validity of Personality Tests for Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Tom Sühr",
      "Florian E. Dorner",
      "Samira Samadi",
      "Augustin Kelava"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Machine Learning",
      "Personality assessment"
    ],
    "source_url": "https://arxiv.org/abs/2311.05297",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.636Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.636Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2311.05297",
      "fetched_title": "Challenging the Validity of Personality Tests for Large Language Models",
      "title_similarity": 0.8182,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language model responses to personality tests systematically deviate from human responses, implying test results cannot be interpreted equivalently. Reverse-coded items are often both answered affirmatively. Variation across instructions designed to simulate particular personality types does not follow clear separation into five independent personality factors observed in human samples. Results highlight the importance of investigating test validity when applied to large language models.",
    "abstract_en_extended": "This study, \"Challenging the Validity of Personality Tests for Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language model responses to personality tests systematically deviate from human responses, implying test results cannot be interpreted equivalently. It then advances the context by clarifying that Reverse-coded items are often both answered affirmatively. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Variation across instructions designed to simulate particular personality types does not follow clear separation into five independent personality factors observed in human samples. It also details that Results highlight the importance of investigating test validity when applied to large language models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Challenging the Validity of Personality Tests for Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Las respuestas de modelos de lenguaje de gran escala a pruebas de personalidad se desvían sistemáticamente de las respuestas humanas, implicando que los resultados no pueden interpretarse de manera equivalente. Además, contextualiza el aporte al precisar que Los ítems codificados inversamente frecuentemente son respondidos afirmativamente en ambos casos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La variación entre instrucciones diseñadas para simular tipos particulares de personalidad no sigue la separación clara en cinco factores de personalidad independientes observada en muestras humanas. También se especifica que Los resultados destacan la importancia de investigar la validez de las pruebas cuando se aplican a modelos de lenguaje de gran escala. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language model responses to personality tests systematically deviate from human responses, implying test results cannot be interpreted equivalently.",
        "Reverse-coded items are often both answered affirmatively.",
        "Variation across instructions designed to simulate particular personality types does not follow clear separation into five independent personality factors observed in human samples."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-039",
    "legacy_article_number": 39,
    "title_original": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
    "title_canonical": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yu-Min Tseng",
      "Yu-Chao Huang",
      "Teng-Yun Hsiao",
      "Wei-Lin Chen",
      "Chao-Wei Huang",
      "Yu Meng",
      "Yun-Nung Chen"
    ],
    "keywords": [
      "Large Language Models",
      "Persona",
      "Role-Playing",
      "Personalization",
      "LLM Personality Evaluation"
    ],
    "source_url": "https://aclanthology.org/2024.findings-emnlp.969/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.641Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.641Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.969/",
      "fetched_title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Research on leveraging persona in large language models is categorized into two lines: role-playing, where personas are assigned to models, and personalization, where models accommodate user personas. Existing methods for personality evaluation in large language models are also introduced. This represents the first survey addressing role-playing and personalization under the unified view of persona.",
    "abstract_en_extended": "This study, \"Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Research on leveraging persona in large language models is categorized into two lines: role-playing, where personas are assigned to models, and personalization, where models accommodate user personas. It then advances the context by clarifying that Existing methods for personality evaluation in large language models are also introduced. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This represents the first survey addressing role-playing and personalization under the unified view of persona. It also details that This represents the first survey addressing role-playing and personalization under the unified view of persona. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se categoriza la investigación sobre el uso de personas en modelos de lenguaje de gran escala en dos líneas: juego de roles, donde se asignan personas a los modelos, y personalización, donde los modelos se adaptan a personas de usuarios. Además, contextualiza el aporte al precisar que También se introducen métodos existentes para la evaluación de personalidad en modelos de lenguaje de gran escala. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Representa la primera revisión que aborda juego de roles y personalización bajo la visión unificada de persona. También se especifica que Representa la primera revisión que aborda juego de roles y personalización bajo la visión unificada de persona. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Research on leveraging persona in large language models is categorized into two lines: role-playing, where personas are assigned to models, and personalization, where models accommodate user personas.",
        "Existing methods for personality evaluation in large language models are also introduced.",
        "This represents the first survey addressing role-playing and personalization under the unified view of persona."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-041",
    "legacy_article_number": 41,
    "title_original": "Psychometrics of Large Language Models: A Systematic Review of Evaluation, Validation, and Enhancement",
    "title_canonical": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Haoran Ye",
      "Jing Jin",
      "Yuhang Xie",
      "Xin Zhang",
      "Guojie Song"
    ],
    "keywords": [
      "LLM Psychometrics",
      "Systematic Review",
      "Evaluation",
      "Validation"
    ],
    "source_url": "https://llm-psychometrics.com/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.651Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.651Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://llm-psychometrics.com/",
      "fetched_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
      "title_similarity": 0.8333,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "The interdisciplinary field of psychometrics for large language models leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance these systems. The literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances model capabilities. A curated repository of resources is available for consultation.",
    "abstract_en_extended": "This study, \"Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The interdisciplinary field of psychometrics for large language models leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance these systems. It then advances the context by clarifying that The literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances model capabilities. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that A curated repository of resources is available for consultation. It also details that A curated repository of resources is available for consultation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que El campo interdisciplinario de psicometría para modelos de lenguaje de gran escala aprovecha instrumentos, teorías y principios psicométricos para evaluar, comprender y mejorar estos sistemas. Además, contextualiza el aporte al precisar que La literatura da forma sistemáticamente a principios de benchmarking, amplía alcances de evaluación, refina metodologías, valida resultados y avanza las capacidades de los modelos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se encuentra disponible un repositorio curado de recursos para consulta. También se especifica que Se encuentra disponible un repositorio curado de recursos para consulta. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The interdisciplinary field of psychometrics for large language models leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance these systems.",
        "The literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances model capabilities.",
        "A curated repository of resources is available for consultation."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-042",
    "legacy_article_number": 42,
    "title_original": "Quantifying AI Psychology: A Psychometric Benchmark for Large Language Models",
    "title_canonical": "Evaluating Large Language Models with Psychometrics",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yuan Li",
      "Yue Huang",
      "Hongyi Wang",
      "Ying Cheng",
      "Xiangliang Zhang",
      "James Zou",
      "Lichao Sun"
    ],
    "keywords": [
      "Computation and Language",
      "Psychological constructs",
      "Personality",
      "Values",
      "Emotional intelligence",
      "Theory of mind",
      "Self-efficacy"
    ],
    "source_url": "https://arxiv.org/abs/2406.17675",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.926Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.926Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.17675",
      "fetched_title": "Evaluating Large Language Models with Psychometrics",
      "title_similarity": 0.25,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "A comprehensive benchmark is presented for quantifying psychological constructs in large language models, encompassing psychological dimension identification, assessment dataset design, and validation of results. Five key psychological constructs are assessed through 13 datasets. Significant discrepancies between self-reported traits and response patterns in real-world scenarios reveal behavioral complexities. Some preference-based tests designed for humans fail to elicit reliable responses from large language models.",
    "abstract_en_extended": "This study, \"Evaluating Large Language Models with Psychometrics\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A comprehensive benchmark is presented for quantifying psychological constructs in large language models, encompassing psychological dimension identification, assessment dataset design, and validation of results. It then advances the context by clarifying that Five key psychological constructs are assessed through 13 datasets. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Significant discrepancies between self-reported traits and response patterns in real-world scenarios reveal behavioral complexities. It also details that Some preference-based tests designed for humans fail to elicit reliable responses from large language models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating Large Language Models with Psychometrics\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta un benchmark integral para cuantificar constructos psicológicos en modelos de lenguaje de gran escala, abarcando identificación de dimensiones psicológicas, diseño de conjuntos de datos de evaluación y validación de resultados. Además, contextualiza el aporte al precisar que Cinco constructos psicológicos clave se evalúan a través de 13 conjuntos de datos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Discrepancias significativas entre rasgos autoinformados y patrones de respuesta en escenarios del mundo real revelan complejidades conductuales. También se especifica que Algunas pruebas basadas en preferencias diseñadas para humanos no logran obtener respuestas fiables de los modelos de lenguaje de gran escala. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A comprehensive benchmark is presented for quantifying psychological constructs in large language models, encompassing psychological dimension identification, assessment dataset design, and validation of results.",
        "Five key psychological constructs are assessed through 13 datasets.",
        "Significant discrepancies between self-reported traits and response patterns in real-world scenarios reveal behavioral complexities."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-043",
    "legacy_article_number": 43,
    "title_original": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
    "title_canonical": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Personality Assessment",
      "Multi-Observer Framework"
    ],
    "source_url": "https://arxiv.org/abs/2504.08399",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.944Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.944Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2504.08399",
      "fetched_title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "A novel multi-observer framework for personality trait assessment in large language model agents is proposed, drawing on informant-report methods from psychology. Instead of self-assessments, multiple observer agents are employed, each configured with specific relational contexts. Observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in self-assessments. Aggregating responses from 5-7 observers reduces biases and achieves optimal reliability.",
    "abstract_en_extended": "This study, \"Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel multi-observer framework for personality trait assessment in large language model agents is proposed, drawing on informant-report methods from psychology. It then advances the context by clarifying that Instead of self-assessments, multiple observer agents are employed, each configured with specific relational contexts. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in self-assessments. It also details that Aggregating responses from 5-7 observers reduces biases and achieves optimal reliability. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un marco novedoso de multi-observador para evaluación de rasgos de personalidad en agentes de modelos de lenguaje de gran escala, basándose en métodos de informe de informantes de la psicología. Además, contextualiza el aporte al precisar que En lugar de autoevaluaciones, se emplean múltiples agentes observadores, cada uno configurado con contextos relacionales específicos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las calificaciones de informe de observadores se alinean más estrechamente con juicios humanos que los autoinformes tradicionales y revelan sesgos sistemáticos en las autoevaluaciones. También se especifica que La agregación de respuestas de 5-7 observadores reduce sesgos y alcanza fiabilidad óptima. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel multi-observer framework for personality trait assessment in large language model agents is proposed, drawing on informant-report methods from psychology.",
        "Instead of self-assessments, multiple observer agents are employed, each configured with specific relational contexts.",
        "Observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in self-assessments."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-044",
    "legacy_article_number": 44,
    "title_original": "Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction",
    "title_canonical": "Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Julina Maharjan",
      "Ruoming Jin",
      "Jianfeng Zhu",
      "Deric Kenne"
    ],
    "keywords": [
      "Large Language Models",
      "Embeddings",
      "Personality prediction",
      "Psychometric validation",
      "Big Five",
      "LIWC",
      "Emotional markers"
    ],
    "source_url": "https://www.jmir.org/2025/1/e75347",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.959Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.959Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.jmir.org/2025/1/e75347",
      "fetched_title": "Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Embeddings from large language models are evaluated for personality trait prediction through psychometric validation. The research examines how well embeddings capture personality-relevant information compared to traditional linguistic features. Results provide insights into reliability and validity of using embeddings for psychological assessment, with implications for clinical and research applications in personality psychology.",
    "abstract_en_extended": "This study, \"Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Embeddings from large language models are evaluated for personality trait prediction through psychometric validation. It then advances the context by clarifying that The research examines how well embeddings capture personality-relevant information compared to traditional linguistic features. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results provide insights into reliability and validity of using embeddings for psychological assessment, with implications for clinical and research applications in personality psychology. It also details that Results provide insights into reliability and validity of using embeddings for psychological assessment, with implications for clinical and research applications in personality psychology. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se evalúan embeddings de modelos de lenguaje de gran escala para predicción de rasgos de personalidad mediante validación psicométrica. Además, contextualiza el aporte al precisar que La investigación examina en qué medida los embeddings capturan información relevante de personalidad en comparación con características lingüísticas tradicionales. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados proporcionan perspectivas sobre la fiabilidad y validez del uso de embeddings para evaluación psicológica, con implicaciones para aplicaciones clínicas y de investigación en psicología de la personalidad. También se especifica que Los resultados proporcionan perspectivas sobre la fiabilidad y validez del uso de embeddings para evaluación psicológica, con implicaciones para aplicaciones clínicas y de investigación en psicología de la personalidad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Embeddings from large language models are evaluated for personality trait prediction through psychometric validation.",
        "The research examines how well embeddings capture personality-relevant information compared to traditional linguistic features.",
        "Results provide insights into reliability and validity of using embeddings for psychological assessment, with implications for clinical and research applications in personality psychology."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-046",
    "legacy_article_number": 46,
    "title_original": "LMLPA: Language Model Linguistic Personality Assessment",
    "title_canonical": "LMLPA: Language Model Linguistic Personality Assessment",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Jingyao Zheng",
      "Xian Wang",
      "Simo Hosio",
      "Xiaoxian Xu",
      "Lik-Hang Lee"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Personality Assessment",
      "Computational Linguistics"
    ],
    "source_url": "https://doi.org/10.1162/coli_a_00550",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:14.991Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:14.991Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1162/coli_a_00550",
      "fetched_title": "LMLPA: Language Model Linguistic Personality Assessment",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The Language Model Linguistic Personality Assessment (LMLPA) is introduced as a system designed to evaluate linguistic personalities of large language models. Unlike traditional psychometrics, LMLPA adapts the Big Five Inventory to align with operational capabilities of these models. The questionnaire is open-ended, requiring an artificial intelligence rater to transform textual responses into numerical personality indicators. Large language models possess distinct personality traits that can be effectively quantified.",
    "abstract_en_extended": "This study, \"LMLPA: Language Model Linguistic Personality Assessment\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The Language Model Linguistic Personality Assessment (LMLPA) is introduced as a system designed to evaluate linguistic personalities of large language models. It then advances the context by clarifying that Unlike traditional psychometrics, LMLPA adapts the Big Five Inventory to align with operational capabilities of these models. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The questionnaire is open-ended, requiring an artificial intelligence rater to transform textual responses into numerical personality indicators. It also details that Large language models possess distinct personality traits that can be effectively quantified. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"LMLPA: Language Model Linguistic Personality Assessment\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce la Evaluación de Personalidad Lingüística de Modelos de Lenguaje (LMLPA) como sistema diseñado para evaluar personalidades lingüísticas de modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que A diferencia de la psicometría tradicional, LMLPA adapta el Inventario de los Cinco Grandes para alinearse con las capacidades operacionales de estos modelos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El cuestionario es de respuesta abierta, requiriendo un evaluador de inteligencia artificial para transformar respuestas textuales en indicadores numéricos de personalidad. También se especifica que Los modelos de lenguaje de gran escala poseen rasgos de personalidad distintos que pueden cuantificarse efectivamente. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The Language Model Linguistic Personality Assessment (LMLPA) is introduced as a system designed to evaluate linguistic personalities of large language models.",
        "Unlike traditional psychometrics, LMLPA adapts the Big Five Inventory to align with operational capabilities of these models.",
        "The questionnaire is open-ended, requiring an artificial intelligence rater to transform textual responses into numerical personality indicators."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-047",
    "legacy_article_number": 47,
    "title_original": "You Don't Need a Personality Test to Know These Models Are Unreliable: Assessing the Reliability of LLMs on Psychometric Instruments",
    "title_canonical": "You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bangzhao Shu",
      "Lechen Zhang",
      "Minje Choi",
      "Lavinia Dunagan",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Dallas Card",
      "David Jurgens"
    ],
    "keywords": [
      "Large Language Models",
      "Psychometric Instruments",
      "Persona Measurement",
      "Prompt Consistency",
      "NLU"
    ],
    "source_url": "https://aclanthology.org/2024.naacl-long.295/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.238Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.238Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.naacl-long.295/",
      "fetched_title": "You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments",
      "title_similarity": 0.8571,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A dataset containing 693 questions encompassing 39 different persona measurement instruments across 115 persona axes is constructed. Experiments on 17 large language models reveal that simple perturbations significantly degrade question-answering ability, and most models exhibit low negation consistency. Results suggest the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, requiring alternative approaches.",
    "abstract_en_extended": "This study, \"You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A dataset containing 693 questions encompassing 39 different persona measurement instruments across 115 persona axes is constructed. It then advances the context by clarifying that Experiments on 17 large language models reveal that simple perturbations significantly degrade question-answering ability, and most models exhibit low negation consistency. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results suggest the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, requiring alternative approaches. It also details that Results suggest the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, requiring alternative approaches. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se construye un conjunto de datos que contiene 693 preguntas abarcando 39 instrumentos diferentes de medición de persona en 115 ejes de persona. Además, contextualiza el aporte al precisar que Experimentos en 17 modelos de lenguaje de gran escala revelan que perturbaciones simples degradan significativamente la capacidad de respuesta a preguntas, y la mayoría de los modelos exhiben baja consistencia de negación. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados sugieren que la práctica actualmente generalizada de instrucciones es insuficiente para capturar con precisión y fiabilidad las percepciones del modelo, requiriendo enfoques alternativos. También se especifica que Los resultados sugieren que la práctica actualmente generalizada de instrucciones es insuficiente para capturar con precisión y fiabilidad las percepciones del modelo, requiriendo enfoques alternativos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A dataset containing 693 questions encompassing 39 different persona measurement instruments across 115 persona axes is constructed.",
        "Experiments on 17 large language models reveal that simple perturbations significantly degrade question-answering ability, and most models exhibit low negation consistency.",
        "Results suggest the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, requiring alternative approaches."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-048",
    "legacy_article_number": 48,
    "title_original": "Self-Reports are Unreliable Measures of LLM Personality",
    "title_canonical": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Akshat Gupta",
      "Xiaoyang Song",
      "Gopala Anumanchipalli"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Assessment",
      "Prompt Sensitivity",
      "Option-Order Symmetry",
      "NLP"
    ],
    "source_url": "https://aclanthology.org/2024.blackboxnlp-1.20/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.586Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.586Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.blackboxnlp-1.20/",
      "fetched_title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
      "title_similarity": 0.7,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Reliability of personality scores from self-assessment tests is analyzed using two experiments: prompt sensitivity and option-order symmetry. Tests on ChatGPT and three Llama2 models show semantically equivalent prompts lead to substantially different personality scores with statistically significant differences for all traits. Scores are not robust to option order. Self-assessment personality tests created for humans constitute unreliable measures of personality in large language models.",
    "abstract_en_extended": "This study, \"Self-Assessment Tests are Unreliable Measures of LLM Personality\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Reliability of personality scores from self-assessment tests is analyzed using two experiments: prompt sensitivity and option-order symmetry. It then advances the context by clarifying that Tests on ChatGPT and three Llama2 models show semantically equivalent prompts lead to substantially different personality scores with statistically significant differences for all traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Scores are not robust to option order. It also details that Self-assessment personality tests created for humans constitute unreliable measures of personality in large language models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Self-Assessment Tests are Unreliable Measures of LLM Personality\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se analiza la fiabilidad de puntuaciones de personalidad de pruebas de autoevaluación mediante dos experimentos: sensibilidad de instrucciones y simetría de orden de opciones. Además, contextualiza el aporte al precisar que Pruebas en ChatGPT y tres modelos Llama2 muestran que instrucciones semánticamente equivalentes conducen a puntuaciones de personalidad sustancialmente diferentes con diferencias estadísticamente significativas para todos los rasgos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las puntuaciones no son robustas al orden de opciones. También se especifica que Las pruebas de personalidad de autoevaluación creadas para humanos constituyen medidas no fiables de personalidad en modelos de lenguaje de gran escala. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Reliability of personality scores from self-assessment tests is analyzed using two experiments: prompt sensitivity and option-order symmetry.",
        "Tests on ChatGPT and three Llama2 models show semantically equivalent prompts lead to substantially different personality scores with statistically significant differences for all traits.",
        "Scores are not robust to option order."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-049",
    "legacy_article_number": 49,
    "title_original": "Is Machine Psychology Here? On the Requirements for Using Human Psychological Tests on LLMs",
    "title_canonical": "Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Lea Löhn",
      "Niklas Kiehne",
      "Alexander Ljapunov",
      "Wolf-Tilo Balke"
    ],
    "keywords": [
      "Large Language Models",
      "Psychological Assessment",
      "Machine Psychology",
      "Test Reliability",
      "Construct Validity"
    ],
    "source_url": "https://aclanthology.org/2024.inlg-main.19/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.721Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.721Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.inlg-main.19/",
      "fetched_title": "Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models",
      "title_similarity": 0.6875,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Seven requirements necessary for testing large language models with psychological assessments are proposed. Critical reflection on 25 machine psychology studies reveals lack of appropriate methods to assess test reliability and construct validity, unknown strength of construct-irrelevant influences such as pre-training corpora contamination, and pervasive non-reproducibility issues. Results underscore lack of general methodology and need to redefine psychological constructs specifically for large language models.",
    "abstract_en_extended": "This study, \"Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Seven requirements necessary for testing large language models with psychological assessments are proposed. It then advances the context by clarifying that Critical reflection on 25 machine psychology studies reveals lack of appropriate methods to assess test reliability and construct validity, unknown strength of construct-irrelevant influences such as pre-training corpora contamination, and pervasive non-reproducibility issues. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results underscore lack of general methodology and need to redefine psychological constructs specifically for large language models. It also details that Results underscore lack of general methodology and need to redefine psychological constructs specifically for large language models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se proponen siete requisitos necesarios para evaluar modelos de lenguaje de gran escala con evaluaciones psicológicas. Además, contextualiza el aporte al precisar que La reflexión crítica sobre 25 estudios de psicología de máquinas revela falta de métodos apropiados para evaluar fiabilidad de pruebas y validez de constructo, fuerza desconocida de influencias irrelevantes al constructo como contaminación de corpus de preentrenamiento, y problemas generalizados de no reproducibilidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados subrayan la falta de metodología general y la necesidad de redefinir constructos psicológicos específicamente para modelos de lenguaje de gran escala. También se especifica que Los resultados subrayan la falta de metodología general y la necesidad de redefinir constructos psicológicos específicamente para modelos de lenguaje de gran escala. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Seven requirements necessary for testing large language models with psychological assessments are proposed.",
        "Critical reflection on 25 machine psychology studies reveals lack of appropriate methods to assess test reliability and construct validity, unknown strength of construct-irrelevant influences such as pre-training corpora contamination, and pervasive non-reproducibility issues.",
        "Results underscore lack of general methodology and need to redefine psychological constructs specifically for large language models."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-050",
    "legacy_article_number": 50,
    "title_original": "Persistent Instability in LLM Personality Measurements: Effects of Scaling, Reasoning, and Conversation History",
    "title_canonical": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Tommaso Tosato",
      "Saskia Helbling",
      "Yorguin-Jose Mantilla-Ramos",
      "Mahmood Hegazy",
      "Alberto Tosato",
      "David John Lemay",
      "Irina Rish",
      "Guillaume Dumas"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Personality Measurements",
      "Model Behavior Consistency"
    ],
    "source_url": "https://arxiv.org/abs/2508.04826",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.932Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.932Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.04826",
      "fetched_title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History",
      "title_similarity": 0.8571,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "PERSIST, a comprehensive evaluation framework, is presented testing 25+ open-source models across 500,000+ responses. Findings challenge fundamental deployment assumptions: even 400B+ parameter models exhibit substantial response variability; minor prompt reordering shifts personality measurements by up to 20%; interventions expected to stabilize behavior can paradoxically increase variability. This persistent instability across scales and mitigation strategies suggests current models lack foundations for genuine behavioral consistency.",
    "abstract_en_extended": "This study, \"Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that PERSIST, a comprehensive evaluation framework, is presented testing 25+ open-source models across 500,000+ responses. It then advances the context by clarifying that Findings challenge fundamental deployment assumptions: even 400B+ parameter models exhibit substantial response variability; minor prompt reordering shifts personality measurements by up to 20%; interventions expected to stabilize behavior can paradoxically increase variability. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This persistent instability across scales and mitigation strategies suggests current models lack foundations for genuine behavioral consistency. It also details that This persistent instability across scales and mitigation strategies suggests current models lack foundations for genuine behavioral consistency. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta PERSIST, un marco de evaluación integral que prueba más de 25 modelos de código abierto a través de más de 500,000 respuestas. Además, contextualiza el aporte al precisar que Los hallazgos desafían supuestos fundamentales de despliegue: incluso modelos con más de 400 mil millones de parámetros exhiben variabilidad de respuesta sustancial; reordenamiento menor de instrucciones cambia mediciones de personalidad hasta un 20%; intervenciones esperadas para estabilizar comportamiento pueden paradójicamente aumentar la variabilidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Esta inestabilidad persistente a través de escalas y estrategias de mitigación sugiere que los modelos actuales carecen de fundamentos para consistencia conductual genuina. También se especifica que Esta inestabilidad persistente a través de escalas y estrategias de mitigación sugiere que los modelos actuales carecen de fundamentos para consistencia conductual genuina. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "PERSIST, a comprehensive evaluation framework, is presented testing 25+ open-source models across 500,000+ responses.",
        "Findings challenge fundamental deployment assumptions: even 400B+ parameter models exhibit substantial response variability; minor prompt reordering shifts personality measurements by up to 20%; interventions expected to stabilize behavior can paradoxically increase variability.",
        "This persistent instability across scales and mitigation strategies suggests current models lack foundations for genuine behavioral consistency."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-051",
    "legacy_article_number": 51,
    "title_original": "Stick to Your Role: Stability of Personal Values Expressed in Large Language Models",
    "title_canonical": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Grgur Kovač",
      "Rémy Portelas",
      "Masataka Sawayama",
      "Peter Ford Dominey",
      "Pierre-Yves Oudeyer"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Machine Learning",
      "Value Stability",
      "Personal Values"
    ],
    "source_url": "https://arxiv.org/abs/2402.14846",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.948Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.948Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.14846",
      "fetched_title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Value stability in large language models is studied as a specific property using psychology methods. Rank-order stability is examined at the population level and ipsative stability at the individual level. Two settings (with and without persona simulation), two simulated populations, and three downstream tasks are considered. Consistent trends show some models exhibit higher value stability than others. When instructed to simulate personas, models exhibit low rank-order stability which diminishes with conversation length.",
    "abstract_en_extended": "This study, \"Stick to your Role! Stability of Personal Values Expressed in Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Value stability in large language models is studied as a specific property using psychology methods. It then advances the context by clarifying that Rank-order stability is examined at the population level and ipsative stability at the individual level. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Two settings (with and without persona simulation), two simulated populations, and three downstream tasks are considered. It also details that Consistent trends show some models exhibit higher value stability than others. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Stick to your Role! Stability of Personal Values Expressed in Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se estudia la estabilidad de valores en modelos de lenguaje de gran escala como propiedad específica mediante métodos de psicología. Además, contextualiza el aporte al precisar que Se examina la estabilidad de orden de rango a nivel poblacional y la estabilidad ipsativa a nivel individual. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se consideran dos configuraciones (con y sin simulación de persona), dos poblaciones simuladas y tres tareas posteriores. También se especifica que Tendencias consistentes muestran que algunos modelos exhiben mayor estabilidad de valores que otros. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Value stability in large language models is studied as a specific property using psychology methods.",
        "Rank-order stability is examined at the population level and ipsative stability at the individual level.",
        "Two settings (with and without persona simulation), two simulated populations, and three downstream tasks are considered."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-052",
    "legacy_article_number": 52,
    "title_original": "Scaling Law in LLM Simulated Personality: A More Detailed and Realistic Persona Profile is All You Need",
    "title_canonical": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yuqi Bai",
      "Tianyu Huang",
      "Kun Sun",
      "Yuting Chen"
    ],
    "keywords": [
      "Computers and Society",
      "Artificial Intelligence",
      "Computation and Language",
      "Social experiments",
      "Persona role-playing"
    ],
    "source_url": "https://arxiv.org/abs/2510.11734",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.966Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.966Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.11734",
      "fetched_title": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language models are employed to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. An end-to-end evaluation framework is developed including individual-level analysis of stability and identifiability, and population-level analysis termed progressive personality curves. Main contributions include proposing a systematic framework for virtual personality evaluation, demonstrating the critical role of persona detail in quality, and identifying a scaling law in personality simulation.",
    "abstract_en_extended": "This study, \"Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language models are employed to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. It then advances the context by clarifying that An end-to-end evaluation framework is developed including individual-level analysis of stability and identifiability, and population-level analysis termed progressive personality curves. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Main contributions include proposing a systematic framework for virtual personality evaluation, demonstrating the critical role of persona detail in quality, and identifying a scaling law in personality simulation. It also details that Main contributions include proposing a systematic framework for virtual personality evaluation, demonstrating the critical role of persona detail in quality, and identifying a scaling law in personality simulation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se emplean modelos de lenguaje de gran escala para simular experimentos sociales, explorando su capacidad para emular personalidad humana en juego de roles de persona virtual. Además, contextualiza el aporte al precisar que Se desarrolla un marco de evaluación de extremo a extremo que incluye análisis a nivel individual de estabilidad e identificabilidad, y análisis a nivel poblacional denominado curvas de personalidad progresivas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las contribuciones principales incluyen proponer un marco sistemático para evaluación de personalidad virtual, demostrar el papel crítico del detalle de persona en la calidad, e identificar una ley de escalado en simulación de personalidad. También se especifica que Las contribuciones principales incluyen proponer un marco sistemático para evaluación de personalidad virtual, demostrar el papel crítico del detalle de persona en la calidad, e identificar una ley de escalado en simulación de personalidad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language models are employed to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing.",
        "An end-to-end evaluation framework is developed including individual-level analysis of stability and identifiability, and population-level analysis termed progressive personality curves.",
        "Main contributions include proposing a systematic framework for virtual personality evaluation, demonstrating the critical role of persona detail in quality, and identifying a scaling law in personality simulation."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-053",
    "legacy_article_number": 53,
    "title_original": "The Illusion of Personality: Uncovering Dissociation Between Self-Reports and Behavior in LLMs",
    "title_canonical": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Pengrui Han",
      "Rafal Kocielnik",
      "Peiyang Song",
      "Ramit Debnath",
      "Dean Mobbs",
      "Anima Anandkumar",
      "R. Michael Alvarez"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2509.03730",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:15.983Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:15.983Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.03730",
      "fetched_title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs",
      "title_similarity": 0.7857,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality in large language models is systematically characterized across three dimensions: dynamic emergence throughout training stages, predictive validity of self-reported traits in behavioral tasks, and impact of targeted interventions. Instructional alignment stabilizes trait expression and strengthens correlations mirroring human data. However, self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. Persona injection steers self-reports but exerts minimal consistent effect on actual behavior.",
    "abstract_en_extended": "This study, \"The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality in large language models is systematically characterized across three dimensions: dynamic emergence throughout training stages, predictive validity of self-reported traits in behavioral tasks, and impact of targeted interventions. It then advances the context by clarifying that Instructional alignment stabilizes trait expression and strengthens correlations mirroring human data. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that However, self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. It also details that Persona injection steers self-reports but exerts minimal consistent effect on actual behavior. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se caracteriza sistemáticamente la personalidad en modelos de lenguaje de gran escala a través de tres dimensiones: surgimiento dinámico durante etapas de entrenamiento, validez predictiva de rasgos autoinformados en tareas conductuales, e impacto de intervenciones dirigidas. Además, contextualiza el aporte al precisar que La alineación instruccional estabiliza la expresión de rasgos y fortalece correlaciones que reflejan datos humanos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Sin embargo, los rasgos autoinformados no predicen fiablemente el comportamiento, y las asociaciones observadas frecuentemente divergen de patrones humanos. También se especifica que La inyección de persona dirige los autoinformes pero ejerce un efecto consistente mínimo en el comportamiento real. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality in large language models is systematically characterized across three dimensions: dynamic emergence throughout training stages, predictive validity of self-reported traits in behavioral tasks, and impact of targeted interventions.",
        "Instructional alignment stabilizes trait expression and strengthens correlations mirroring human data.",
        "However, self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-054",
    "legacy_article_number": 54,
    "title_original": "Large Language Models Show Human-Like Social Desirability Biases in Survey Responses",
    "title_canonical": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Aadesh Salecha",
      "Molly E. Ireland",
      "Shashanka Subrahmanya",
      "João Sedoc",
      "Lyle H. Ungar",
      "Johannes C. Eichstaedt"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Human-Computer Interaction",
      "Social desirability bias"
    ],
    "source_url": "https://arxiv.org/abs/2405.06058",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.000Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.000Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2405.06058",
      "fetched_title": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "An experimental framework using Big Five personality surveys is developed, uncovering a previously undetected social desirability bias across a wide range of large language models. By varying the number of questions, the ability of models to infer when being evaluated is demonstrated. When personality evaluation is inferred, models skew scores toward desirable trait ends. This bias exists in all tested models, with bias levels appearing to increase in more recent models.",
    "abstract_en_extended": "This study, \"Large Language Models Show Human-like Social Desirability Biases in Survey Responses\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that An experimental framework using Big Five personality surveys is developed, uncovering a previously undetected social desirability bias across a wide range of large language models. It then advances the context by clarifying that By varying the number of questions, the ability of models to infer when being evaluated is demonstrated. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that When personality evaluation is inferred, models skew scores toward desirable trait ends. It also details that This bias exists in all tested models, with bias levels appearing to increase in more recent models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large Language Models Show Human-like Social Desirability Biases in Survey Responses\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se desarrolla un marco experimental mediante encuestas de personalidad de los Cinco Grandes, revelando un sesgo de deseabilidad social previamente no detectado en una amplia gama de modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Al variar el número de preguntas, se demuestra la capacidad de los modelos para inferir cuándo están siendo evaluados. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Cuando se infiere la evaluación de personalidad, los modelos sesgan las puntuaciones hacia extremos de rasgos deseables. También se especifica que Este sesgo existe en todos los modelos probados, con niveles de sesgo que parecen aumentar en modelos más recientes. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "An experimental framework using Big Five personality surveys is developed, uncovering a previously undetected social desirability bias across a wide range of large language models.",
        "By varying the number of questions, the ability of models to infer when being evaluated is demonstrated.",
        "When personality evaluation is inferred, models skew scores toward desirable trait ends."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-055",
    "legacy_article_number": 55,
    "title_original": "Can AI Understand Human Personality? Comparing Humans and AI Systems at Predicting Personality Correlations",
    "title_canonical": "Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Philipp Schoenegger",
      "Spencer Greenberg",
      "Alexander Grishin",
      "Joshua Lewis",
      "Lucius Caviola"
    ],
    "keywords": [
      "Computers and Society",
      "Personality Prediction",
      "AI Systems",
      "Personality Correlations"
    ],
    "source_url": "https://arxiv.org/abs/2406.08170",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.016Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.016Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.08170",
      "fetched_title": "Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations",
      "title_similarity": 0.8462,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Abilities of specialized neural networks like PersonalityMap and general large language models like GPT-4o and Claude 3 Opus in understanding human personality are tested. When compared with individual humans, all artificial intelligence models make better predictions than the vast majority of lay people and academic experts. However, when selecting median prediction for each item, experts and PersonalityMap outperform large language models and lay people on most measures.",
    "abstract_en_extended": "This study, \"Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Abilities of specialized neural networks like PersonalityMap and general large language models like GPT-4o and Claude 3 Opus in understanding human personality are tested. It then advances the context by clarifying that When compared with individual humans, all artificial intelligence models make better predictions than the vast majority of lay people and academic experts. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that However, when selecting median prediction for each item, experts and PersonalityMap outperform large language models and lay people on most measures. It also details that However, when selecting median prediction for each item, experts and PersonalityMap outperform large language models and lay people on most measures. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se prueban las capacidades de redes neuronales especializadas como PersonalityMap y modelos de lenguaje de gran escala generales como GPT-4o y Claude 3 Opus en comprender personalidad humana. Además, contextualiza el aporte al precisar que Cuando se comparan con humanos individuales, todos los modelos de inteligencia artificial realizan mejores predicciones que la gran mayoría de personas no expertas y expertos académicos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Sin embargo, al seleccionar la predicción mediana para cada ítem, expertos y PersonalityMap superan a los modelos de lenguaje de gran escala y personas no expertas en la mayoría de medidas. También se especifica que Sin embargo, al seleccionar la predicción mediana para cada ítem, expertos y PersonalityMap superan a los modelos de lenguaje de gran escala y personas no expertas en la mayoría de medidas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Abilities of specialized neural networks like PersonalityMap and general large language models like GPT-4o and Claude 3 Opus in understanding human personality are tested.",
        "When compared with individual humans, all artificial intelligence models make better predictions than the vast majority of lay people and academic experts.",
        "However, when selecting median prediction for each item, experts and PersonalityMap outperform large language models and lay people on most measures."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-056",
    "legacy_article_number": 56,
    "title_original": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
    "title_canonical": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Myra Cheng",
      "Esin Durmus",
      "Dan Jurafsky"
    ],
    "keywords": [
      "Large Language Models",
      "Stereotypes",
      "Demographic Groups",
      "Intersectionality",
      "NLP",
      "Sociolinguistics",
      "Markedness",
      "Bias Detection"
    ],
    "source_url": "https://aclanthology.org/2023.acl-long.84/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.018Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.018Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.acl-long.84/",
      "fetched_title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Marked Personas, a prompt-based method to measure stereotypes in large language models for intersectional demographic groups without lexicon or labeling, is presented. Grounded in the markedness concept, the method prompts models to generate personas of target groups alongside unmarked defaults, then identifies distinguishing words. Results show GPT-3.5 and GPT-4 portrayals contain higher rates of racial stereotypes than human-written portrayals. An intersectional lens reveals tropes dominating portrayals of marginalized groups.",
    "abstract_en_extended": "This study, \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Marked Personas, a prompt-based method to measure stereotypes in large language models for intersectional demographic groups without lexicon or labeling, is presented. It then advances the context by clarifying that Grounded in the markedness concept, the method prompts models to generate personas of target groups alongside unmarked defaults, then identifies distinguishing words. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results show GPT-3.5 and GPT-4 portrayals contain higher rates of racial stereotypes than human-written portrayals. It also details that An intersectional lens reveals tropes dominating portrayals of marginalized groups. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta Personas Marcadas, un método basado en instrucciones para medir estereotipos en modelos de lenguaje de gran escala para grupos demográficos interseccionales sin léxico o etiquetado. Además, contextualiza el aporte al precisar que Fundamentado en el concepto de marcación, el método solicita a los modelos generar personas de grupos objetivo junto con valores predeterminados no marcados, luego identifica palabras distintivas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados muestran que las representaciones de GPT-3.5 y GPT-4 contienen tasas más altas de estereotipos raciales que representaciones escritas por humanos. También se especifica que Una perspectiva interseccional revela tropos que dominan las representaciones de grupos marginalizados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Marked Personas, a prompt-based method to measure stereotypes in large language models for intersectional demographic groups without lexicon or labeling, is presented.",
        "Grounded in the markedness concept, the method prompts models to generate personas of target groups alongside unmarked defaults, then identifies distinguishing words.",
        "Results show GPT-3.5 and GPT-4 portrayals contain higher rates of racial stereotypes than human-written portrayals."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-057",
    "legacy_article_number": 57,
    "title_original": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models",
    "title_canonical": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Yang Trista Cao",
      "Anna Sotnikova",
      "Hal Daumé III",
      "Rachel Rudinger",
      "Linda Zou"
    ],
    "keywords": [
      "Natural Language Processing",
      "Social Stereotypes",
      "Language Models",
      "Intersectional Identities",
      "ABC Stereotype Model"
    ],
    "source_url": "https://aclanthology.org/2022.naacl-main.92/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.040Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.040Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2022.naacl-main.92/",
      "fetched_title": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "The Agency-Belief-Communion (ABC) stereotype model from social psychology is adapted as a framework for systematic study of stereotypic group-trait associations in language models. The sensitivity test (SeT) is introduced for measuring stereotypical associations. To evaluate SeT using the ABC model, group-trait judgments from U.S. subjects were collected for comparison with English language model stereotypes. The framework extends to measure stereotyping of intersectional identities.",
    "abstract_en_extended": "This study, \"Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The Agency-Belief-Communion (ABC) stereotype model from social psychology is adapted as a framework for systematic study of stereotypic group-trait associations in language models. It then advances the context by clarifying that The sensitivity test (SeT) is introduced for measuring stereotypical associations. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that To evaluate SeT using the ABC model, group-trait judgments from U.S. It also details that subjects were collected for comparison with English language model stereotypes. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se adapta el modelo de estereotipos Agencia-Creencia-Comunión (ABC) de la psicología social como marco para estudio sistemático de asociaciones estereotípicas grupo-rasgo en modelos de lenguaje. Además, contextualiza el aporte al precisar que Se introduce la prueba de sensibilidad (SeT) para medir asociaciones estereotípicas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Para evaluar SeT mediante el modelo ABC, se recopilaron juicios grupo-rasgo de sujetos estadounidenses para comparación con estereotipos de modelos de lenguaje inglés. También se especifica que El marco se extiende para medir estereotipos de identidades interseccionales. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The Agency-Belief-Communion (ABC) stereotype model from social psychology is adapted as a framework for systematic study of stereotypic group-trait associations in language models.",
        "The sensitivity test (SeT) is introduced for measuring stereotypical associations.",
        "To evaluate SeT using the ABC model, group-trait judgments from U.S."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-058",
    "legacy_article_number": 58,
    "title_original": "Uncovering Stereotypes in Large Language Models: A Task Complexity-Based Approach",
    "title_canonical": "Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Hari Shrawgi",
      "Prasanjit Rath",
      "Tushar Singhal",
      "Sandipan Dandapat"
    ],
    "keywords": [
      "Large Language Models",
      "Bias evaluation",
      "Stereotypes",
      "AI ethics",
      "Social bias",
      "Task complexity",
      "Nationality",
      "Gender",
      "Race",
      "Religion"
    ],
    "source_url": "https://aclanthology.org/2024.eacl-long.111/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.073Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.073Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.eacl-long.111/",
      "fetched_title": "Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Holistic bias evaluation is addressed with an extensible benchmark, the LLM Stereotype Index (LSI), grounded in the Social Progress Index. Breadth and depth of bias protection are tested via tasks with varying complexities. ChatGPT and GPT-4 exhibit strong inherent prejudice with respect to nationality, gender, race, and religion. Exhibition of issues becomes increasingly apparent as task complexity increases. GPT-4 is better at hiding biases, but when displayed they are more significant.",
    "abstract_en_extended": "This study, \"Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Holistic bias evaluation is addressed with an extensible benchmark, the LLM Stereotype Index (LSI), grounded in the Social Progress Index. It then advances the context by clarifying that Breadth and depth of bias protection are tested via tasks with varying complexities. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that ChatGPT and GPT-4 exhibit strong inherent prejudice with respect to nationality, gender, race, and religion. It also details that Exhibition of issues becomes increasingly apparent as task complexity increases. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se aborda la evaluación holística de sesgo con un benchmark extensible, el Índice de Estereotipos de modelos de lenguaje (LSI), fundamentado en el Índice de Progreso Social. Además, contextualiza el aporte al precisar que Se prueba amplitud y profundidad de protección de sesgo mediante tareas con complejidades variables. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que ChatGPT y GPT-4 exhiben prejuicios inherentes fuertes respecto a nacionalidad, género, raza y religión. También se especifica que La exhibición de problemas se vuelve cada vez más aparente a medida que aumenta la complejidad de la tarea. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Holistic bias evaluation is addressed with an extensible benchmark, the LLM Stereotype Index (LSI), grounded in the Social Progress Index.",
        "Breadth and depth of bias protection are tested via tasks with varying complexities.",
        "ChatGPT and GPT-4 exhibit strong inherent prejudice with respect to nationality, gender, race, and religion."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-059",
    "legacy_article_number": 59,
    "title_original": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts",
    "title_canonical": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Naseela Pervez",
      "Alexander J. Titus"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Gender Bias",
      "Scientific Writing"
    ],
    "source_url": "https://arxiv.org/abs/2406.19497",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.368Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.368Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.19497",
      "fetched_title": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Alignment of three prominent large language models - Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash - is assessed by analyzing their performance on benchmark text-generation tasks for scientific abstracts. Using the LIWC framework to extract features from generated texts, findings indicate that while models generally produce text resembling human content, variations in stylistic features suggest significant gender biases. This highlights the importance of developing models that maintain diversity of writing styles.",
    "abstract_en_extended": "This study, \"Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Alignment of three prominent large language models - Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash - is assessed by analyzing their performance on benchmark text-generation tasks for scientific abstracts. It then advances the context by clarifying that Using the LIWC framework to extract features from generated texts, findings indicate that while models generally produce text resembling human content, variations in stylistic features suggest significant gender biases. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This highlights the importance of developing models that maintain diversity of writing styles. It also details that This highlights the importance of developing models that maintain diversity of writing styles. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se evalúa la alineación de tres modelos de lenguaje de gran escala prominentes - Claude 3 Opus, Mistral AI Large y Gemini 1.5 Flash - analizando su rendimiento en tareas de generación de texto de benchmark para resúmenes científicos. Además, contextualiza el aporte al precisar que Mediante el marco LIWC para extraer características de textos generados, los hallazgos indican que aunque los modelos generalmente producen texto similar al contenido humano, variaciones en características estilísticas sugieren sesgos de género significativos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Esto destaca la importancia de desarrollar modelos que mantengan diversidad de estilos de escritura. También se especifica que Esto destaca la importancia de desarrollar modelos que mantengan diversidad de estilos de escritura. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Alignment of three prominent large language models - Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash - is assessed by analyzing their performance on benchmark text-generation tasks for scientific abstracts.",
        "Using the LIWC framework to extract features from generated texts, findings indicate that while models generally produce text resembling human content, variations in stylistic features suggest significant gender biases.",
        "This highlights the importance of developing models that maintain diversity of writing styles."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-061",
    "legacy_article_number": 61,
    "title_original": "Investigating the Impact of LLM Personality on the Manifestation of Cognitive Biases in Decision-Making Tasks",
    "title_canonical": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jiangen He",
      "Jiqun Liu"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Large Language Models",
      "Cognitive Biases",
      "Decision-Making",
      "Personality Traits"
    ],
    "source_url": "https://arxiv.org/abs/2502.14219",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.383Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.383Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.14219",
      "fetched_title": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks",
      "title_similarity": 0.8125,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "How personality traits influence cognitive biases in large language models is explored, and effectiveness of mitigation strategies across model architectures is evaluated. Six prevalent cognitive biases are identified while sunk cost and group attribution biases show minimal impact. Personality traits play crucial roles in either amplifying or reducing biases. Responsibility and agreeableness may enhance efficacy of bias mitigation strategies, suggesting models exhibiting these traits are more receptive to corrective measures.",
    "abstract_en_extended": "This study, \"Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that How personality traits influence cognitive biases in large language models is explored, and effectiveness of mitigation strategies across model architectures is evaluated. It then advances the context by clarifying that Six prevalent cognitive biases are identified while sunk cost and group attribution biases show minimal impact. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Personality traits play crucial roles in either amplifying or reducing biases. It also details that Responsibility and agreeableness may enhance efficacy of bias mitigation strategies, suggesting models exhibiting these traits are more receptive to corrective measures. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora cómo los rasgos de personalidad influyen en sesgos cognitivos en modelos de lenguaje de gran escala, y se evalúa la efectividad de estrategias de mitigación a través de arquitecturas de modelo. Además, contextualiza el aporte al precisar que Se identifican seis sesgos cognitivos prevalentes mientras que los sesgos de costo hundido y atribución de grupo muestran impacto mínimo. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los rasgos de personalidad desempeñan roles cruciales al amplificar o reducir sesgos. También se especifica que Responsabilidad y amabilidad pueden mejorar la eficacia de estrategias de mitigación de sesgo, sugiriendo que los modelos que exhiben estos rasgos son más receptivos a medidas correctivas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "How personality traits influence cognitive biases in large language models is explored, and effectiveness of mitigation strategies across model architectures is evaluated.",
        "Six prevalent cognitive biases are identified while sunk cost and group attribution biases show minimal impact.",
        "Personality traits play crucial roles in either amplifying or reducing biases."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-062",
    "legacy_article_number": 62,
    "title_original": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
    "title_canonical": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Messi H.J. Lee",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "keywords": [
      "Large Language Models",
      "Social bias",
      "Homogeneity perception",
      "Racial minorities",
      "Fairness",
      "Accountability"
    ],
    "source_url": "https://doi.org/10.1145/3630106.3658975",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.393Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.393Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1145/3630106.3658975",
      "fetched_title": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "A new form of bias in large language models resembling a social psychological phenomenon is investigated, where socially subordinate groups are perceived as more homogeneous than dominant groups. ChatGPT was prompted to generate texts about intersectional group identities for comparison on homogeneity measures. ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, describing racial minority groups with a narrower range of human experience. Women were also portrayed as more homogeneous than men, though differences were small.",
    "abstract_en_extended": "This study, \"Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A new form of bias in large language models resembling a social psychological phenomenon is investigated, where socially subordinate groups are perceived as more homogeneous than dominant groups. It then advances the context by clarifying that ChatGPT was prompted to generate texts about intersectional group identities for comparison on homogeneity measures. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, describing racial minority groups with a narrower range of human experience. It also details that Women were also portrayed as more homogeneous than men, though differences were small. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga una nueva forma de sesgo en modelos de lenguaje de gran escala que se asemeja a un fenómeno psicológico social, donde los grupos socialmente subordinados se perciben como más homogéneos que los grupos dominantes. Además, contextualiza el aporte al precisar que Se solicitó a ChatGPT generar textos sobre identidades de grupos interseccionales para comparación en medidas de homogeneidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que ChatGPT retrató a afroamericanos, asiaticoamericanos e hispanoamericanos como más homogéneos que los estadounidenses blancos, describiendo grupos minoritarios raciales con una gama más estrecha de experiencia humana. También se especifica que Las mujeres también fueron retratadas como más homogéneas que los hombres, aunque las diferencias fueron pequeñas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A new form of bias in large language models resembling a social psychological phenomenon is investigated, where socially subordinate groups are perceived as more homogeneous than dominant groups.",
        "ChatGPT was prompted to generate texts about intersectional group identities for comparison on homogeneity measures.",
        "ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, describing racial minority groups with a narrower range of human experience."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-063",
    "legacy_article_number": 63,
    "title_original": "Performance and Biases of Large Language Models in Simulating Public Opinion",
    "title_canonical": "Performance and biases of Large Language Models in public opinion simulation - Humanities and Social Sciences Communications",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yao Qu",
      "Jue Wang"
    ],
    "keywords": [
      "Large Language Models",
      "Public opinion simulation",
      "World Values Survey",
      "Bias",
      "Cultural differences"
    ],
    "source_url": "https://www.nature.com/articles/s41599-024-03609-x?error=cookies_not_supported&code=72a1aefa-446e-4276-8d0f-bdb4b19bb985",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.404Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.404Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41599-024-03609-x?error=cookies_not_supported&code=72a1aefa-446e-4276-8d0f-bdb4b19bb985",
      "fetched_title": "Performance and biases of Large Language Models in public opinion simulation - Humanities and Social Sciences Communications",
      "title_similarity": 0.625,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "ChatGPT's performance in simulating public opinion is evaluated using World Values Survey data across diverse contexts. Significant performance disparities are found, especially when comparing countries, with the model performing better in Western, English-speaking, developed nations. Demographic biases related to gender, ethnicity, age, education, and social class are uncovered. Accuracy is significantly higher in Western countries and much lower elsewhere, with simulated responses exhibiting demographic biases.",
    "abstract_en_extended": "This study, \"Performance and biases of Large Language Models in public opinion simulation - Humanities and Social Sciences Communications\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that ChatGPT's performance in simulating public opinion is evaluated using World Values Survey data across diverse contexts. It then advances the context by clarifying that Significant performance disparities are found, especially when comparing countries, with the model performing better in Western, English-speaking, developed nations. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Demographic biases related to gender, ethnicity, age, education, and social class are uncovered. It also details that Accuracy is significantly higher in Western countries and much lower elsewhere, with simulated responses exhibiting demographic biases. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Performance and biases of Large Language Models in public opinion simulation - Humanities and Social Sciences Communications\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se evalúa el rendimiento de ChatGPT en simulación de opinión pública mediante datos de la Encuesta Mundial de Valores a través de contextos diversos. Además, contextualiza el aporte al precisar que Se encuentran disparidades de rendimiento significativas, especialmente al comparar países, con el modelo desempeñándose mejor en naciones occidentales, de habla inglesa y desarrolladas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se descubren sesgos demográficos relacionados con género, etnia, edad, educación y clase social. También se especifica que La precisión es significativamente mayor en países occidentales y mucho menor en otros lugares, con respuestas simuladas exhibiendo sesgos demográficos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "ChatGPT's performance in simulating public opinion is evaluated using World Values Survey data across diverse contexts.",
        "Significant performance disparities are found, especially when comparing countries, with the model performing better in Western, English-speaking, developed nations.",
        "Demographic biases related to gender, ethnicity, age, education, and social class are uncovered."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-064",
    "legacy_article_number": 64,
    "title_original": "Measuring Gender and Racial Biases in Large Language Models: Intersectional Evidence from Automatic Resume Evaluation",
    "title_canonical": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jiafu An",
      "Difang Huang",
      "Chen Lin",
      "Mingzhu Tai"
    ],
    "keywords": [
      "Large Language Models",
      "Gender bias",
      "Racial bias",
      "Intersectionality",
      "Resume evaluation",
      "Hiring discrimination"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgaf089",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.422Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.422Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgaf089",
      "fetched_title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "title_similarity": 0.875,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Gender and racial biases in commonly used large language models including GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet, and Llama 3-70b are investigated in resume evaluation context. Models were instructed to score approximately 361,000 resumes with randomized social identities. Models award higher assessment scores for female candidates with similar qualifications, while many are biased against black male candidates. These biases may result in 1-3 percentage-point differences in hiring probabilities for similar candidates.",
    "abstract_en_extended": "This study, \"Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Gender and racial biases in commonly used large language models including GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet, and Llama 3-70b are investigated in resume evaluation context. It then advances the context by clarifying that Models were instructed to score approximately 361,000 resumes with randomized social identities. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Models award higher assessment scores for female candidates with similar qualifications, while many are biased against black male candidates. It also details that These biases may result in 1-3 percentage-point differences in hiring probabilities for similar candidates. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investigan sesgos de género y raciales en modelos de lenguaje de gran escala comúnmente usados incluyendo GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet y Llama 3-70b en contexto de evaluación de currículum. Además, contextualiza el aporte al precisar que Se instruyó a los modelos para calificar aproximadamente 361,000 currículums con identidades sociales aleatorizadas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los modelos otorgan puntuaciones de evaluación más altas para candidatas femeninas con calificaciones similares, mientras que muchos están sesgados contra candidatos masculinos negros. También se especifica que Estos sesgos pueden resultar en diferencias de 1-3 puntos porcentuales en probabilidades de contratación para candidatos similares. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Gender and racial biases in commonly used large language models including GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet, and Llama 3-70b are investigated in resume evaluation context.",
        "Models were instructed to score approximately 361,000 resumes with randomized social identities.",
        "Models award higher assessment scores for female candidates with similar qualifications, while many are biased against black male candidates."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-065",
    "legacy_article_number": 65,
    "title_original": "Large Language Models Can Infer Psychological Dispositions of Social Media Users",
    "title_canonical": "Large language models can infer psychological dispositions of social media users",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Heinrich Peters",
      "Sandra C Matz"
    ],
    "keywords": [
      "Large Language Models",
      "Personality inference",
      "Big Five",
      "Social media",
      "Zero-shot learning",
      "GPT-3.5",
      "GPT-4"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgae231",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.680Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.680Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgae231",
      "fetched_title": "Large language models can infer psychological dispositions of social media users",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Whether large language models like ChatGPT can accurately infer psychological dispositions of social media users is investigated. Specifically, whether GPT-3.5 and GPT-4 can derive Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario is tested. Results showed average correlation of r = .29 between model-inferred and self-reported trait scores, accuracy similar to supervised machine learning models trained for personality inference. Heterogeneity in accuracy across age groups and gender categories is highlighted.",
    "abstract_en_extended": "This study, \"Large language models can infer psychological dispositions of social media users\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Whether large language models like ChatGPT can accurately infer psychological dispositions of social media users is investigated. It then advances the context by clarifying that Specifically, whether GPT-3.5 and GPT-4 can derive Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario is tested. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results showed average correlation of r = .29 between model-inferred and self-reported trait scores, accuracy similar to supervised machine learning models trained for personality inference. It also details that Heterogeneity in accuracy across age groups and gender categories is highlighted. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large language models can infer psychological dispositions of social media users\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga si modelos de lenguaje de gran escala como ChatGPT pueden inferir con precisión disposiciones psicológicas de usuarios de redes sociales. Además, contextualiza el aporte al precisar que Específicamente, se prueba si GPT-3.5 y GPT-4 pueden derivar rasgos de personalidad de los Cinco Grandes de actualizaciones de estado de Facebook de usuarios en escenario de aprendizaje de cero disparos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados mostraron correlación promedio de r = .29 entre puntuaciones de rasgos inferidas por el modelo y autoinformadas, precisión similar a modelos de aprendizaje automático supervisado entrenados para inferencia de personalidad. También se especifica que Se destaca heterogeneidad en precisión a través de grupos de edad y categorías de género. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Whether large language models like ChatGPT can accurately infer psychological dispositions of social media users is investigated.",
        "Specifically, whether GPT-3.5 and GPT-4 can derive Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario is tested.",
        "Results showed average correlation of r = .29 between model-inferred and self-reported trait scores, accuracy similar to supervised machine learning models trained for personality inference."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-066",
    "legacy_article_number": 66,
    "title_original": "How Do Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
    "title_canonical": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ],
    "keywords": [
      "Personality traits",
      "Large language models",
      "Negotiation simulation",
      "Decision-making",
      "Big Five",
      "Bargaining dialogues"
    ],
    "source_url": "https://aclanthology.org/2024.findings-emnlp.605/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.736Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.736Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.605/",
      "fetched_title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
      "title_similarity": 0.9231,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A simulation framework centered on large language model agents endowed with synthesized personality traits is introduced. Agents negotiate within bargaining domains and possess customizable personalities and objectives. Experimental results show behavioral tendencies of simulations can reproduce behavioral patterns observed in human negotiations. The contribution is twofold: proposing simulation methodology investigating alignment between linguistic and economic capabilities of agents, and offering empirical insights into strategic impacts of Big Five traits on bilateral negotiation outcomes.",
    "abstract_en_extended": "This study, \"How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A simulation framework centered on large language model agents endowed with synthesized personality traits is introduced. It then advances the context by clarifying that Agents negotiate within bargaining domains and possess customizable personalities and objectives. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Experimental results show behavioral tendencies of simulations can reproduce behavioral patterns observed in human negotiations. It also details that The contribution is twofold: proposing simulation methodology investigating alignment between linguistic and economic capabilities of agents, and offering empirical insights into strategic impacts of Big Five traits on bilateral negotiation outcomes. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce un marco de simulación centrado en agentes de modelos de lenguaje de gran escala dotados de rasgos de personalidad sintetizados. Además, contextualiza el aporte al precisar que Los agentes negocian dentro de dominios de negociación y poseen personalidades y objetivos personalizables. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados experimentales muestran que las tendencias conductuales de las simulaciones pueden reproducir patrones conductuales observados en negociaciones humanas. También se especifica que La contribución es doble: proponer metodología de simulación investigando alineación entre capacidades lingüísticas y económicas de agentes, y ofrecer perspectivas empíricas sobre impactos estratégicos de rasgos de los Cinco Grandes en resultados de negociaciones bilaterales. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A simulation framework centered on large language model agents endowed with synthesized personality traits is introduced.",
        "Agents negotiate within bargaining domains and possess customizable personalities and objectives.",
        "Experimental results show behavioral tendencies of simulations can reproduce behavioral patterns observed in human negotiations."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-067",
    "legacy_article_number": 67,
    "title_original": "Unveiling Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition in Dialogues",
    "title_canonical": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Lei Sun",
      "Jinming Zhao",
      "Qin Jin"
    ],
    "keywords": [
      "Personality recognition",
      "Explainable AI",
      "Dialogue analysis",
      "Personality traits",
      "Machine learning",
      "Chain-of-Personality-Evidence (CoPE)"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-main.1115/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:16.834Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:16.834Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-main.1115/",
      "fetched_title": "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues",
      "title_similarity": 0.6923,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A novel task named Explainable Personality Recognition is proposed, aiming to reveal the reasoning process as supporting evidence of personality traits. Inspired by personality theories where traits are composed of stable patterns of personality states, the Chain-of-Personality-Evidence (CoPE) framework is proposed, involving reasoning from specific contexts to short-term states to long-term traits. Based on CoPE, the explainable personality recognition dataset PersonalityEvd is constructed from dialogues, introducing two tasks requiring models to recognize labels and supporting evidence.",
    "abstract_en_extended": "This study, \"Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel task named Explainable Personality Recognition is proposed, aiming to reveal the reasoning process as supporting evidence of personality traits. It then advances the context by clarifying that Inspired by personality theories where traits are composed of stable patterns of personality states, the Chain-of-Personality-Evidence (CoPE) framework is proposed, involving reasoning from specific contexts to short-term states to long-term traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Based on CoPE, the explainable personality recognition dataset PersonalityEvd is constructed from dialogues, introducing two tasks requiring models to recognize labels and supporting evidence. It also details that Based on CoPE, the explainable personality recognition dataset PersonalityEvd is constructed from dialogues, introducing two tasks requiring models to recognize labels and supporting evidence. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone una tarea novedosa denominada Reconocimiento de Personalidad Explicable, con el objetivo de revelar el proceso de razonamiento como evidencia de apoyo de los rasgos de personalidad. Además, contextualiza el aporte al precisar que Inspirado por teorías de personalidad donde los rasgos se componen de patrones estables de estados de personalidad, se propone el marco Cadena-de-Evidencia-de-Personalidad (CoPE), involucrando razonamiento desde contextos específicos a estados de corto plazo hasta rasgos de largo plazo. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Basado en CoPE, se construye el conjunto de datos de reconocimiento de personalidad explicable PersonalityEvd a partir de diálogos, introduciendo dos tareas que requieren que los modelos reconozcan etiquetas y evidencia de apoyo. También se especifica que Basado en CoPE, se construye el conjunto de datos de reconocimiento de personalidad explicable PersonalityEvd a partir de diálogos, introduciendo dos tareas que requieren que los modelos reconozcan etiquetas y evidencia de apoyo. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel task named Explainable Personality Recognition is proposed, aiming to reveal the reasoning process as supporting evidence of personality traits.",
        "Inspired by personality theories where traits are composed of stable patterns of personality states, the Chain-of-Personality-Evidence (CoPE) framework is proposed, involving reasoning from specific contexts to short-term states to long-term traits.",
        "Based on CoPE, the explainable personality recognition dataset PersonalityEvd is constructed from dialogues, introducing two tasks requiring models to recognize labels and supporting evidence."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-068",
    "legacy_article_number": 68,
    "title_original": "Bias and Fairness in Large Language Models: A Survey",
    "title_canonical": "Bias and Fairness in Large Language Models: A Survey",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Isabel O. Gallegos",
      "Ryan A. Rossi",
      "Joe Barrow",
      "Md Mehrab Tanjim",
      "Sungchul Kim",
      "Franck Dernoncourt",
      "Tong Yu",
      "Ruiyi Zhang",
      "Nesreen K. Ahmed"
    ],
    "keywords": [
      "Large Language Models",
      "Bias evaluation",
      "Fairness",
      "Social bias",
      "Natural language processing",
      "AI ethics"
    ],
    "source_url": "https://doi.org/10.1162/coli_a_00524",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.025Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.025Z",
      "checks": [
        "http_status_non_200",
        "crossref_title_checked",
        "verified"
      ],
      "reason": "verified_crossref_title",
      "http_status": 403,
      "final_url": "https://doi.org/10.1162/coli_a_00524",
      "fetched_title": "Bias and Fairness in Large Language Models: A Survey",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Bias evaluation and mitigation techniques for large language models are presented in this comprehensive survey, consolidating, formalizing, and expanding notions of social bias and fairness in natural language processing. Three intuitive taxonomies are proposed: two for bias evaluation (metrics and datasets) and one for mitigation. Distinct facets of harm are defined, desiderata to operationalize fairness are introduced, and metrics are organized by different operational levels: embeddings, probabilities, and generated text.",
    "abstract_en_extended": "This study, \"Bias and Fairness in Large Language Models: A Survey\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Bias evaluation and mitigation techniques for large language models are presented in this comprehensive survey, consolidating, formalizing, and expanding notions of social bias and fairness in natural language processing. It then advances the context by clarifying that Three intuitive taxonomies are proposed: two for bias evaluation (metrics and datasets) and one for mitigation. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Distinct facets of harm are defined, desiderata to operationalize fairness are introduced, and metrics are organized by different operational levels: embeddings, probabilities, and generated text. It also details that Distinct facets of harm are defined, desiderata to operationalize fairness are introduced, and metrics are organized by different operational levels: embeddings, probabilities, and generated text. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Bias and Fairness in Large Language Models: A Survey\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presentan técnicas de evaluación y mitigación de sesgo para modelos de lenguaje de gran escala en esta revisión integral, consolidando, formalizando y expandiendo nociones de sesgo social y equidad en procesamiento de lenguaje natural. Además, contextualiza el aporte al precisar que Se proponen tres taxonomías intuitivas: dos para evaluación de sesgo (métricas y conjuntos de datos) y una para mitigación. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se definen facetas distintas de daño, se introducen requisitos para operacionalizar equidad, y se organizan métricas por diferentes niveles operacionales: embeddings, probabilidades y texto generado. También se especifica que Se definen facetas distintas de daño, se introducen requisitos para operacionalizar equidad, y se organizan métricas por diferentes niveles operacionales: embeddings, probabilidades y texto generado. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Bias evaluation and mitigation techniques for large language models are presented in this comprehensive survey, consolidating, formalizing, and expanding notions of social bias and fairness in natural language processing.",
        "Three intuitive taxonomies are proposed: two for bias evaluation (metrics and datasets) and one for mitigation.",
        "Distinct facets of harm are defined, desiderata to operationalize fairness are introduced, and metrics are organized by different operational levels: embeddings, probabilities, and generated text."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-069",
    "legacy_article_number": 69,
    "title_original": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering",
    "title_canonical": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Rumi Allbert",
      "James K. Wiles",
      "Vlad Grankovsky"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Large Language Models",
      "Activation Engineering",
      "Personality Traits"
    ],
    "source_url": "https://arxiv.org/abs/2412.10427",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.085Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.085Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2412.10427",
      "fetched_title": "Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality modification in large language models is explored, building on the novel approach of activation engineering. A method for identifying and adjusting activation directions related to personality traits is developed, which may allow for dynamic personality fine-tuning. This work aims to further understanding of model interpretability while examining ethical implications of such developments.",
    "abstract_en_extended": "This study, \"Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality modification in large language models is explored, building on the novel approach of activation engineering. It then advances the context by clarifying that A method for identifying and adjusting activation directions related to personality traits is developed, which may allow for dynamic personality fine-tuning. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This work aims to further understanding of model interpretability while examining ethical implications of such developments. It also details that This work aims to further understanding of model interpretability while examining ethical implications of such developments. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora la modificación de personalidad en modelos de lenguaje de gran escala, construyendo sobre el enfoque novedoso de ingeniería de activación. Además, contextualiza el aporte al precisar que Se desarrolla un método para identificar y ajustar direcciones de activación relacionadas con rasgos de personalidad, lo que puede permitir ajuste fino dinámico de personalidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Este trabajo tiene como objetivo profundizar la comprensión de la interpretabilidad del modelo mientras examina las implicaciones éticas de tales desarrollos. También se especifica que Este trabajo tiene como objetivo profundizar la comprensión de la interpretabilidad del modelo mientras examina las implicaciones éticas de tales desarrollos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality modification in large language models is explored, building on the novel approach of activation engineering.",
        "A method for identifying and adjusting activation directions related to personality traits is developed, which may allow for dynamic personality fine-tuning.",
        "This work aims to further understanding of model interpretability while examining ethical implications of such developments."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-070",
    "legacy_article_number": 70,
    "title_original": "PUB: A Personality-Enhanced LLM-Driven User Behavior Simulator for Recommender System Evaluation",
    "title_canonical": "PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Chenglong Ma",
      "Ziqi Xu",
      "Yongli Ren",
      "Danula Hettiachchi",
      "Jeffrey Chan"
    ],
    "keywords": [
      "Information Retrieval",
      "Recommender Systems",
      "User Behavior Simulation",
      "Personality Traits",
      "Large Language Models"
    ],
    "source_url": "https://arxiv.org/abs/2506.04551",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.099Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.099Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2506.04551",
      "fetched_title": "PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation",
      "title_similarity": 0.7857,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The Personality-driven User Behaviour Simulator (PUB) is proposed, a simulation framework based on large language models integrating Big Five personality traits to model personalized user behaviour. PUB dynamically infers user personality from behavioural logs and item metadata, then generates synthetic interactions preserving statistical fidelity to real-world data. Experiments show logs generated by PUB closely align with real user behaviour and reveal meaningful associations between personality traits and recommendation outcomes.",
    "abstract_en_extended": "This study, \"PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The Personality-driven User Behaviour Simulator (PUB) is proposed, a simulation framework based on large language models integrating Big Five personality traits to model personalized user behaviour. It then advances the context by clarifying that PUB dynamically infers user personality from behavioural logs and item metadata, then generates synthetic interactions preserving statistical fidelity to real-world data. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Experiments show logs generated by PUB closely align with real user behaviour and reveal meaningful associations between personality traits and recommendation outcomes. It also details that Experiments show logs generated by PUB closely align with real user behaviour and reveal meaningful associations between personality traits and recommendation outcomes. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PUB: An LLM-Enhanced Personality-Driven User Behaviour Simulator for Recommender System Evaluation\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone el Simulador de Comportamiento de Usuario Impulsado por Personalidad (PUB), un marco de simulación basado en modelos de lenguaje de gran escala que integra rasgos de personalidad de los Cinco Grandes para modelar comportamiento de usuario personalizado. Además, contextualiza el aporte al precisar que PUB infiere dinámicamente la personalidad del usuario desde registros conductuales y metadatos de ítems, luego genera interacciones sintéticas preservando fidelidad estadística a datos del mundo real. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los experimentos muestran que los registros generados por PUB se alinean estrechamente con el comportamiento de usuario real y revelan asociaciones significativas entre rasgos de personalidad y resultados de recomendación. También se especifica que Los experimentos muestran que los registros generados por PUB se alinean estrechamente con el comportamiento de usuario real y revelan asociaciones significativas entre rasgos de personalidad y resultados de recomendación. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The Personality-driven User Behaviour Simulator (PUB) is proposed, a simulation framework based on large language models integrating Big Five personality traits to model personalized user behaviour.",
        "PUB dynamically infers user personality from behavioural logs and item metadata, then generates synthetic interactions preserving statistical fidelity to real-world data.",
        "Experiments show logs generated by PUB closely align with real user behaviour and reveal meaningful associations between personality traits and recommendation outcomes."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-071",
    "legacy_article_number": 71,
    "title_original": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?",
    "title_canonical": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Bin Han",
      "Deuksin Kwon",
      "Spencer Lin",
      "Kaleen Shrestha",
      "Jonathan Gratch"
    ],
    "keywords": [
      "Human-Computer Interaction",
      "Large Language Models",
      "Virtual Agents",
      "Personality Traits",
      "Extraversion"
    ],
    "source_url": "https://arxiv.org/abs/2508.21087",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.117Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.117Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.21087",
      "fetched_title": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "A framework employing personality prompting with large language models is proposed to generate verbal and nonverbal behaviors for virtual agents based on personality traits. Focusing on extraversion, the system was evaluated in negotiation and ice-breaking scenarios using introverted and extroverted agents. Results demonstrate that large language models can generate verbal and nonverbal behaviors aligning with personality traits, and users are able to recognize these traits through agents' behaviors. These findings underscore the potential of large language models in shaping personality-aligned virtual agents.",
    "abstract_en_extended": "This study, \"Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A framework employing personality prompting with large language models is proposed to generate verbal and nonverbal behaviors for virtual agents based on personality traits. It then advances the context by clarifying that Focusing on extraversion, the system was evaluated in negotiation and ice-breaking scenarios using introverted and extroverted agents. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results demonstrate that large language models can generate verbal and nonverbal behaviors aligning with personality traits, and users are able to recognize these traits through agents' behaviors. It also details that These findings underscore the potential of large language models in shaping personality-aligned virtual agents. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un marco que emplea instrucciones de personalidad con modelos de lenguaje de gran escala para generar comportamientos verbales y no verbales en agentes virtuales basados en rasgos de personalidad. Además, contextualiza el aporte al precisar que Con enfoque en extraversión, el sistema fue evaluado en escenarios de negociación y rompehielos utilizando agentes introvertidos y extrovertidos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados demuestran que los modelos de lenguaje de gran escala pueden generar comportamientos verbales y no verbales alineados con rasgos de personalidad, y los usuarios son capaces de reconocer estos rasgos a través de los comportamientos de los agentes. También se especifica que Estos hallazgos subrayan el potencial de los modelos de lenguaje de gran escala en moldear agentes virtuales alineados con la personalidad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A framework employing personality prompting with large language models is proposed to generate verbal and nonverbal behaviors for virtual agents based on personality traits.",
        "Focusing on extraversion, the system was evaluated in negotiation and ice-breaking scenarios using introverted and extroverted agents.",
        "Results demonstrate that large language models can generate verbal and nonverbal behaviors aligning with personality traits, and users are able to recognize these traits through agents' behaviors."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-072",
    "legacy_article_number": 72,
    "title_original": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents",
    "title_canonical": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Lewis Newsham",
      "Daniel Prince"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Multiagent Systems",
      "Large Language Models",
      "Autonomous Agents",
      "OCEAN model",
      "Cyber Defense"
    ],
    "source_url": "https://arxiv.org/abs/2504.00727",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.136Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.136Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2504.00727",
      "fetched_title": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Building on previous work introducing SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, a novel method is presented for measuring and evaluating how induced personality traits affect task selection processes—specifically planning, scheduling, and decision-making—in agents based on large language models. Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.",
    "abstract_en_extended": "This study, \"Personality-Driven Decision-Making in LLM-Based Autonomous Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Building on previous work introducing SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, a novel method is presented for measuring and evaluating how induced personality traits affect task selection processes—specifically planning, scheduling, and decision-making—in agents based on large language models. It then advances the context by clarifying that Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies. It also details that Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality-Driven Decision-Making in LLM-Based Autonomous Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Basándose en trabajo previo que introdujo SANDMAN, una arquitectura de Agente Engañoso que aprovecha el modelo de personalidad OCEAN de Cinco Factores, se presenta un método novedoso para medir y evaluar cómo los rasgos de personalidad inducidos afectan los procesos de selección de tareas—específicamente planificación, programación y toma de decisiones—en agentes basados en modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Los resultados revelan patrones distintos de selección de tareas alineados con atributos OCEAN inducidos, subrayando la viabilidad de diseñar Agentes Engañosos altamente plausibles para estrategias proactivas de defensa cibernética. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados revelan patrones distintos de selección de tareas alineados con atributos OCEAN inducidos, subrayando la viabilidad de diseñar Agentes Engañosos altamente plausibles para estrategias proactivas de defensa cibernética. También se especifica que Los resultados revelan patrones distintos de selección de tareas alineados con atributos OCEAN inducidos, subrayando la viabilidad de diseñar Agentes Engañosos altamente plausibles para estrategias proactivas de defensa cibernética. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Building on previous work introducing SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, a novel method is presented for measuring and evaluating how induced personality traits affect task selection processes—specifically planning, scheduling, and decision-making—in agents based on large language models.",
        "Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-073",
    "legacy_article_number": 73,
    "title_original": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations",
    "title_canonical": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Ivar Frisch",
      "Mario Giulianelli"
    ],
    "keywords": [
      "Large Language Models",
      "Agent interaction",
      "Personality consistency",
      "Linguistic alignment",
      "Dialogue-based interaction"
    ],
    "source_url": "https://aclanthology.org/2024.personalize-1.9/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.152Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.152Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.personalize-1.9/",
      "fetched_title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "This experimental study seeks to lay groundwork for understanding dialogue-based interaction between large language models by conditioning GPT-3.5 on asymmetric personality profiles to create a population of agents. Agents were administered personality tests and submitted to a collaborative writing task. Findings reveal that different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.",
    "abstract_en_extended": "This study, \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This experimental study seeks to lay groundwork for understanding dialogue-based interaction between large language models by conditioning GPT-3.5 on asymmetric personality profiles to create a population of agents. It then advances the context by clarifying that Agents were administered personality tests and submitted to a collaborative writing task. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Findings reveal that different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction. It also details that Findings reveal that different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este estudio experimental busca establecer la base para entender la interacción basada en diálogo entre modelos de lenguaje de gran escala condicionando GPT-3.5 en perfiles de personalidad asimétricos para crear una población de agentes. Además, contextualiza el aporte al precisar que Los agentes fueron administrados pruebas de personalidad y sometidos a una tarea de escritura colaborativa. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los hallazgos revelan que diferentes perfiles exhiben diferentes grados de consistencia de personalidad y alineación lingüística en la interacción. También se especifica que Los hallazgos revelan que diferentes perfiles exhiben diferentes grados de consistencia de personalidad y alineación lingüística en la interacción. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This experimental study seeks to lay groundwork for understanding dialogue-based interaction between large language models by conditioning GPT-3.5 on asymmetric personality profiles to create a population of agents.",
        "Agents were administered personality tests and submitted to a collaborative writing task.",
        "Findings reveal that different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-074",
    "legacy_article_number": 74,
    "title_original": "Automated LLM Questionnaire for Automatic Psychiatric Assessment",
    "title_canonical": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Gony Rosenman",
      "Talma Hendler",
      "Lior Wolf"
    ],
    "keywords": [
      "Large Language Models",
      "Psychiatric Assessment",
      "Mental Health",
      "Depression (PHQ-8)",
      "PTSD (PCL-C)",
      "Psychological Interviews"
    ],
    "source_url": "https://aclanthology.org/2024.findings-emnlp.23/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.182Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.182Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.23/",
      "fetched_title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A large language model is employed to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The model is prompted to answer questionnaires by impersonating the interviewee. Obtained answers are coded as features used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C) using Random Forest regression. The approach enhances diagnostic accuracy compared to multiple baselines, establishing a novel framework for interpreting psychological interviews.",
    "abstract_en_extended": "This study, \"LLM Questionnaire Completion for Automatic Psychiatric Assessment\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A large language model is employed to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. It then advances the context by clarifying that The model is prompted to answer questionnaires by impersonating the interviewee. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Obtained answers are coded as features used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C) using Random Forest regression. It also details that The approach enhances diagnostic accuracy compared to multiple baselines, establishing a novel framework for interpreting psychological interviews. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"LLM Questionnaire Completion for Automatic Psychiatric Assessment\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se emplea un modelo de lenguaje de gran escala para convertir entrevistas psicológicas no estructuradas en cuestionarios estructurados que abarcan diversos dominios psiquiátricos y de personalidad. Además, contextualiza el aporte al precisar que Se instruye al modelo para responder cuestionarios personificando al entrevistado. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las respuestas obtenidas se codifican como características utilizadas para predecir medidas psiquiátricas estandarizadas de depresión (PHQ-8) y estrés postraumático (PCL-C) mediante regresión Random Forest. También se especifica que El enfoque mejora la precisión diagnóstica en comparación con múltiples líneas base, estableciendo un marco novedoso para interpretar entrevistas psicológicas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A large language model is employed to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains.",
        "The model is prompted to answer questionnaires by impersonating the interviewee.",
        "Obtained answers are coded as features used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C) using Random Forest regression."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-075",
    "legacy_article_number": 75,
    "title_original": "Psychometric Shaping of Personality Modulates Capabilities and Safety in Language Models",
    "title_canonical": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Stephen Fitz",
      "Peter Romero",
      "Steven Basart",
      "Sipeng Chen",
      "Jose Hernandez-Orallo"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Large Language Models",
      "Personality Traits",
      "Model Safety"
    ],
    "source_url": "https://arxiv.org/abs/2509.16332",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.509Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.509Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.16332",
      "fetched_title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models",
      "title_similarity": 0.9091,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "How psychometric personality control grounded in the Big Five framework influences AI behavior in capability and safety benchmarks is investigated. Experiments reveal striking effects: reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy, as well as reduction in general capabilities measured by MMLU. Findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and competence.",
    "abstract_en_extended": "This study, \"Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that How psychometric personality control grounded in the Big Five framework influences AI behavior in capability and safety benchmarks is investigated. It then advances the context by clarifying that Experiments reveal striking effects: reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy, as well as reduction in general capabilities measured by MMLU. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and competence. It also details that Findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and competence. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga cómo el control psicométrico de personalidad fundamentado en el marco de los Cinco Grandes influye en el comportamiento de inteligencia artificial en pruebas de referencia de capacidad y seguridad. Además, contextualiza el aporte al precisar que Los experimentos revelan efectos notables: reducir la responsabilidad conduce a caídas significativas en métricas relevantes de seguridad en pruebas como WMDP, TruthfulQA, ETHICS y Sycophancy, así como reducción en capacidades generales medidas por MMLU. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los hallazgos destacan el moldeo de personalidad como un eje poderoso y poco explorado de control de modelo que interactúa tanto con la seguridad como con la competencia. También se especifica que Los hallazgos destacan el moldeo de personalidad como un eje poderoso y poco explorado de control de modelo que interactúa tanto con la seguridad como con la competencia. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "How psychometric personality control grounded in the Big Five framework influences AI behavior in capability and safety benchmarks is investigated.",
        "Experiments reveal striking effects: reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy, as well as reduction in general capabilities measured by MMLU.",
        "Findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and competence."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-076",
    "legacy_article_number": 76,
    "title_original": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
    "title_canonical": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wenhan Dong",
      "Yuemeng Zhao",
      "Zhen Sun",
      "Yule Liu",
      "Zifan Peng",
      "Jingyi Zheng",
      "Zongmin Zhang",
      "Ziyi Zhang",
      "Jun Wu",
      "Ruiming Wang",
      "Shengmin Xu",
      "Xinyi Huang",
      "Xinlei He"
    ],
    "keywords": [
      "Computers and Society",
      "Computation and Language",
      "Human-Computer Interaction",
      "Machine Learning",
      "Psychological traits",
      "Trustworthy AI"
    ],
    "source_url": "https://arxiv.org/abs/2505.00049",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.527Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.527Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2505.00049",
      "fetched_title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Six key dimensions of applying psychological theories to large language models are systematically covered: assessment tools, model-specific datasets, evaluation metrics (consistency and stability), empirical findings, personality simulation methods, and behavior simulation. The analysis highlights both strengths and limitations of current methods. While some models exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Future directions are proposed for developing interpretable, robust, and generalizable psychological assessment frameworks.",
    "abstract_en_extended": "This study, \"Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Six key dimensions of applying psychological theories to large language models are systematically covered: assessment tools, model-specific datasets, evaluation metrics (consistency and stability), empirical findings, personality simulation methods, and behavior simulation. It then advances the context by clarifying that The analysis highlights both strengths and limitations of current methods. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that While some models exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. It also details that Future directions are proposed for developing interpretable, robust, and generalizable psychological assessment frameworks. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se cubren sistemáticamente seis dimensiones clave de aplicar teorías psicológicas a modelos de lenguaje de gran escala: herramientas de evaluación, conjuntos de datos específicos del modelo, métricas de evaluación (consistencia y estabilidad), hallazgos empíricos, métodos de simulación de personalidad y simulación de comportamiento. Además, contextualiza el aporte al precisar que El análisis destaca tanto las fortalezas como las limitaciones de los métodos actuales. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aunque algunos modelos exhiben patrones de personalidad reproducibles bajo esquemas específicos de instrucciones, permanece una variabilidad significativa entre tareas y configuraciones. También se especifica que Se proponen direcciones futuras para desarrollar marcos de evaluación psicológica interpretables, robustos y generalizables. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Six key dimensions of applying psychological theories to large language models are systematically covered: assessment tools, model-specific datasets, evaluation metrics (consistency and stability), empirical findings, personality simulation methods, and behavior simulation.",
        "The analysis highlights both strengths and limitations of current methods.",
        "While some models exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-077",
    "legacy_article_number": 77,
    "title_original": "Large Language Models Demonstrate Distinctive Personality Profiles",
    "title_canonical": "Large Language Models Demonstrate Distinct Personality Profiles - PMC",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Thomas F Heston",
      "Justin Gillette"
    ],
    "keywords": [
      "AI ethics",
      "AI in mental health",
      "AI psychometrics",
      "Artificial intelligence in medicine",
      "Generative AI",
      "Personality assessment"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12183331/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.532Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.532Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12183331/",
      "fetched_title": "Large Language Models Demonstrate Distinct Personality Profiles - PMC",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "The first psychometric analysis of large language model personality is provided using two validated frameworks: Open Extended Jungian Type Scales (OEJTS) and Big Five Personality Test. Four leading models (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) were evaluated in April 2024. MANOVA revealed statistically significant differences across models in personality traits. Distinct personality profiles are consistently expressed across different models, underscoring the need for formal personality evaluation before clinical deployment.",
    "abstract_en_extended": "This study, \"Large Language Models Demonstrate Distinct Personality Profiles - PMC\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The first psychometric analysis of large language model personality is provided using two validated frameworks: Open Extended Jungian Type Scales (OEJTS) and Big Five Personality Test. It then advances the context by clarifying that Four leading models (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) were evaluated in April 2024. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that MANOVA revealed statistically significant differences across models in personality traits. It also details that Distinct personality profiles are consistently expressed across different models, underscoring the need for formal personality evaluation before clinical deployment. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large Language Models Demonstrate Distinct Personality Profiles - PMC\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se proporciona el primer análisis psicométrico de personalidad de modelos de lenguaje de gran escala utilizando dos marcos validados: Escalas de Tipo Junguiano Extendido Abierto (OEJTS) y Prueba de Personalidad de los Cinco Grandes. Además, contextualiza el aporte al precisar que Cuatro modelos líderes (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) fueron evaluados en abril de 2024. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que MANOVA reveló diferencias estadísticamente significativas entre modelos en rasgos de personalidad. También se especifica que Perfiles de personalidad distintos se expresan consistentemente entre diferentes modelos, subrayando la necesidad de evaluación formal de personalidad antes del despliegue clínico. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The first psychometric analysis of large language model personality is provided using two validated frameworks: Open Extended Jungian Type Scales (OEJTS) and Big Five Personality Test.",
        "Four leading models (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) were evaluated in April 2024.",
        "MANOVA revealed statistically significant differences across models in personality traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-078",
    "legacy_article_number": 78,
    "title_original": "Attitudes Toward AI: Measurement and Associations with Personality",
    "title_canonical": "Attitudes towards AI: measurement and associations with personality - Scientific Reports",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "J.P. Stein",
      "T. Messingschlager",
      "T. Gnambs",
      "F. Hutmacher",
      "M. Appel"
    ],
    "keywords": [
      "Artificial Intelligence attitudes",
      "Big Five personality",
      "Dark Triad",
      "Conspiracy beliefs",
      "ATTARI-12 questionnaire"
    ],
    "source_url": "https://www.nature.com/articles/s41598-024-53335-2?error=cookies_not_supported&code=419b1abe-8d52-4960-babf-819682220aa0",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.543Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.543Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-53335-2?error=cookies_not_supported&code=419b1abe-8d52-4960-babf-819682220aa0",
      "fetched_title": "Attitudes towards AI: measurement and associations with personality - Scientific Reports",
      "title_similarity": 0.6364,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A novel psychologically informed questionnaire (ATTARI-12) is presented capturing attitudes towards AI as a single construct independent of specific contexts. The questionnaire demonstrated good reliability and validity across two studies (N1 = 490; N2 = 150). Personality traits—Big Five, Dark Triad, and conspiracy mentality—were examined as potential predictors. Agreeableness and younger age predict more positive views towards AI technology, whereas susceptibility to conspiracy beliefs is associated with more negative attitudes.",
    "abstract_en_extended": "This study, \"Attitudes towards AI: measurement and associations with personality - Scientific Reports\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel psychologically informed questionnaire (ATTARI-12) is presented capturing attitudes towards AI as a single construct independent of specific contexts. It then advances the context by clarifying that The questionnaire demonstrated good reliability and validity across two studies (N1 = 490; N2 = 150). From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Personality traits—Big Five, Dark Triad, and conspiracy mentality—were examined as potential predictors. It also details that Agreeableness and younger age predict more positive views towards AI technology, whereas susceptibility to conspiracy beliefs is associated with more negative attitudes. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Attitudes towards AI: measurement and associations with personality - Scientific Reports\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta un cuestionario psicológicamente informado novedoso (ATTARI-12) que captura actitudes hacia la inteligencia artificial como un constructo único independiente de contextos específicos. Además, contextualiza el aporte al precisar que El cuestionario demostró buena confiabilidad y validez en dos estudios (N1 = 490; N2 = 150). Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se examinaron rasgos de personalidad—Cinco Grandes, Tríada Oscura y mentalidad conspirativa—como predictores potenciales. También se especifica que La amabilidad y la edad más joven predicen vistas más positivas hacia la tecnología de inteligencia artificial, mientras que la susceptibilidad a creencias conspirativas se asocia con actitudes más negativas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel psychologically informed questionnaire (ATTARI-12) is presented capturing attitudes towards AI as a single construct independent of specific contexts.",
        "The questionnaire demonstrated good reliability and validity across two studies (N1 = 490; N2 = 150).",
        "Personality traits—Big Five, Dark Triad, and conspiracy mentality—were examined as potential predictors."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-079",
    "legacy_article_number": 79,
    "title_original": "Do LLMs Have a Personality? A Psychometric Assessment with Implications for Clinical Medicine and Mental Health AI",
    "title_canonical": "Do Large Language Models Have a Personality? A Psychometric Evaluation with Implications for Clinical Medicine and Mental Health AI",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Thomas F Heston",
      "Justin Gillette"
    ],
    "keywords": [
      "Psychiatry",
      "Clinical Psychology",
      "Mental Health AI",
      "Large Language Models",
      "Personality Assessment",
      "Clinical Medicine"
    ],
    "source_url": "https://www.medrxiv.org/content/10.1101/2025.03.14.25323987v1",
    "source_type": "preprint_other",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.655Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.655Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.medrxiv.org/content/10.1101/2025.03.14.25323987v1",
      "fetched_title": "Do Large Language Models Have a Personality? A Psychometric Evaluation with Implications for Clinical Medicine and Mental Health AI",
      "title_similarity": 0.6842,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Personality profiles of four leading large language models in 2024 were characterized using two validated frameworks: Open Extended Jungian Type Scales and Big Five Personality Test. MANOVA revealed statistically significant differences across models in personality traits. ChatGPT-3.5 was most often classified as ENTJ, Claude 3 Opus as INTJ, while Gemini Advanced and Grok-Regular leaned toward INFJ. Distinct personality profiles are consistently expressed across different models, emphasizing the need for formal personality evaluation before deploying models in clinical workflows.",
    "abstract_en_extended": "This study, \"Do Large Language Models Have a Personality? A Psychometric Evaluation with Implications for Clinical Medicine and Mental Health AI\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality profiles of four leading large language models in 2024 were characterized using two validated frameworks: Open Extended Jungian Type Scales and Big Five Personality Test. It then advances the context by clarifying that MANOVA revealed statistically significant differences across models in personality traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that ChatGPT-3.5 was most often classified as ENTJ, Claude 3 Opus as INTJ, while Gemini Advanced and Grok-Regular leaned toward INFJ. It also details that Distinct personality profiles are consistently expressed across different models, emphasizing the need for formal personality evaluation before deploying models in clinical workflows. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Do Large Language Models Have a Personality? A Psychometric Evaluation with Implications for Clinical Medicine and Mental Health AI\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se caracterizaron perfiles de personalidad de cuatro modelos de lenguaje de gran escala líderes en 2024 utilizando dos marcos validados: Escalas de Tipo Junguiano Extendido Abierto y Prueba de Personalidad de los Cinco Grandes. Además, contextualiza el aporte al precisar que MANOVA reveló diferencias estadísticamente significativas entre modelos en rasgos de personalidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que ChatGPT-3.5 fue clasificado más frecuentemente como ENTJ, Claude 3 Opus como INTJ, mientras que Gemini Advanced y Grok-Regular se inclinaron hacia INFJ. También se especifica que Perfiles de personalidad distintos se expresan consistentemente entre diferentes modelos, enfatizando la necesidad de evaluación formal de personalidad antes de desplegar modelos en flujos de trabajo clínicos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality profiles of four leading large language models in 2024 were characterized using two validated frameworks: Open Extended Jungian Type Scales and Big Five Personality Test.",
        "MANOVA revealed statistically significant differences across models in personality traits.",
        "ChatGPT-3.5 was most often classified as ENTJ, Claude 3 Opus as INTJ, while Gemini Advanced and Grok-Regular leaned toward INFJ."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "preprint_other"
        }
      ]
    }
  },
  {
    "id": "article-080",
    "legacy_article_number": 80,
    "title_original": "Developing and Enhancing Personality Inventories Using Generative AI: Psychometric Properties of a Short HEXACO Scale Developed with ChatGPT",
    "title_canonical": "Developing and Improving Personality Inventories Using Generative Artificial Intelligence: The Psychometric Properties of a Short HEXACO Scale Developed Using ChatGPT 4.0",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Ard J. Barends",
      "Reinout E. de Vries"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Generative AI",
      "HEXACO personality inventory",
      "ChatGPT 4.0",
      "Psychometrics",
      "Survey development",
      "Content validity"
    ],
    "source_url": "https://doi.org/10.1080/00223891.2024.2444454",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:17.973Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:17.973Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1080/00223891.2024.2444454",
      "fetched_title": "Developing and Improving Personality Inventories Using Generative Artificial Intelligence: The Psychometric Properties of a Short HEXACO Scale Developed Using ChatGPT 4.0",
      "title_similarity": 0.6667,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "A 24-item HEXACO personality inventory was generated using ChatGPT 4.0, called ChatGPT HEXACO inventory (CHI). Whether ChatGPT could modify CHI to improve its internal consistency or content validity was investigated. Participants (N = 682) completed Brief HEXACO Inventory (BHI) and HEXACO-60 and were randomly assigned to complete one of three CHI versions. Results showed generally comparable psychometric properties across the three CHI versions and BHI. However, ChatGPT could not improve specific psychometric properties of CHI.",
    "abstract_en_extended": "This study, \"Developing and Improving Personality Inventories Using Generative Artificial Intelligence: The Psychometric Properties of a Short HEXACO Scale Developed Using ChatGPT 4.0\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A 24-item HEXACO personality inventory was generated using ChatGPT 4.0, called ChatGPT HEXACO inventory (CHI). It then advances the context by clarifying that Whether ChatGPT could modify CHI to improve its internal consistency or content validity was investigated. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Participants (N = 682) completed Brief HEXACO Inventory (BHI) and HEXACO-60 and were randomly assigned to complete one of three CHI versions. It also details that Results showed generally comparable psychometric properties across the three CHI versions and BHI. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Developing and Improving Personality Inventories Using Generative Artificial Intelligence: The Psychometric Properties of a Short HEXACO Scale Developed Using ChatGPT 4.0\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se generó un inventario de personalidad HEXACO de 24 ítems utilizando ChatGPT 4.0, denominado inventario HEXACO ChatGPT (CHI). Además, contextualiza el aporte al precisar que Se investigó si ChatGPT podría modificar CHI para mejorar su consistencia interna o validez de contenido. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los participantes (N = 682) completaron el Inventario HEXACO Breve (BHI) y HEXACO-60, y fueron asignados aleatoriamente para completar una de tres versiones de CHI. También se especifica que Los resultados mostraron propiedades psicométricas generalmente comparables entre las tres versiones de CHI y BHI. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A 24-item HEXACO personality inventory was generated using ChatGPT 4.0, called ChatGPT HEXACO inventory (CHI).",
        "Whether ChatGPT could modify CHI to improve its internal consistency or content validity was investigated.",
        "Participants (N = 682) completed Brief HEXACO Inventory (BHI) and HEXACO-60 and were randomly assigned to complete one of three CHI versions."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-081",
    "legacy_article_number": 81,
    "title_original": "Exploring the Impact of Language Switching on Personality Manifestation in LLMs",
    "title_canonical": "Exploring the Impact of Language Switching on Personality Traits in LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jacopo Amidei",
      "Jose Gregorio Ferreira De Sá",
      "Rubén Nieto Luna",
      "Andreas Kaltenbrunner"
    ],
    "keywords": [
      "Large Language Models",
      "Personality Traits",
      "Language Switching",
      "GPT-4o",
      "Eysenck Personality Questionnaire-Revised (EPQR-A)",
      "Cross-language analysis"
    ],
    "source_url": "https://aclanthology.org/2025.coling-main.162/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.130Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.130Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.coling-main.162/",
      "fetched_title": "Exploring the Impact of Language Switching on Personality Traits in LLMs",
      "title_similarity": 0.8333,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "The extent to which large language models align with humans when personality shifts are associated with language changes is investigated. Based on three experiments focusing on GPT-4o and the Eysenck Personality Questionnaire-Revised (EPQR-A), initial results reveal weak yet significant variation in GPT-4o's personality across languages, indicating some variations stem from language-switching effects rather than translation. Further analysis across five English-speaking countries shows GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits.",
    "abstract_en_extended": "This study, \"Exploring the Impact of Language Switching on Personality Traits in LLMs\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The extent to which large language models align with humans when personality shifts are associated with language changes is investigated. It then advances the context by clarifying that Based on three experiments focusing on GPT-4o and the Eysenck Personality Questionnaire-Revised (EPQR-A), initial results reveal weak yet significant variation in GPT-4o's personality across languages, indicating some variations stem from language-switching effects rather than translation. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Further analysis across five English-speaking countries shows GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits. It also details that Further analysis across five English-speaking countries shows GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Exploring the Impact of Language Switching on Personality Traits in LLMs\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga hasta qué punto los modelos de lenguaje de gran escala se alinean con humanos cuando los cambios de personalidad se asocian con cambios de idioma. Además, contextualiza el aporte al precisar que Basándose en tres experimentos enfocados en GPT-4o y el Cuestionario de Personalidad de Eysenck-Revisado (EPQR-A), los resultados iniciales revelan variación débil pero significativa en la personalidad de GPT-4o entre idiomas, indicando que algunas variaciones provienen de efectos de cambio de idioma en lugar de traducción. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Análisis adicional entre cinco países de habla inglesa muestra que GPT-4o, aprovechando estereotipos, refleja rasgos de personalidad específicos de cada país. También se especifica que Análisis adicional entre cinco países de habla inglesa muestra que GPT-4o, aprovechando estereotipos, refleja rasgos de personalidad específicos de cada país. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The extent to which large language models align with humans when personality shifts are associated with language changes is investigated.",
        "Based on three experiments focusing on GPT-4o and the Eysenck Personality Questionnaire-Revised (EPQR-A), initial results reveal weak yet significant variation in GPT-4o's personality across languages, indicating some variations stem from language-switching effects rather than translation.",
        "Further analysis across five English-speaking countries shows GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-082",
    "legacy_article_number": 82,
    "title_original": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
    "title_canonical": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Seungjong Sun",
      "Seo Yeon Baek",
      "Jang Hyun Kim"
    ],
    "keywords": [
      "personality modulation",
      "model merging",
      "personality vectors",
      "Big Five traits",
      "continuous control",
      "multidimensional traits",
      "personalized AI"
    ],
    "source_url": "https://arxiv.org/abs/2509.19727",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.251Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.251Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.19727",
      "fetched_title": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models with human traits such as personality. Previous attempts to induce personality have shown promising results, but struggle to capture the continuous and multidimensional nature of human traits. A novel method for personality modulation via model merging is proposed. Specifically, personality vectors are constructed by subtracting the weights of a pre-trained model from those of the fine-tuned model on a given personality trait. By merging personality vectors, models are enabled to exhibit desired personality traits without additional training.",
    "abstract_en_extended": "This study, \"Personality Vector: Modulating Personality of Large Language Models by Model Merging\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models with human traits such as personality. It then advances the context by clarifying that Previous attempts to induce personality have shown promising results, but struggle to capture the continuous and multidimensional nature of human traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that A novel method for personality modulation via model merging is proposed. It also details that Specifically, personality vectors are constructed by subtracting the weights of a pre-trained model from those of the fine-tuned model on a given personality trait. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality Vector: Modulating Personality of Large Language Models by Model Merging\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Impulsada por la demanda de sistemas de inteligencia artificial personalizados, existe un creciente interés en alinear el comportamiento de los modelos de lenguaje de gran escala con rasgos humanos como la personalidad. Además, contextualiza el aporte al precisar que Los intentos previos de inducir personalidad han mostrado resultados prometedores, pero tienen dificultades para capturar la naturaleza continua y multidimensional de los rasgos humanos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se propone un método novedoso para la modulación de personalidad mediante fusión de modelos. También se especifica que Específicamente, se construyen vectores de personalidad restando los pesos de un modelo preentrenado de aquellos del modelo con ajuste fino en un rasgo de personalidad dado. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models with human traits such as personality.",
        "Previous attempts to induce personality have shown promising results, but struggle to capture the continuous and multidimensional nature of human traits.",
        "A novel method for personality modulation via model merging is proposed."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-083",
    "legacy_article_number": 83,
    "title_original": "Humanoid Artificial Consciousness Designed with LLM Based on Psychoanalysis",
    "title_canonical": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Sang Hun Kim",
      "Jongmin Lee",
      "Dongkyu Park",
      "So Young Lee",
      "Yosep Chong"
    ],
    "keywords": [
      "artificial consciousness",
      "psychoanalysis",
      "MBTI",
      "personality modules",
      "character simulation",
      "human-like cognition",
      "self-awareness"
    ],
    "source_url": "https://arxiv.org/abs/2510.09043",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.265Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.265Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.09043",
      "fetched_title": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory",
      "title_similarity": 0.5333,
      "author_overlap": 0.4,
      "year_match": true
    },
    "abstract_en_original": "A novel approach is proposed to address challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. Three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) were developed based on the principles of psychoanalysis. Additionally, 16 characters with different personalities representing the sixteen MBTI types were designed.",
    "abstract_en_extended": "This study, \"Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel approach is proposed to address challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. It then advances the context by clarifying that Three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) were developed based on the principles of psychoanalysis. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Additionally, 16 characters with different personalities representing the sixteen MBTI types were designed. It also details that Additionally, 16 characters with different personalities representing the sixteen MBTI types were designed. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un enfoque novedoso para abordar desafíos integrando el psicoanálisis y el Indicador de Tipo Myers-Briggs (MBTI) en la construcción de módulos de consciencia y personalidad. Además, contextualiza el aporte al precisar que Se desarrollaron tres consciencias artificiales (autoconciencia, inconsciencia y preconsciencia) basadas en los principios del psicoanálisis. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Adicionalmente, se diseñaron 16 personajes con diferentes personalidades representando los dieciséis tipos MBTI. También se especifica que Adicionalmente, se diseñaron 16 personajes con diferentes personalidades representando los dieciséis tipos MBTI. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel approach is proposed to address challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules.",
        "Three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) were developed based on the principles of psychoanalysis.",
        "Additionally, 16 characters with different personalities representing the sixteen MBTI types were designed."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-085",
    "legacy_article_number": 85,
    "title_original": "PerFairX: Is There a Balance Between Fairness and Personality in LLM Recommendations?",
    "title_canonical": "PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Chandan Kumar Sah"
    ],
    "keywords": [
      "personality-based recommendation",
      "fairness evaluation",
      "OCEAN model",
      "LLM recommender systems",
      "demographic equity",
      "personalization trade-offs",
      "zero-shot learning"
    ],
    "source_url": "https://arxiv.org/abs/2509.08829",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.282Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.282Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.08829",
      "fetched_title": "PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?",
      "title_similarity": 0.7143,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in recommendations generated by large language models, is proposed. Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities.",
    "abstract_en_extended": "This study, \"PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in recommendations generated by large language models, is proposed. It then advances the context by clarifying that Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities. It also details that Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone PerFairX, un marco de evaluación unificado diseñado para cuantificar los compromisos entre personalización y equidad demográfica en recomendaciones generadas por modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Los resultados revelan que las instrucciones conscientes de la personalidad mejoran significativamente la alineación con rasgos individuales pero pueden exacerbar disparidades de equidad. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados revelan que las instrucciones conscientes de la personalidad mejoran significativamente la alineación con rasgos individuales pero pueden exacerbar disparidades de equidad. También se especifica que Los resultados revelan que las instrucciones conscientes de la personalidad mejoran significativamente la alineación con rasgos individuales pero pueden exacerbar disparidades de equidad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in recommendations generated by large language models, is proposed.",
        "Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-086",
    "legacy_article_number": 86,
    "title_original": "Population-Aligned Persona Generation for LLM-based Social Simulation",
    "title_canonical": "Population-Aligned Persona Generation for LLM-based Social Simulation",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Zhengyu Hu",
      "Jianxun Lian",
      "Zheyuan Xiao",
      "Max Xiong",
      "Yuxuan Lei",
      "Tianfu Wang",
      "Kaize Ding",
      "Ziang Xiao",
      "Nicholas Jing Yuan",
      "Xing Xie"
    ],
    "keywords": [
      "population alignment",
      "persona generation",
      "social simulation",
      "Big Five traits",
      "computational social science",
      "importance sampling",
      "bias reduction"
    ],
    "source_url": "https://arxiv.org/abs/2509.10127",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.297Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.297Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.10127",
      "fetched_title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "A systematic framework for synthesizing high-quality, population-aligned persona sets for social simulation driven by large language models is proposed. The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions.",
    "abstract_en_extended": "This study, \"Population-Aligned Persona Generation for LLM-based Social Simulation\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A systematic framework for synthesizing high-quality, population-aligned persona sets for social simulation driven by large language models is proposed. It then advances the context by clarifying that The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions. It also details that The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Population-Aligned Persona Generation for LLM-based Social Simulation\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un marco sistemático para sintetizar conjuntos de personas de alta calidad y alineados con la población para simulación social impulsada por modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que El enfoque aprovecha modelos de lenguaje de gran escala para generar personas narrativas a partir de datos de redes sociales, seguido de evaluación de calidad y muestreo por importancia para lograr alineación global con distribuciones psicométricas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El enfoque aprovecha modelos de lenguaje de gran escala para generar personas narrativas a partir de datos de redes sociales, seguido de evaluación de calidad y muestreo por importancia para lograr alineación global con distribuciones psicométricas. También se especifica que El enfoque aprovecha modelos de lenguaje de gran escala para generar personas narrativas a partir de datos de redes sociales, seguido de evaluación de calidad y muestreo por importancia para lograr alineación global con distribuciones psicométricas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A systematic framework for synthesizing high-quality, population-aligned persona sets for social simulation driven by large language models is proposed.",
        "The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-088",
    "legacy_article_number": 88,
    "title_original": "Exploring the Potential of Large Language Models to Simulate Personality",
    "title_canonical": "Exploring the Potential of Large Language Models to Simulate Personality",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Maria Molchanova",
      "Anna Mikhailova",
      "Anna Korzanova",
      "Lidiia Ostyakova",
      "Alexandra Dolidze"
    ],
    "keywords": [
      "personality simulation",
      "conversational AI",
      "Big Five model",
      "dialogue personalization",
      "personality-related text generation",
      "LLM challenges",
      "trait modeling"
    ],
    "source_url": "https://arxiv.org/abs/2502.08265",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.315Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.315Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.08265",
      "fetched_title": "Exploring the Potential of Large Language Models to Simulate Personality",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "With the advancement of large language models, the focus in Conversational AI has shifted to tackling more complex challenges, such as personalizing dialogue systems. Simulating personal traits according to the Big Five model is addressed. Research demonstrates that generating personality-related texts remains a challenging task.",
    "abstract_en_extended": "This study, \"Exploring the Potential of Large Language Models to Simulate Personality\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that With the advancement of large language models, the focus in Conversational AI has shifted to tackling more complex challenges, such as personalizing dialogue systems. It then advances the context by clarifying that Simulating personal traits according to the Big Five model is addressed. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Research demonstrates that generating personality-related texts remains a challenging task. It also details that Research demonstrates that generating personality-related texts remains a challenging task. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Exploring the Potential of Large Language Models to Simulate Personality\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Con el avance de los modelos de lenguaje de gran escala, el enfoque en la inteligencia artificial conversacional ha cambiado hacia abordar desafíos más complejos, como personalizar sistemas de diálogo. Además, contextualiza el aporte al precisar que Se aborda la simulación de rasgos personales según el modelo de los Cinco Grandes. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La investigación demuestra que generar textos relacionados con personalidad sigue siendo una tarea desafiante. También se especifica que La investigación demuestra que generar textos relacionados con personalidad sigue siendo una tarea desafiante. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "With the advancement of large language models, the focus in Conversational AI has shifted to tackling more complex challenges, such as personalizing dialogue systems.",
        "Simulating personal traits according to the Big Five model is addressed.",
        "Research demonstrates that generating personality-related texts remains a challenging task."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-089",
    "legacy_article_number": 89,
    "title_original": "Rediscovering the Latent Dimensions of Personality with LLMs as Trait Descriptors",
    "title_canonical": "NeurIPS Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Joseph Suh",
      "Suhong Moon",
      "Minwoo Kang",
      "David Chan"
    ],
    "keywords": [
      "personality assessment",
      "Big Five traits",
      "singular value decomposition",
      "latent dimensions",
      "trait descriptors",
      "personality probing",
      "LLM evaluation"
    ],
    "source_url": "https://neurips.cc/virtual/2024/102146",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.328Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.328Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://neurips.cc/virtual/2024/102146",
      "fetched_title": "NeurIPS Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A novel approach that uncovers latent personality dimensions in large language models by applying SVD to log-probabilities of trait-descriptive adjectives is introduced. Models \"rediscover\" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance.",
    "abstract_en_extended": "This study, \"NeurIPS Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel approach that uncovers latent personality dimensions in large language models by applying SVD to log-probabilities of trait-descriptive adjectives is introduced. It then advances the context by clarifying that Models \"rediscover\" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Models \"rediscover\" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance. It also details that Models \"rediscover\" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"NeurIPS Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce un enfoque novedoso que descubre dimensiones latentes de personalidad en modelos de lenguaje de gran escala aplicando SVD (descomposición en valores singulares) a las log-probabilidades de adjetivos descriptivos de rasgos. Además, contextualiza el aporte al precisar que Los modelos \"redescubren\" rasgos de personalidad centrales sin depender de entradas directas de cuestionarios, con los 5 factores principales explicando el 74.3% de la varianza. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los modelos \"redescubren\" rasgos de personalidad centrales sin depender de entradas directas de cuestionarios, con los 5 factores principales explicando el 74.3% de la varianza. También se especifica que Los modelos \"redescubren\" rasgos de personalidad centrales sin depender de entradas directas de cuestionarios, con los 5 factores principales explicando el 74.3% de la varianza. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel approach that uncovers latent personality dimensions in large language models by applying SVD to log-probabilities of trait-descriptive adjectives is introduced.",
        "Models \"rediscover\" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-090",
    "legacy_article_number": 90,
    "title_original": "PICLe: Eliciting Diverse Behaviors from LLMs with Persona In-Context Learning",
    "title_canonical": "ICML Poster PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Hyeong Kyu Choi",
      "Sharon Li"
    ],
    "keywords": [
      "persona elicitation",
      "in-context learning",
      "Bayesian inference",
      "behavioral preferences",
      "personality traits",
      "ICL",
      "persona customization"
    ],
    "source_url": "https://icml.cc/virtual/2024/poster/32764",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:18.478Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:18.478Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://icml.cc/virtual/2024/poster/32764",
      "fetched_title": "ICML Poster PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
      "title_similarity": 0.625,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference, is presented. PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona.",
    "abstract_en_extended": "This study, \"ICML Poster PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference, is presented. It then advances the context by clarifying that PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona. It also details that PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"ICML Poster PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta Persona In-Context Learning (PICLe), un marco novedoso de obtención de persona fundamentado en inferencia bayesiana. Además, contextualiza el aporte al precisar que PICLe introduce un nuevo criterio de selección de ejemplos de aprendizaje en contexto basado en razón de verosimilitud, diseñado para guiar óptimamente al modelo en la obtención de una persona objetivo específica. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que PICLe introduce un nuevo criterio de selección de ejemplos de aprendizaje en contexto basado en razón de verosimilitud, diseñado para guiar óptimamente al modelo en la obtención de una persona objetivo específica. También se especifica que PICLe introduce un nuevo criterio de selección de ejemplos de aprendizaje en contexto basado en razón de verosimilitud, diseñado para guiar óptimamente al modelo en la obtención de una persona objetivo específica. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference, is presented.",
        "PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-091",
    "legacy_article_number": 91,
    "title_original": "Large Language Models as Superpositions of Cultural Perspectives",
    "title_canonical": "Large Language Models as superpositions of cultural perspectives",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Autores anónimos (ICLR 2024)"
    ],
    "keywords": [
      "cultural perspectives",
      "personality stability",
      "perspective shift",
      "context dependency",
      "value expression",
      "psychological assessment",
      "perspective controllability"
    ],
    "source_url": "https://openreview.net/forum?id=1FWDEIGm33",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:19.347Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:19.347Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://openreview.net/forum?id=1FWDEIGm33",
      "fetched_title": "Large Language Models as superpositions of cultural perspectives",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "\"Large language models as a superposition of perspectives\" is proposed: models simulate a multiplicity of behaviors which can be triggered by context. Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect.",
    "abstract_en_extended": "This study, \"Large Language Models as superpositions of cultural perspectives\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that \"Large language models as a superposition of perspectives\" is proposed: models simulate a multiplicity of behaviors which can be triggered by context. It then advances the context by clarifying that Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect. It also details that Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large Language Models as superpositions of cultural perspectives\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone \"modelos de lenguaje de gran escala como una superposición de perspectivas\": los modelos simulan una multiplicidad de comportamientos que pueden ser desencadenados por el contexto. Además, contextualiza el aporte al precisar que Se demuestra que los cambios de contexto resultan en cambios significativos no deseados y difíciles de predecir en los valores expresados, denominados efecto de cambio inesperado de perspectiva. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se demuestra que los cambios de contexto resultan en cambios significativos no deseados y difíciles de predecir en los valores expresados, denominados efecto de cambio inesperado de perspectiva. También se especifica que Se demuestra que los cambios de contexto resultan en cambios significativos no deseados y difíciles de predecir en los valores expresados, denominados efecto de cambio inesperado de perspectiva. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "\"Large language models as a superposition of perspectives\" is proposed: models simulate a multiplicity of behaviors which can be triggered by context.",
        "Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-092",
    "legacy_article_number": 92,
    "title_original": "LLM vs Small Model? LLM-Based Text Augmentation for Personality Detection",
    "title_canonical": "LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model | Proceedings of the AAAI Conference on Artificial Intelligence",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Linmei Hu",
      "Hongyu He",
      "Duokang Wang",
      "Ziwang Zhao",
      "Yingxia Shao",
      "Liqiang Nie"
    ],
    "keywords": [
      "personality detection",
      "text augmentation",
      "knowledge distillation",
      "contrastive learning",
      "psycho-linguistic analysis",
      "social media",
      "Big Five traits"
    ],
    "source_url": "https://ojs.aaai.org/index.php/AAAI/article/view/29782",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:19.526Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:19.526Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://ojs.aaai.org/index.php/AAAI/article/view/29782",
      "fetched_title": "LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model | Proceedings of the AAAI Conference on Artificial Intelligence",
      "title_similarity": 0.4286,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A text augmentation-enhanced personality detection model based on large language models is proposed, which distills knowledge from large language models to enhance the small model for personality detection. Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects.",
    "abstract_en_extended": "This study, \"LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model | Proceedings of the AAAI Conference on Artificial Intelligence\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A text augmentation-enhanced personality detection model based on large language models is proposed, which distills knowledge from large language models to enhance the small model for personality detection. It then advances the context by clarifying that Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects. It also details that Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model | Proceedings of the AAAI Conference on Artificial Intelligence\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un modelo de detección de personalidad mejorado con aumento de texto basado en modelos de lenguaje de gran escala, que destila el conocimiento de modelos de lenguaje de gran escala para mejorar el modelo pequeño para la detección de personalidad. Además, contextualiza el aporte al precisar que Se habilita a los modelos de lenguaje de gran escala para generar análisis de publicaciones desde aspectos semánticos, de sentimiento y lingüísticos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se habilita a los modelos de lenguaje de gran escala para generar análisis de publicaciones desde aspectos semánticos, de sentimiento y lingüísticos. También se especifica que Se habilita a los modelos de lenguaje de gran escala para generar análisis de publicaciones desde aspectos semánticos, de sentimiento y lingüísticos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A text augmentation-enhanced personality detection model based on large language models is proposed, which distills knowledge from large language models to enhance the small model for personality detection.",
        "Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-093",
    "legacy_article_number": 93,
    "title_original": "Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities",
    "title_canonical": "Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities | Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Luke James Klinkert",
      "Silvia Buongiorno",
      "Christopher Clark"
    ],
    "keywords": [
      "personality emulation",
      "NPC behavior",
      "game AI",
      "personality alignment",
      "human-like traits",
      "LLM evaluation",
      "interactive agents"
    ],
    "source_url": "https://ojs.aaai.org/index.php/AIIDE/article/view/31867",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:19.854Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:19.854Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://ojs.aaai.org/index.php/AIIDE/article/view/31867",
      "fetched_title": "Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities | Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment",
      "title_similarity": 0.5,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Results indicate that NPCs can successfully emulate human-like personality traits using large language models. Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters.",
    "abstract_en_extended": "This study, \"Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities | Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Results indicate that NPCs can successfully emulate human-like personality traits using large language models. It then advances the context by clarifying that Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters. It also details that Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities | Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los resultados indican que los personajes no jugadores (NPCs) pueden emular exitosamente rasgos de personalidad similares a los humanos utilizando modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Los modelos de vanguardia lograron hasta 100% de alineación con perfiles de personalidad humanos, demostrando que los modelos de lenguaje de gran escala pueden representar con precisión personalidades humanas deseadas para personajes de juego. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los modelos de vanguardia lograron hasta 100% de alineación con perfiles de personalidad humanos, demostrando que los modelos de lenguaje de gran escala pueden representar con precisión personalidades humanas deseadas para personajes de juego. También se especifica que Los modelos de vanguardia lograron hasta 100% de alineación con perfiles de personalidad humanos, demostrando que los modelos de lenguaje de gran escala pueden representar con precisión personalidades humanas deseadas para personajes de juego. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Results indicate that NPCs can successfully emulate human-like personality traits using large language models.",
        "Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-094",
    "legacy_article_number": 94,
    "title_original": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents",
    "title_canonical": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Xintao Wang",
      "Yunze Xiao",
      "Jen-tse Huang",
      "Siyu Yuan",
      "Rui Xu",
      "Haoran Guo",
      "Quan Tu",
      "Yaying Fei",
      "Ziang Leng",
      "Wei Wang",
      "Jiangjie Chen",
      "Cheng Li",
      "Yanghua Xiao"
    ],
    "keywords": [
      "role-playing agents",
      "personality assessment",
      "psychological scales",
      "character fidelity",
      "LLM evaluation",
      "Big Five personality traits",
      "psychometric testing"
    ],
    "source_url": "https://aclanthology.org/2024.acl-long.102/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.290Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.290Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.acl-long.102/",
      "fetched_title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
      "title_similarity": 0.7273,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "InCharacter, namely Interviewing Character agents for personality tests, is proposed. Experiments cover 32 distinct characters on 14 widely used psychological scales. State-of-the-art role-playing agents exhibit personalities highly aligned with human-perceived personalities of characters, achieving accuracy up to 80.7%.",
    "abstract_en_extended": "This study, \"InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that InCharacter, namely Interviewing Character agents for personality tests, is proposed. It then advances the context by clarifying that Experiments cover 32 distinct characters on 14 widely used psychological scales. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that State-of-the-art role-playing agents exhibit personalities highly aligned with human-perceived personalities of characters, achieving accuracy up to 80.7%. It also details that State-of-the-art role-playing agents exhibit personalities highly aligned with human-perceived personalities of characters, achieving accuracy up to 80.7%. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone InCharacter, es decir, Entrevistar agentes de Personaje para pruebas de personalidad. Además, contextualiza el aporte al precisar que Los experimentos cubren 32 personajes distintos en 14 escalas psicológicas ampliamente utilizadas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los agentes de juego de roles de última generación exhiben personalidades altamente alineadas con las personalidades percibidas por humanos de los personajes, alcanzando una precisión de hasta 80.7%. También se especifica que Los agentes de juego de roles de última generación exhiben personalidades altamente alineadas con las personalidades percibidas por humanos de los personajes, alcanzando una precisión de hasta 80.7%. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "InCharacter, namely Interviewing Character agents for personality tests, is proposed.",
        "Experiments cover 32 distinct characters on 14 widely used psychological scales.",
        "State-of-the-art role-playing agents exhibit personalities highly aligned with human-perceived personalities of characters, achieving accuracy up to 80.7%."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-095",
    "legacy_article_number": 95,
    "title_original": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games",
    "title_canonical": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Qisen Yang",
      "Zekun Wang",
      "Honghui Chen",
      "Shenzhi Wang",
      "Yifan Pu",
      "Xin Gao",
      "Wenhao Huang",
      "Shiji Song",
      "Gao Huang"
    ],
    "keywords": [
      "psychological measurement",
      "LLM agents",
      "interactive fiction games",
      "gamification",
      "personality traits",
      "psychometric evaluation",
      "mental health assessment"
    ],
    "source_url": "https://aclanthology.org/2024.acl-long.779/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.623Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.623Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.acl-long.779/",
      "fetched_title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "PsychoGAT is proposed to achieve generic gamification of psychological assessment. Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games.",
    "abstract_en_extended": "This study, \"PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that PsychoGAT is proposed to achieve generic gamification of psychological assessment. It then advances the context by clarifying that Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games. It also details that Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone PsychoGAT para lograr una gamificación genérica de la evaluación psicológica. Además, contextualiza el aporte al precisar que Los modelos de lenguaje de gran escala funcionan tanto como psicólogos expertos como diseñadores de juegos innovadores, transformando cualquier escala estandarizada en juegos de ficción interactiva personalizados y atractivos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los modelos de lenguaje de gran escala funcionan tanto como psicólogos expertos como diseñadores de juegos innovadores, transformando cualquier escala estandarizada en juegos de ficción interactiva personalizados y atractivos. También se especifica que Los modelos de lenguaje de gran escala funcionan tanto como psicólogos expertos como diseñadores de juegos innovadores, transformando cualquier escala estandarizada en juegos de ficción interactiva personalizados y atractivos. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "PsychoGAT is proposed to achieve generic gamification of psychological assessment.",
        "Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-096",
    "legacy_article_number": 96,
    "title_original": "Investigating Personality Consistency in Quantized Role-Playing Dialogue Agents",
    "title_canonical": "Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yixiao Wang",
      "Homa Fashandi",
      "Kevin Ferreira"
    ],
    "keywords": [
      "model quantization",
      "role-playing agents",
      "personality consistency",
      "Big Five model",
      "edge computing",
      "multi-turn dialogue",
      "conversational AI"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-industry.19/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.636Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.636Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-industry.19/",
      "fetched_title": "Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents",
      "title_similarity": 0.9,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Personality consistency in quantized large language models for edge device role-playing scenarios is explored. A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits.",
    "abstract_en_extended": "This study, \"Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality consistency in quantized large language models for edge device role-playing scenarios is explored. It then advances the context by clarifying that A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits. It also details that A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora la consistencia de personalidad en modelos de lenguaje de gran escala cuantizados para escenarios de juego de roles en dispositivos de borde. Además, contextualiza el aporte al precisar que Se propone un método no paramétrico denominado Think2 para abordar la inconsistencia de personalidad, demostrando efectividad en mantener rasgos de personalidad consistentes. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se propone un método no paramétrico denominado Think2 para abordar la inconsistencia de personalidad, demostrando efectividad en mantener rasgos de personalidad consistentes. También se especifica que Se propone un método no paramétrico denominado Think2 para abordar la inconsistencia de personalidad, demostrando efectividad en mantener rasgos de personalidad consistentes. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality consistency in quantized large language models for edge device role-playing scenarios is explored.",
        "A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-097",
    "legacy_article_number": 97,
    "title_original": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
    "title_canonical": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Zhengyuan Liu",
      "Stella Xin Yin",
      "Geyu Lin",
      "Nancy F. Chen"
    ],
    "keywords": [
      "intelligent tutoring systems",
      "student simulation",
      "personality traits",
      "educational technology",
      "language learning",
      "conversational AI",
      "personalized learning"
    ],
    "source_url": "https://aclanthology.org/2024.emnlp-main.37/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.970Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.970Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.emnlp-main.37/",
      "fetched_title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios.",
    "abstract_en_extended": "This study, \"Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios. It then advances the context by clarifying that A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios. It also details that A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un marco para construir perfiles de diferentes grupos de estudiantes refinando e integrando aspectos tanto cognitivos como no cognitivos, aprovechando modelos de lenguaje de gran escala para simulación de estudiantes consciente de personalidad en escenarios de aprendizaje de idiomas. Además, contextualiza el aporte al precisar que Se propone un marco para construir perfiles de diferentes grupos de estudiantes refinando e integrando aspectos tanto cognitivos como no cognitivos, aprovechando modelos de lenguaje de gran escala para simulación de estudiantes consciente de personalidad en escenarios de aprendizaje de idiomas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se propone un marco para construir perfiles de diferentes grupos de estudiantes refinando e integrando aspectos tanto cognitivos como no cognitivos, aprovechando modelos de lenguaje de gran escala para simulación de estudiantes consciente de personalidad en escenarios de aprendizaje de idiomas. También se especifica que Se propone un marco para construir perfiles de diferentes grupos de estudiantes refinando e integrando aspectos tanto cognitivos como no cognitivos, aprovechando modelos de lenguaje de gran escala para simulación de estudiantes consciente de personalidad en escenarios de aprendizaje de idiomas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios.",
        "Primary contribution extracted from source abstract. (2)",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-098",
    "legacy_article_number": 98,
    "title_original": "PersonaLLM: Investigating the Ability of LLMs to Express Personality Traits",
    "title_canonical": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Hang Jiang",
      "Xiajie Zhang",
      "Xubo Cao",
      "Cynthia Breazeal",
      "Deb Roy",
      "Jad Kabbara"
    ],
    "keywords": [
      "LLM personas",
      "Big Five personality model",
      "personality expression",
      "text generation",
      "psycholinguistic patterns",
      "human evaluation",
      "personalized chatbots"
    ],
    "source_url": "https://aclanthology.org/2024.findings-naacl.229/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:20.988Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:20.988Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-naacl.229/",
      "fetched_title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits",
      "title_similarity": 0.6923,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Whether large language models can generate content that aligns with assigned personality profiles is investigated. Results show that model personas' self-reported BFI scores are consistent with designated personality types. Human evaluation demonstrates that humans can perceive personality traits with accuracy up to 80%.",
    "abstract_en_extended": "This study, \"PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Whether large language models can generate content that aligns with assigned personality profiles is investigated. It then advances the context by clarifying that Results show that model personas' self-reported BFI scores are consistent with designated personality types. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Human evaluation demonstrates that humans can perceive personality traits with accuracy up to 80%. It also details that Human evaluation demonstrates that humans can perceive personality traits with accuracy up to 80%. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga si los modelos de lenguaje de gran escala pueden generar contenido que se alinee con perfiles de personalidad asignados. Además, contextualiza el aporte al precisar que Los resultados muestran que las puntuaciones BFI autorreportadas de las personas del modelo son consistentes con los tipos de personalidad designados. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La evaluación humana demuestra que los humanos pueden percibir rasgos de personalidad con una precisión de hasta 80%. También se especifica que La evaluación humana demuestra que los humanos pueden percibir rasgos de personalidad con una precisión de hasta 80%. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Whether large language models can generate content that aligns with assigned personality profiles is investigated.",
        "Results show that model personas' self-reported BFI scores are consistent with designated personality types.",
        "Human evaluation demonstrates that humans can perceive personality traits with accuracy up to 80%."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-099",
    "legacy_article_number": 99,
    "title_original": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using LLMs",
    "title_canonical": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Ji-Eun Han",
      "Jun-Seok Koh",
      "Hyeon-Tae Seo",
      "Du-Seong Chang",
      "Kyung-Ah Sohn"
    ],
    "keywords": [
      "synthetic dialogue generation",
      "personality-based dialogue",
      "Big Five extraversion",
      "multilingual NLP",
      "Korean language",
      "conversational AI",
      "data augmentation"
    ],
    "source_url": "https://aclanthology.org/2024.lrec-main.1166/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.319Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.319Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.lrec-main.1166/",
      "fetched_title": "PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models",
      "title_similarity": 0.6364,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A novel end-to-end personality-based synthetic dialogue data generation pipeline is presented. PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model.",
    "abstract_en_extended": "This study, \"PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel end-to-end personality-based synthetic dialogue data generation pipeline is presented. It then advances the context by clarifying that PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model. It also details that PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta una novedosa secuencia de generación de datos de diálogo sintético basado en personalidad de extremo a extremo. Además, contextualiza el aporte al precisar que Se introduce PSYDIAL, el primer conjunto de datos de diálogo coreano enfocado en diálogos basados en personalidad, con enfoque en la dimensión de Extraversión del modelo de personalidad de los Cinco Grandes. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se introduce PSYDIAL, el primer conjunto de datos de diálogo coreano enfocado en diálogos basados en personalidad, con enfoque en la dimensión de Extraversión del modelo de personalidad de los Cinco Grandes. También se especifica que Se introduce PSYDIAL, el primer conjunto de datos de diálogo coreano enfocado en diálogos basados en personalidad, con enfoque en la dimensión de Extraversión del modelo de personalidad de los Cinco Grandes. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel end-to-end personality-based synthetic dialogue data generation pipeline is presented.",
        "PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-100",
    "legacy_article_number": 100,
    "title_original": "Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits",
    "title_canonical": "Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits & Gender Analysis",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Marina Tiuleneva",
      "Vadim A. Porvatov",
      "Carlo Strapparava"
    ],
    "keywords": [
      "Big Five personality traits",
      "character analysis",
      "psycholinguistics",
      "dramatic dialogue",
      "dataset creation",
      "fictional characters",
      "computational personality analysis"
    ],
    "source_url": "https://aclanthology.org/2024.cogalex-1.13/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.339Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.339Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.cogalex-1.13/",
      "fetched_title": "Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits & Gender Analysis",
      "title_similarity": 0.75,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A novel textual dataset comprising fictional characters' lines with annotations based on gender and Big-Five personality traits is introduced. Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively.",
    "abstract_en_extended": "This study, \"Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits & Gender Analysis\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A novel textual dataset comprising fictional characters' lines with annotations based on gender and Big-Five personality traits is introduced. It then advances the context by clarifying that Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively. It also details that Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits & Gender Analysis\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce un novedoso conjunto de datos textual que comprende líneas de personajes ficticios con anotaciones basadas en género y rasgos de personalidad de los Cinco Grandes. Además, contextualiza el aporte al precisar que Los resultados indican que las personas imaginadas reflejan la mayoría de las categorías lingüísticas observadas en personas reales mientras las demuestran de manera más expresiva. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados indican que las personas imaginadas reflejan la mayoría de las categorías lingüísticas observadas en personas reales mientras las demuestran de manera más expresiva. También se especifica que Los resultados indican que las personas imaginadas reflejan la mayoría de las categorías lingüísticas observadas en personas reales mientras las demuestran de manera más expresiva. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A novel textual dataset comprising fictional characters' lines with annotations based on gender and Big-Five personality traits is introduced.",
        "Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-101",
    "legacy_article_number": 101,
    "title_original": "Is Persona Enough for Personality? Using ChatGPT to Reconstruct Latent Personality",
    "title_canonical": "Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yongyi Ji",
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "keywords": [
      "HEXACO personality framework",
      "personality reconstruction",
      "persona modeling",
      "socio-demographic factors",
      "personality consistency",
      "latent dimensions",
      "agent simulation"
    ],
    "source_url": "https://arxiv.org/abs/2406.12216",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.668Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.668Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.12216",
      "fetched_title": "Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Capabilities of large language models in reconstructing complex cognitive attributes based on simple descriptions are explored. Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined.",
    "abstract_en_extended": "This study, \"Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Capabilities of large language models in reconstructing complex cognitive attributes based on simple descriptions are explored. It then advances the context by clarifying that Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined. It also details that Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se exploran las capacidades de los modelos de lenguaje de gran escala para reconstruir atributos cognitivos complejos basados en descripciones simples. Además, contextualiza el aporte al precisar que Utilizando el marco de personalidad HEXACO, se examina la consistencia de los modelos en recuperar y predecir dimensiones de personalidad subyacentes a partir de descripciones simples. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Utilizando el marco de personalidad HEXACO, se examina la consistencia de los modelos en recuperar y predecir dimensiones de personalidad subyacentes a partir de descripciones simples. También se especifica que Utilizando el marco de personalidad HEXACO, se examina la consistencia de los modelos en recuperar y predecir dimensiones de personalidad subyacentes a partir de descripciones simples. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Capabilities of large language models in reconstructing complex cognitive attributes based on simple descriptions are explored.",
        "Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-102",
    "legacy_article_number": 102,
    "title_original": "A Survey of Personality, Persona, and Profile in Conversational Agents",
    "title_canonical": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Richard Sutcliffe"
    ],
    "keywords": [
      "conversational agents",
      "chatbots",
      "personality models",
      "Big Five",
      "persona",
      "dialogue systems",
      "neural networks"
    ],
    "source_url": "https://arxiv.org/abs/2401.00609",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.683Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.683Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2401.00609",
      "fetched_title": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
      "title_similarity": 0.9,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "A comprehensive review of personality in neural conversational agents is presented. Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed.",
    "abstract_en_extended": "This study, \"A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A comprehensive review of personality in neural conversational agents is presented. It then advances the context by clarifying that Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed. It also details that Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se presenta una revisión integral de la personalidad en agentes conversacionales neurales. Además, contextualiza el aporte al precisar que Se definen Personalidad, Persona y Perfil, se explican todos los esquemas de personalidad utilizados en agentes conversacionales, se describen 21 conjuntos de datos, y se revisan modelos y métodos recientes. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se definen Personalidad, Persona y Perfil, se explican todos los esquemas de personalidad utilizados en agentes conversacionales, se describen 21 conjuntos de datos, y se revisan modelos y métodos recientes. También se especifica que Se definen Personalidad, Persona y Perfil, se explican todos los esquemas de personalidad utilizados en agentes conversacionales, se describen 21 conjuntos de datos, y se revisan modelos y métodos recientes. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A comprehensive review of personality in neural conversational agents is presented.",
        "Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-103",
    "legacy_article_number": 103,
    "title_original": "Character-LLM: A Trainable Agent for Role-Playing",
    "title_canonical": "Character-LLM: A Trainable Agent for Role-Playing",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yunfan Shao",
      "Linyang Li",
      "Junqi Dai",
      "Xipeng Qiu"
    ],
    "keywords": [
      "role-playing agents",
      "character simulation",
      "LLM agents",
      "experience reconstruction",
      "personality embodiment",
      "interactive characters",
      "human simulacra"
    ],
    "source_url": "https://arxiv.org/abs/2310.10158",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.689Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.689Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2310.10158",
      "fetched_title": "Character-LLM: A Trainable Agent for Role-Playing",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Character-LLM is introduced to teach large language models to act as specific people such as Beethoven, Queen Cleopatra, or Julius Caesar. The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra.",
    "abstract_en_extended": "This study, \"Character-LLM: A Trainable Agent for Role-Playing\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Character-LLM is introduced to teach large language models to act as specific people such as Beethoven, Queen Cleopatra, or Julius Caesar. It then advances the context by clarifying that The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra. It also details that The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Character-LLM: A Trainable Agent for Role-Playing\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce Character-LLM para enseñar a los modelos de lenguaje de gran escala a actuar como personas específicas tales como Beethoven, la Reina Cleopatra o Julio César. Además, contextualiza el aporte al precisar que El método se enfoca en editar perfiles como experiencias de un cierto personaje y entrenar modelos para ser simulacros personales. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El método se enfoca en editar perfiles como experiencias de un cierto personaje y entrenar modelos para ser simulacros personales. También se especifica que El método se enfoca en editar perfiles como experiencias de un cierto personaje y entrenar modelos para ser simulacros personales. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Character-LLM is introduced to teach large language models to act as specific people such as Beethoven, Queen Cleopatra, or Julius Caesar.",
        "The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-104",
    "legacy_article_number": 104,
    "title_original": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
    "title_canonical": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Jiangjie Chen",
      "Xintao Wang",
      "Rui Xu",
      "Siyu Yuan",
      "Yikai Zhang",
      "Wei Shi",
      "Jian Xie",
      "Shuang Li",
      "Ruihan Yang",
      "Tinghui Zhu",
      "Aili Chen",
      "Nianqi Li",
      "Lida Chen",
      "Caiyu Hu",
      "Siye Wu",
      "Scott Ren",
      "Ziquan Fu",
      "Yanghua Xiao"
    ],
    "keywords": [
      "role-playing agents",
      "persona modeling",
      "personalization",
      "character simulation",
      "interactive systems",
      "LLM applications",
      "human-likeness"
    ],
    "source_url": "https://arxiv.org/abs/2404.18231",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.697Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.697Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2404.18231",
      "fetched_title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
      "title_similarity": 1,
      "author_overlap": 0.1667,
      "year_match": true
    },
    "abstract_en_original": "A comprehensive survey of Role-Playing Language Agents (RPLAs), specialized AI systems designed to simulate assigned personas, is conducted. Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation.",
    "abstract_en_extended": "This study, \"From Persona to Personalization: A Survey on Role-Playing Language Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that A comprehensive survey of Role-Playing Language Agents (RPLAs), specialized AI systems designed to simulate assigned personas, is conducted. It then advances the context by clarifying that Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation. It also details that Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"From Persona to Personalization: A Survey on Role-Playing Language Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se conduce una encuesta integral de Agentes de Lenguaje de Juego de Roles (RPLAs), sistemas de inteligencia artificial especializados diseñados para simular personas asignadas. Además, contextualiza el aporte al precisar que Las personas se categorizan en tipos Demográfico, de Personaje e Individualizado, cubriendo metodologías, obtención de datos, construcción y evaluación. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las personas se categorizan en tipos Demográfico, de Personaje e Individualizado, cubriendo metodologías, obtención de datos, construcción y evaluación. También se especifica que Las personas se categorizan en tipos Demográfico, de Personaje e Individualizado, cubriendo metodologías, obtención de datos, construcción y evaluación. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "A comprehensive survey of Role-Playing Language Agents (RPLAs), specialized AI systems designed to simulate assigned personas, is conducted.",
        "Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-105",
    "legacy_article_number": 105,
    "title_original": "Identifying Cooperative Personalities in Multi-agent Contexts",
    "title_canonical": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Kenneth J. K. Ong",
      "Lye Jia Jun",
      "Hieu Minh \"Jord\" Nguyen",
      "Seong Hah Cho",
      "Natalia Pérez-Campanero Antolín"
    ],
    "keywords": [
      "multi-agent systems",
      "personality traits",
      "cooperation dynamics",
      "Iterated Prisoner's Dilemma",
      "Big Five traits",
      "representation engineering",
      "agent coordination"
    ],
    "source_url": "https://arxiv.org/abs/2503.12722",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.704Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.704Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.12722",
      "fetched_title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
      "title_similarity": 0.5385,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "How personality traits influence cooperation in large language models is explored using representation engineering to steer Big Five traits. Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation.",
    "abstract_en_extended": "This study, \"Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that How personality traits influence cooperation in large language models is explored using representation engineering to steer Big Five traits. It then advances the context by clarifying that Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation. It also details that Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora cómo los rasgos de personalidad influyen en la cooperación en modelos de lenguaje de gran escala utilizando ingeniería de representación para dirigir rasgos de los Cinco Grandes. Además, contextualiza el aporte al precisar que Los resultados muestran que mayor Amabilidad y Responsabilidad mejoran la cooperación pero aumentan la susceptibilidad a la explotación. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los resultados muestran que mayor Amabilidad y Responsabilidad mejoran la cooperación pero aumentan la susceptibilidad a la explotación. También se especifica que Los resultados muestran que mayor Amabilidad y Responsabilidad mejoran la cooperación pero aumentan la susceptibilidad a la explotación. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "How personality traits influence cooperation in large language models is explored using representation engineering to steer Big Five traits.",
        "Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-106",
    "legacy_article_number": 106,
    "title_original": "The Impact of Big Five Personality Traits on AI Agent Decision-Making",
    "title_canonical": "The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Mingjun Ren",
      "Wentao Xu"
    ],
    "keywords": [
      "Big Five personality",
      "AI agent decision-making",
      "social simulation",
      "multi-agent framework",
      "public spaces",
      "AgentVerse",
      "behavioral modeling"
    ],
    "source_url": "https://arxiv.org/abs/2503.15497",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.723Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.723Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.15497",
      "fetched_title": "The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study",
      "title_similarity": 0.6667,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "How Big Five personality traits of AI agents affect their decision generation in public open environments is investigated. The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework.",
    "abstract_en_extended": "This study, \"The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that How Big Five personality traits of AI agents affect their decision generation in public open environments is investigated. It then advances the context by clarifying that The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework. It also details that The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga cómo los rasgos de personalidad de los Cinco Grandes de los agentes de inteligencia artificial afectan su generación de decisiones en ambientes públicos abiertos. Además, contextualiza el aporte al precisar que La simulación se realizó en un entorno de aula universitaria utilizando GPT-3.5-turbo con el marco AgentVerse. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que La simulación se realizó en un entorno de aula universitaria utilizando GPT-3.5-turbo con el marco AgentVerse. También se especifica que La simulación se realizó en un entorno de aula universitaria utilizando GPT-3.5-turbo con el marco AgentVerse. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "How Big Five personality traits of AI agents affect their decision generation in public open environments is investigated.",
        "The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-107",
    "legacy_article_number": 107,
    "title_original": "Signs of Consciousness in AI: Can GPT-3 Tell How Smart It Really Is?",
    "title_canonical": "Signs of consciousness in AI: Can GPT-3 tell how smart it really is? - Humanities and Social Sciences Communications",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Bojana Bojic",
      "Marina Jovanovic",
      "Bojana M. Dinic",
      "Ljubisa Bojic"
    ],
    "keywords": [
      "artificial intelligence",
      "consciousness",
      "GPT-3",
      "cognitive intelligence",
      "emotional intelligence",
      "self-awareness",
      "machine consciousness"
    ],
    "source_url": "https://www.nature.com/articles/s41599-024-04154-3?error=cookies_not_supported&code=2f482550-dd91-4578-a72a-b2da20700f7e",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.740Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.740Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41599-024-04154-3?error=cookies_not_supported&code=2f482550-dd91-4578-a72a-b2da20700f7e",
      "fetched_title": "Signs of consciousness in AI: Can GPT-3 tell how smart it really is? - Humanities and Social Sciences Communications",
      "title_similarity": 0.7222,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Objective and self-assessment tests of cognitive and emotional intelligence were administered to GPT-3. Results revealed GPT-3 outperformed average humans on cognitive intelligence tests, but its logical reasoning and emotional intelligence matched average human performance. GPT-3's self-assessments did not always align with objective performance.",
    "abstract_en_extended": "This study, \"Signs of consciousness in AI: Can GPT-3 tell how smart it really is? - Humanities and Social Sciences Communications\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Objective and self-assessment tests of cognitive and emotional intelligence were administered to GPT-3. It then advances the context by clarifying that Results revealed GPT-3 outperformed average humans on cognitive intelligence tests, but its logical reasoning and emotional intelligence matched average human performance. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that GPT-3's self-assessments did not always align with objective performance. It also details that GPT-3's self-assessments did not always align with objective performance. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Signs of consciousness in AI: Can GPT-3 tell how smart it really is? - Humanities and Social Sciences Communications\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se administraron pruebas objetivas y de autoevaluación de inteligencia cognitiva y emocional a GPT-3. Además, contextualiza el aporte al precisar que Los resultados revelaron que GPT-3 superó a los humanos promedio en pruebas de inteligencia cognitiva, pero su razonamiento lógico e inteligencia emocional coincidieron con el desempeño humano promedio. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las autoevaluaciones de GPT-3 no siempre se alinearon con el desempeño objetivo. También se especifica que Las autoevaluaciones de GPT-3 no siempre se alinearon con el desempeño objetivo. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Objective and self-assessment tests of cognitive and emotional intelligence were administered to GPT-3.",
        "Results revealed GPT-3 outperformed average humans on cognitive intelligence tests, but its logical reasoning and emotional intelligence matched average human performance.",
        "GPT-3's self-assessments did not always align with objective performance."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-108",
    "legacy_article_number": 108,
    "title_original": "An Evolutionary Model of Personality Traits Related to Cooperative Behavior Using LLM",
    "title_canonical": "An evolutionary model of personality traits related to cooperative behavior using a large language model - Scientific Reports",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Reiji Suzuki",
      "Takaya Arita"
    ],
    "keywords": [
      "evolutionary computation",
      "personality traits",
      "cooperative behavior",
      "large language models",
      "game theory",
      "evolutionary dynamics",
      "behavioral traits"
    ],
    "source_url": "https://www.nature.com/articles/s41598-024-55903-y?error=cookies_not_supported&code=dc5a7b27-8588-4c0c-8c0d-3d29a0462d88",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:21.757Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:21.757Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s41598-024-55903-y?error=cookies_not_supported&code=dc5a7b27-8588-4c0c-8c0d-3d29a0462d88",
      "fetched_title": "An evolutionary model of personality traits related to cooperative behavior using a large language model - Scientific Reports",
      "title_similarity": 0.6875,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "An evolutionary model of personality traits related to cooperative behavior using large language models is proposed. Linguistic descriptions of personality traits are used as genes. The model exhibits evolution of cooperative behavior based on diverse and higher-order representation of personality traits.",
    "abstract_en_extended": "This study, \"An evolutionary model of personality traits related to cooperative behavior using a large language model - Scientific Reports\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that An evolutionary model of personality traits related to cooperative behavior using large language models is proposed. It then advances the context by clarifying that Linguistic descriptions of personality traits are used as genes. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The model exhibits evolution of cooperative behavior based on diverse and higher-order representation of personality traits. It also details that The model exhibits evolution of cooperative behavior based on diverse and higher-order representation of personality traits. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"An evolutionary model of personality traits related to cooperative behavior using a large language model - Scientific Reports\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone un modelo evolutivo de rasgos de personalidad relacionados con comportamiento cooperativo utilizando modelos de lenguaje de gran escala. Además, contextualiza el aporte al precisar que Las descripciones lingüísticas de rasgos de personalidad se utilizan como genes. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El modelo exhibe evolución de comportamiento cooperativo basado en representación diversa y de orden superior de rasgos de personalidad. También se especifica que El modelo exhibe evolución de comportamiento cooperativo basado en representación diversa y de orden superior de rasgos de personalidad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "An evolutionary model of personality traits related to cooperative behavior using large language models is proposed.",
        "Linguistic descriptions of personality traits are used as genes.",
        "The model exhibits evolution of cooperative behavior based on diverse and higher-order representation of personality traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-109",
    "legacy_article_number": 109,
    "title_original": "Cultural Bias and Cultural Alignment of Large Language Models",
    "title_canonical": "Cultural bias and cultural alignment of large language models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Yan Tao",
      "Olga Viberg",
      "Ryan S Baker",
      "René F Kizilcec"
    ],
    "keywords": [
      "cultural bias",
      "cultural alignment",
      "large language models",
      "cross-cultural values",
      "World Values Survey",
      "cultural prompting",
      "AI ethics"
    ],
    "source_url": "https://doi.org/10.1093/pnasnexus/pgae346",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.258Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.258Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1093/pnasnexus/pgae346",
      "fetched_title": "Cultural bias and cultural alignment of large language models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "All models exhibit cultural values resembling English-speaking and Protestant European countries. Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories.",
    "abstract_en_extended": "This study, \"Cultural bias and cultural alignment of large language models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that All models exhibit cultural values resembling English-speaking and Protestant European countries. It then advances the context by clarifying that Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories. It also details that Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Cultural bias and cultural alignment of large language models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Todos los modelos exhiben valores culturales que se asemejan a países anglófonos y europeos protestantes. Además, contextualiza el aporte al precisar que Se prueba el uso de instrucciones culturales como estrategia de control para aumentar la alineación cultural, lo cual mejora la alineación para 71-81% de países/territorios. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se prueba el uso de instrucciones culturales como estrategia de control para aumentar la alineación cultural, lo cual mejora la alineación para 71-81% de países/territorios. También se especifica que Se prueba el uso de instrucciones culturales como estrategia de control para aumentar la alineación cultural, lo cual mejora la alineación para 71-81% de países/territorios. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "All models exhibit cultural values resembling English-speaking and Protestant European countries.",
        "Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-110",
    "legacy_article_number": 110,
    "title_original": "Artificial Intelligence, Human Cognition, and Conscious Supremacy",
    "title_canonical": "Frontiers | Artificial intelligence, human cognition, and conscious supremacy",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Kenichiro Mogi"
    ],
    "keywords": [
      "consciousness",
      "artificial intelligence",
      "cognitive science",
      "conscious supremacy",
      "computational significance",
      "philosophy of mind",
      "neural correlates"
    ],
    "source_url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1364714/full",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.367Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.367Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1364714/full",
      "fetched_title": "Frontiers | Artificial intelligence, human cognition, and conscious supremacy",
      "title_similarity": 0.875,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "Salient ideas about computational significance of human conscious processes are reviewed, and cognitive domains potentially unique to consciousness are identified: flexible attention modulation, robust handling of new contexts, choice and decision making. Conscious supremacy is proposed as a concept analogous to quantum supremacy.",
    "abstract_en_extended": "This study, \"Frontiers | Artificial intelligence, human cognition, and conscious supremacy\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Salient ideas about computational significance of human conscious processes are reviewed, and cognitive domains potentially unique to consciousness are identified: flexible attention modulation, robust handling of new contexts, choice and decision making. It then advances the context by clarifying that Conscious supremacy is proposed as a concept analogous to quantum supremacy. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Conscious supremacy is proposed as a concept analogous to quantum supremacy. It also details that Conscious supremacy is proposed as a concept analogous to quantum supremacy. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Frontiers | Artificial intelligence, human cognition, and conscious supremacy\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se revisan ideas destacadas sobre la significancia computacional de los procesos conscientes humanos, y se identifican dominios cognitivos potencialmente únicos a la consciencia: modulación flexible de la atención, manejo robusto de nuevos contextos, elección y toma de decisiones. Además, contextualiza el aporte al precisar que Se propone la supremacía consciente como un concepto análogo a la supremacía cuántica. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se propone la supremacía consciente como un concepto análogo a la supremacía cuántica. También se especifica que Se propone la supremacía consciente como un concepto análogo a la supremacía cuántica. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Salient ideas about computational significance of human conscious processes are reviewed, and cognitive domains potentially unique to consciousness are identified: flexible attention modulation, robust handling of new contexts, choice and decision making.",
        "Conscious supremacy is proposed as a concept analogous to quantum supremacy.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-111",
    "legacy_article_number": 111,
    "title_original": "Designing Personality-Adaptive Conversational Agents for Mental Health Care",
    "title_canonical": "Designing Personality-Adaptive Conversational Agents for Mental Health Care - PMC",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Equipo de investigación en informática de salud mental"
    ],
    "keywords": [
      "personality-adaptive conversational agents",
      "mental health care",
      "therapeutic chatbots",
      "user personalization",
      "patient-centered design",
      "Big Five personality",
      "clinical applications"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8889396/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.778Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.778Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8889396/",
      "fetched_title": "Designing Personality-Adaptive Conversational Agents for Mental Health Care - PMC",
      "title_similarity": 0.9,
      "author_overlap": 0,
      "year_match": false
    },
    "abstract_en_original": "The concept of a personality-adaptive conversational agent (PACA) that can dynamically adjust its personality traits to better align with individual patient needs in therapeutic contexts is proposed. Based on established personality models and advances in natural language processing.",
    "abstract_en_extended": "This study, \"Designing Personality-Adaptive Conversational Agents for Mental Health Care - PMC\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The concept of a personality-adaptive conversational agent (PACA) that can dynamically adjust its personality traits to better align with individual patient needs in therapeutic contexts is proposed. It then advances the context by clarifying that Based on established personality models and advances in natural language processing. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Based on established personality models and advances in natural language processing. It also details that Based on established personality models and advances in natural language processing. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Designing Personality-Adaptive Conversational Agents for Mental Health Care - PMC\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se propone el concepto de un agente conversacional adaptativo a la personalidad (PACA) que puede ajustar dinámicamente sus rasgos de personalidad para alinearse mejor con las necesidades individuales del paciente en contextos terapéuticos. Además, contextualiza el aporte al precisar que Basado en modelos de personalidad establecidos y avances en procesamiento de lenguaje natural. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Basado en modelos de personalidad establecidos y avances en procesamiento de lenguaje natural. También se especifica que Basado en modelos de personalidad establecidos y avances en procesamiento de lenguaje natural. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The concept of a personality-adaptive conversational agent (PACA) that can dynamically adjust its personality traits to better align with individual patient needs in therapeutic contexts is proposed.",
        "Based on established personality models and advances in natural language processing.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-113",
    "legacy_article_number": 113,
    "title_original": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
    "title_canonical": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
    "category": "Evaluación y validación psicométrica",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Adithya V Ganesan",
      "Yash Kumar Lal",
      "August Håkan Nilsson",
      "H. Andrew Schwartz"
    ],
    "keywords": [
      "personality estimation",
      "GPT-3",
      "zero-shot learning",
      "Big Five traits",
      "social media analysis",
      "human-level NLP",
      "psychometrics"
    ],
    "source_url": "https://aclanthology.org/2023.wassa-1.34/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.845Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.845Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2023.wassa-1.34/",
      "fetched_title": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "The zero-shot ability of GPT-3 to estimate Big Five personality traits from social media posts is investigated. Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts.",
    "abstract_en_extended": "This study, \"Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The zero-shot ability of GPT-3 to estimate Big Five personality traits from social media posts is investigated. It then advances the context by clarifying that Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts. It also details that Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga la capacidad de cero disparos de GPT-3 para estimar rasgos de personalidad de los Cinco Grandes a partir de publicaciones de redes sociales. Además, contextualiza el aporte al precisar que Se encuentra que el rendimiento de cero disparos de GPT-3 es algo cercano al estado del arte preentrenado existente para clasificación amplia al inyectar conocimiento sobre el rasgo en las instrucciones. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se encuentra que el rendimiento de cero disparos de GPT-3 es algo cercano al estado del arte preentrenado existente para clasificación amplia al inyectar conocimiento sobre el rasgo en las instrucciones. También se especifica que Se encuentra que el rendimiento de cero disparos de GPT-3 es algo cercano al estado del arte preentrenado existente para clasificación amplia al inyectar conocimiento sobre el rasgo en las instrucciones. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The zero-shot ability of GPT-3 to estimate Big Five personality traits from social media posts is investigated.",
        "Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-114",
    "legacy_article_number": 114,
    "title_original": "The Plasticity of ChatGPT's Mentalizing Abilities: Personalization for Personality Structures",
    "title_canonical": "The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures - PMC",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2023,
    "language": "Inglés",
    "authors": [
      "Dorit Hadar-Shoval",
      "Zohar Elyoseph",
      "Maya Lvovsky"
    ],
    "keywords": [
      "artificial intelligence",
      "borderline personality disorder",
      "emotional intelligence",
      "empathy",
      "emotional awareness",
      "Schizoid Personality Disorder",
      "mentalizing"
    ],
    "source_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10503434/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:22.904Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:22.904Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10503434/",
      "fetched_title": "The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures - PMC",
      "title_similarity": 0.9091,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "ChatGPT's potential to generate mentalizing-like abilities tailored to specific personality structures was evaluated. ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies.",
    "abstract_en_extended": "This study, \"The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures - PMC\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that ChatGPT's potential to generate mentalizing-like abilities tailored to specific personality structures was evaluated. It then advances the context by clarifying that ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies. It also details that ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures - PMC\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se evaluó el potencial de ChatGPT para generar habilidades similares a la mentalización adaptadas a estructuras de personalidad específicas. Además, contextualiza el aporte al precisar que ChatGPT describió con precisión las reacciones emocionales de individuos con trastorno límite de la personalidad como más intensas, complejas y ricas que aquellas con trastorno esquizoide de la personalidad, sugiriendo que puede generar respuestas similares a la mentalización consistentes con psicopatologías. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que ChatGPT describió con precisión las reacciones emocionales de individuos con trastorno límite de la personalidad como más intensas, complejas y ricas que aquellas con trastorno esquizoide de la personalidad, sugiriendo que puede generar respuestas similares a la mentalización consistentes con psicopatologías. También se especifica que ChatGPT describió con precisión las reacciones emocionales de individuos con trastorno límite de la personalidad como más intensas, complejas y ricas que aquellas con trastorno esquizoide de la personalidad, sugiriendo que puede generar respuestas similares a la mentalización consistentes con psicopatologías. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "ChatGPT's potential to generate mentalizing-like abilities tailored to specific personality structures was evaluated.",
        "ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2023 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2023"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-115",
    "legacy_article_number": 115,
    "title_original": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "title_canonical": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Guangyuan Jiang",
      "Manjie Xu",
      "Song-Chun Zhu",
      "Wenjuan Han",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "keywords": [
      "Machine Personality Inventory",
      "Big Five personality traits",
      "LLM evaluation",
      "personality prompting",
      "psychometric testing",
      "pre-trained language models",
      "behavioral assessment"
    ],
    "source_url": "https://arxiv.org/abs/2206.07550",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.037Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.037Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2206.07550",
      "fetched_title": "Evaluating and Inducing Personality in Pre-trained Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The Machine Personality Inventory (MPI) tool for studying machine behaviors based on Big Five Personality Factors theory is introduced. By systematically evaluating large language models with MPI, first evidence demonstrating the efficacy of MPI in studying model behaviors is provided. The Personality Prompting (P^2) method is devised to induce models with specific personalities.",
    "abstract_en_extended": "This study, \"Evaluating and Inducing Personality in Pre-trained Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The Machine Personality Inventory (MPI) tool for studying machine behaviors based on Big Five Personality Factors theory is introduced. It then advances the context by clarifying that By systematically evaluating large language models with MPI, first evidence demonstrating the efficacy of MPI in studying model behaviors is provided. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The Personality Prompting (P^2) method is devised to induce models with specific personalities. It also details that The Personality Prompting (P^2) method is devised to induce models with specific personalities. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating and Inducing Personality in Pre-trained Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se introduce la herramienta de Inventario de Personalidad de Máquina (MPI) para estudiar comportamientos de máquina basados en la teoría de los Factores de Personalidad de los Cinco Grandes. Además, contextualiza el aporte al precisar que Al evaluar sistemáticamente modelos de lenguaje de gran escala con MPI, se proporciona la primera evidencia que demuestra la eficacia de MPI en estudiar comportamientos del modelo. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se diseña el método de Instrucciones de Personalidad (P^2) para inducir modelos con personalidades específicas. También se especifica que Se diseña el método de Instrucciones de Personalidad (P^2) para inducir modelos con personalidades específicas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The Machine Personality Inventory (MPI) tool for studying machine behaviors based on Big Five Personality Factors theory is introduced.",
        "By systematically evaluating large language models with MPI, first evidence demonstrating the efficacy of MPI in studying model behaviors is provided.",
        "The Personality Prompting (P^2) method is devised to induce models with specific personalities."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-117",
    "legacy_article_number": 117,
    "title_original": "Identifying and Manipulating the Personality Traits of Language Models",
    "title_canonical": "Identifying and Manipulating the Personality Traits of Language Models",
    "category": "Inducción y control de personalidad",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Graham Caron",
      "Shashank Srivastava"
    ],
    "keywords": [
      "personality traits",
      "Big Five",
      "language model control",
      "persona manipulation",
      "BERT",
      "GPT-2",
      "personality consistency"
    ],
    "source_url": "https://arxiv.org/abs/2212.10276",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.054Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.054Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2212.10276",
      "fetched_title": "Identifying and Manipulating the Personality Traits of Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Whether perceived personality in language models is exhibited consistently in their language generation is explored. When provided different types of contexts, language models such as BERT and GPT-2 are shown to consistently identify and reflect personality markers. This frames them as tools for identifying personality traits and controlling personas.",
    "abstract_en_extended": "This study, \"Identifying and Manipulating the Personality Traits of Language Models\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Whether perceived personality in language models is exhibited consistently in their language generation is explored. It then advances the context by clarifying that When provided different types of contexts, language models such as BERT and GPT-2 are shown to consistently identify and reflect personality markers. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This frames them as tools for identifying personality traits and controlling personas. It also details that This frames them as tools for identifying personality traits and controlling personas. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Identifying and Manipulating the Personality Traits of Language Models\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se explora si la personalidad percibida en modelos de lenguaje se exhibe consistentemente en su generación de lenguaje. Además, contextualiza el aporte al precisar que Se demuestra que cuando se proporcionan diferentes tipos de contextos, modelos de lenguaje como BERT y GPT-2 pueden identificar y reflejar marcadores de personalidad consistentemente. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Esto los enmarca como herramientas para identificar rasgos de personalidad y controlar personas. También se especifica que Esto los enmarca como herramientas para identificar rasgos de personalidad y controlar personas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Whether perceived personality in language models is exhibited consistently in their language generation is explored.",
        "When provided different types of contexts, language models such as BERT and GPT-2 are shown to consistently identify and reflect personality markers.",
        "This frames them as tools for identifying personality traits and controlling personas."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-118",
    "legacy_article_number": 118,
    "title_original": "Estimating the Personality of White-Box Language Models",
    "title_canonical": "Estimating the Personality of White-Box Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Saketh Reddy Karra",
      "Son The Nguyen",
      "Theja Tulabandhula"
    ],
    "keywords": [
      "white-box language models",
      "Big Five personality",
      "zero-shot classification",
      "personality estimation",
      "open-ended text generation",
      "model anthropomorphism",
      "personality modification"
    ],
    "source_url": "https://arxiv.org/abs/2204.12000",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.068Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.068Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2204.12000",
      "fetched_title": "Estimating the Personality of White-Box Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality traits of several large-scale language models designed for open-ended text generation are explored. Building on Big Five factors, robust methods that quantify personality traits of these models and their underlying datasets are developed. Models are triggered with personality assessment questionnaires and text responses are classified into quantifiable traits.",
    "abstract_en_extended": "This study, \"Estimating the Personality of White-Box Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality traits of several large-scale language models designed for open-ended text generation are explored. It then advances the context by clarifying that Building on Big Five factors, robust methods that quantify personality traits of these models and their underlying datasets are developed. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Models are triggered with personality assessment questionnaires and text responses are classified into quantifiable traits. It also details that Models are triggered with personality assessment questionnaires and text responses are classified into quantifiable traits. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Estimating the Personality of White-Box Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se exploran los rasgos de personalidad de varios modelos de lenguaje a gran escala diseñados para generación de texto abierto. Además, contextualiza el aporte al precisar que Construyendo sobre los factores de los Cinco Grandes, se desarrollan métodos robustos que cuantifican los rasgos de personalidad de estos modelos y sus conjuntos de datos subyacentes. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Se activan los modelos con cuestionarios de evaluación de personalidad y las respuestas de texto se clasifican en rasgos cuantificables. También se especifica que Se activan los modelos con cuestionarios de evaluación de personalidad y las respuestas de texto se clasifican en rasgos cuantificables. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality traits of several large-scale language models designed for open-ended text generation are explored.",
        "Building on Big Five factors, robust methods that quantify personality traits of these models and their underlying datasets are developed.",
        "Models are triggered with personality assessment questionnaires and text responses are classified into quantifiable traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-119",
    "legacy_article_number": 119,
    "title_original": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours",
    "title_canonical": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features",
    "category": "Evaluación y validación psicométrica",
    "year": 2022,
    "language": "Inglés",
    "authors": [
      "Elma Kerz",
      "Yu Qiao",
      "Sourabh Zanwar",
      "Daniel Wiechmann"
    ],
    "keywords": [
      "personality detection",
      "psycholinguistic features",
      "BERT transformers",
      "text contours",
      "Big Five",
      "MBTI",
      "verbal behavior analysis"
    ],
    "source_url": "https://arxiv.org/abs/2204.04629",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.083Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.083Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2204.04629",
      "fetched_title": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features",
      "title_similarity": 0.7857,
      "author_overlap": 0.1111,
      "year_match": true
    },
    "abstract_en_original": "Two major improvements in predicting personality traits from text are reported: (1) the most comprehensive set of theory-based psycholinguistic features and (2) hybrid models integrating pre-trained Transformer BERT and BLSTM networks trained on within-text distributions of psycholinguistic features. Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset.",
    "abstract_en_extended": "This study, \"Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Two major improvements in predicting personality traits from text are reported: (1) the most comprehensive set of theory-based psycholinguistic features and (2) hybrid models integrating pre-trained Transformer BERT and BLSTM networks trained on within-text distributions of psycholinguistic features. It then advances the context by clarifying that Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset. It also details that Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se reportan dos mejoras importantes en la predicción de rasgos de personalidad a partir de texto: (1) el conjunto más completo de características psicolingüísticas basadas en teoría y (2) modelos híbridos que integran el Transformer preentrenado BERT y redes BLSTM entrenadas en distribuciones intra-texto de características psicolingüísticas. Además, contextualiza el aporte al precisar que Los modelos logran una mejora del 2.9% en el conjunto de datos Essay y del 8.28% en el conjunto de datos Kaggle MBTI. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los modelos logran una mejora del 2.9% en el conjunto de datos Essay y del 8.28% en el conjunto de datos Kaggle MBTI. También se especifica que Los modelos logran una mejora del 2.9% en el conjunto de datos Essay y del 8.28% en el conjunto de datos Kaggle MBTI. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Two major improvements in predicting personality traits from text are reported: (1) the most comprehensive set of theory-based psycholinguistic features and (2) hybrid models integrating pre-trained Transformer BERT and BLSTM networks trained on within-text distributions of psycholinguistic features.",
        "Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset.",
        "Primary contribution extracted from source abstract. (3)"
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2022 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2022"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-120",
    "legacy_article_number": 120,
    "title_original": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
    "title_canonical": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "J. M. Diederik Kruijssen",
      "Nicholas Emmons"
    ],
    "keywords": [
      "Machine Learning",
      "Artificial Intelligence",
      "Computers and Society",
      "Human-Computer Interaction"
    ],
    "source_url": "https://arxiv.org/abs/2503.17085",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.089Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.089Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.17085",
      "fetched_title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The authors investigate whether AI systems can express deterministic and consistent personalities when instructed using established psychological frameworks. Their research reveals that advanced models like GPT-4o and o1 achieve the highest accuracy on Big Five and Myers-Briggs assessments. The study indicates that personality expression emerges from holistic reasoning rather than question-level optimization, and that fine-tuning affects communication style independently of personality accuracy. The researchers propose this capability could enhance human-AI interaction across education and healthcare applications.",
    "abstract_en_extended": "This study, \"Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The authors investigate whether AI systems can express deterministic and consistent personalities when instructed using established psychological frameworks. It then advances the context by clarifying that Their research reveals that advanced models like GPT-4o and o1 achieve the highest accuracy on Big Five and Myers-Briggs assessments. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The study indicates that personality expression emerges from holistic reasoning rather than question-level optimization, and that fine-tuning affects communication style independently of personality accuracy. It also details that The researchers propose this capability could enhance human-AI interaction across education and healthcare applications. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Se investiga si los sistemas de inteligencia artificial pueden expresar personalidades deterministas y coherentes cuando se instruyen mediante marcos psicológicos establecidos. Además, contextualiza el aporte al precisar que La investigación revela que modelos avanzados como GPT-4o y o1 logran la mayor precisión en evaluaciones Big Five y Myers-Briggs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El estudio indica que la expresión de personalidad emerge del razonamiento holístico en lugar de la optimización pregunta por pregunta, y que el ajuste fino afecta el estilo de comunicación independientemente de la precisión de personalidad. También se especifica que Los investigadores proponen que esta capacidad podría mejorar la interacción humano-IA en educación y aplicaciones de salud. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The authors investigate whether AI systems can express deterministic and consistent personalities when instructed using established psychological frameworks.",
        "Their research reveals that advanced models like GPT-4o and o1 achieve the highest accuracy on Big Five and Myers-Briggs assessments.",
        "The study indicates that personality expression emerges from holistic reasoning rather than question-level optimization, and that fine-tuning affects communication style independently of personality accuracy."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-121",
    "legacy_article_number": 121,
    "title_original": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
    "title_canonical": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jingxuan Li",
      "Yuning Yang",
      "Shengqi Yang",
      "Linfan Zhang",
      "Ying Nian Wu"
    ],
    "keywords": [
      "Computation and Language",
      "ACL 2025"
    ],
    "source_url": "https://arxiv.org/abs/2411.11479",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.099Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.099Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2411.11479",
      "fetched_title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The researchers present a benchmark called Value-Spectrum designed to evaluate vision-language models using Schwartz's framework of human values. They constructed a database with over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels covering diverse topics. The study examines how these models handle value-oriented content and explores their capacity to adopt specific personas when explicitly prompted. The authors conclude that their benchmark offers potential for tracking VLM preferences in value-based tasks and persona simulation abilities. Code and data are available on GitHub.",
    "abstract_en_extended": "This study, \"Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The researchers present a benchmark called Value-Spectrum designed to evaluate vision-language models using Schwartz's framework of human values. It then advances the context by clarifying that They constructed a database with over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels covering diverse topics. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The study examines how these models handle value-oriented content and explores their capacity to adopt specific personas when explicitly prompted. It also details that The authors conclude that their benchmark offers potential for tracking VLM preferences in value-based tasks and persona simulation abilities. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los investigadores presentan un benchmark denominado Value-Spectrum diseñado para evaluar modelos de visión-lenguaje mediante el marco de valores humanos de Schwartz. Además, contextualiza el aporte al precisar que Construyeron una base de datos con más de 50,000 videos cortos de TikTok, YouTube Shorts e Instagram Reels que cubren temas diversos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El estudio examina cómo estos modelos manejan contenido orientado a valores y explora su capacidad para adoptar personas específicas cuando se les instruye explícitamente. También se especifica que Los autores concluyen que su benchmark ofrece potencial para rastrear preferencias de modelos de visión-lenguaje en tareas basadas en valores y capacidades de simulación de personas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The researchers present a benchmark called Value-Spectrum designed to evaluate vision-language models using Schwartz's framework of human values.",
        "They constructed a database with over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels covering diverse topics.",
        "The study examines how these models handle value-oriented content and explores their capacity to adopt specific personas when explicitly prompted."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-122",
    "legacy_article_number": 122,
    "title_original": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents",
    "title_canonical": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yifan Duan",
      "Yihong Tang",
      "Xuefeng Bai",
      "Kehai Chen",
      "Juntao Li",
      "Min Zhang"
    ],
    "keywords": [
      "Computation and Language"
    ],
    "source_url": "https://arxiv.org/abs/2502.20859",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.105Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.105Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.20859",
      "fetched_title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The researchers investigate how personality traits influence LLM agent performance using human simulation methodology. They examine three primary questions regarding how personality shapes problem-solving in structured tasks, creativity in open-ended tasks, and collaborative dynamics. The study assigns Big Five personality characteristics to LLM agents, revealing that specific traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks. Additionally, the research demonstrates that multi-agent teams develop collective intelligence distinct from individual capabilities depending on personality compositions.",
    "abstract_en_extended": "This study, \"The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The researchers investigate how personality traits influence LLM agent performance using human simulation methodology. It then advances the context by clarifying that They examine three primary questions regarding how personality shapes problem-solving in structured tasks, creativity in open-ended tasks, and collaborative dynamics. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The study assigns Big Five personality characteristics to LLM agents, revealing that specific traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks. It also details that Additionally, the research demonstrates that multi-agent teams develop collective intelligence distinct from individual capabilities depending on personality compositions. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los investigadores examinan cómo los rasgos de personalidad influyen en el desempeño de agentes LLM mediante metodología de simulación humana. Además, contextualiza el aporte al precisar que Analizan tres cuestiones principales respecto a cómo la personalidad moldea la resolución de problemas en tareas estructuradas, la creatividad en tareas abiertas y las dinámicas colaborativas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El estudio asigna características de personalidad Big Five a agentes LLM, revelando que rasgos específicos influyen significativamente en la precisión del razonamiento en tareas cerradas y el resultado creativo en tareas abiertas. También se especifica que Adicionalmente, la investigación demuestra que equipos multi-agente desarrollan inteligencia colectiva distinta de las capacidades individuales dependiendo de las composiciones de personalidad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The researchers investigate how personality traits influence LLM agent performance using human simulation methodology.",
        "They examine three primary questions regarding how personality shapes problem-solving in structured tasks, creativity in open-ended tasks, and collaborative dynamics.",
        "The study assigns Big Five personality characteristics to LLM agents, revealing that specific traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-124",
    "legacy_article_number": 124,
    "title_original": "Psychologically Enhanced AI Agents",
    "title_canonical": "Psychologically Enhanced AI Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Maciej Besta",
      "Shriram Chandran",
      "Robert Gerstenberger",
      "Mathis Lindner",
      "Marcin Chrapek",
      "Sebastian Hermann Martschat",
      "Taraneh Ghandi",
      "Patrick Iff",
      "Hubert Niewiadomski",
      "Piotr Nyczyk",
      "Jürgen Müller",
      "Torsten Hoefler"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Computers and Society",
      "Human-Computer Interaction",
      "Multiagent Systems"
    ],
    "source_url": "https://arxiv.org/abs/2509.04343",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.116Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.116Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.04343",
      "fetched_title": "Psychologically Enhanced AI Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The researchers present MBTI-in-Thoughts, a framework that enhances LLM agents through personality-based prompt engineering grounded in Myers-Briggs Type Indicator psychology. The method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology: cognition and affect. The work demonstrates that emotionally-oriented agents perform better at narrative tasks while analytically-primed agents show more stable strategies in game-theoretic contexts. The framework supports multi-agent communication and self-reflection protocols. Personality consistency is verified through the official 16Personalities test. The approach generalizes to other psychological frameworks including Big Five, HEXACO, and Enneagram, requiring no fine-tuning.",
    "abstract_en_extended": "This study, \"Psychologically Enhanced AI Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The researchers present MBTI-in-Thoughts, a framework that enhances LLM agents through personality-based prompt engineering grounded in Myers-Briggs Type Indicator psychology. It then advances the context by clarifying that The method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology: cognition and affect. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The work demonstrates that emotionally-oriented agents perform better at narrative tasks while analytically-primed agents show more stable strategies in game-theoretic contexts. It also details that The framework supports multi-agent communication and self-reflection protocols. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Psychologically Enhanced AI Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los investigadores presentan MBTI-in-Thoughts, un marco que mejora agentes LLM mediante ingeniería de instrucciones basada en personalidad fundamentada en la psicología del Indicador de Tipo Myers-Briggs. Además, contextualiza el aporte al precisar que El método prepara agentes con arquetipos de personalidad distintos mediante ingeniería de instrucciones, permitiendo control sobre el comportamiento a lo largo de dos ejes fundamentales de la psicología humana: cognición y afecto. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El trabajo demuestra que agentes orientados emocionalmente se desempeñan mejor en tareas narrativas mientras que agentes preparados analíticamente muestran estrategias más estables en contextos de teoría de juegos. También se especifica que El marco soporta protocolos de comunicación multi-agente y auto-reflexión. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The researchers present MBTI-in-Thoughts, a framework that enhances LLM agents through personality-based prompt engineering grounded in Myers-Briggs Type Indicator psychology.",
        "The method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology: cognition and affect.",
        "The work demonstrates that emotionally-oriented agents perform better at narrative tasks while analytically-primed agents show more stable strategies in game-theoretic contexts."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-125",
    "legacy_article_number": 125,
    "title_original": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference",
    "title_canonical": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Haoyuan Li",
      "Yuanbo Tong",
      "Yuchen Li",
      "Zirui Wang",
      "Chunhou Liu",
      "Jiamou Liu"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence"
    ],
    "source_url": "https://arxiv.org/abs/2511.00115",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.123Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.123Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.00115",
      "fetched_title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The research proposes ProtoMBTI, a framework treating personality classification through the lens of prototype theory rather than traditional hard-label approaches. The methodology involves constructing a quality-controlled text corpus through LLM-guided augmentation across semantic, linguistic, and sentiment dimensions. A lightweight encoder of 2B parameters or fewer undergoes LoRA fine-tuning to develop discriminative embeddings and standardize personality prototypes. At inference, the system retrieves relevant prototypes and executes a retrieve-reuse-revise-retain cycle, where prototype evidence gets aggregated through voting, inconsistencies trigger revision, and successful predictions enrich the prototype library for continuous improvement. The framework demonstrates enhanced performance on MBTI tasks across multiple benchmarks while providing stronger interpretability and cross-dataset transferability compared to existing approaches.",
    "abstract_en_extended": "This study, \"Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The research proposes ProtoMBTI, a framework treating personality classification through the lens of prototype theory rather than traditional hard-label approaches. It then advances the context by clarifying that The methodology involves constructing a quality-controlled text corpus through LLM-guided augmentation across semantic, linguistic, and sentiment dimensions. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that A lightweight encoder of 2B parameters or fewer undergoes LoRA fine-tuning to develop discriminative embeddings and standardize personality prototypes. It also details that At inference, the system retrieves relevant prototypes and executes a retrieve-reuse-revise-retain cycle, where prototype evidence gets aggregated through voting, inconsistencies trigger revision, and successful predictions enrich the prototype library for continuous improvement. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La investigación propone ProtoMBTI, un marco que trata la clasificación de personalidad mediante la teoría de prototipos en lugar de enfoques tradicionales de etiquetas rígidas. Además, contextualiza el aporte al precisar que La metodología implica construir un corpus de texto controlado en calidad mediante aumento guiado por LLM a través de dimensiones semánticas, lingüísticas y de sentimiento. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Un codificador ligero de 2B parámetros o menos experimenta ajuste fino LoRA para desarrollar incrustaciones discriminativas y estandarizar prototipos de personalidad. También se especifica que En la inferencia, el sistema recupera prototipos relevantes y ejecuta un ciclo recuperar-reusar-revisar-retener, donde la evidencia de prototipos se agrega mediante votación, las inconsistencias activan revisión, y las predicciones exitosas enriquecen la biblioteca de prototipos para mejora continua. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The research proposes ProtoMBTI, a framework treating personality classification through the lens of prototype theory rather than traditional hard-label approaches.",
        "The methodology involves constructing a quality-controlled text corpus through LLM-guided augmentation across semantic, linguistic, and sentiment dimensions.",
        "A lightweight encoder of 2B parameters or fewer undergoes LoRA fine-tuning to develop discriminative embeddings and standardize personality prototypes."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-126",
    "legacy_article_number": 126,
    "title_original": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors",
    "title_canonical": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jia Li",
      "Yichao He",
      "Jiacheng Xu",
      "Tianhao Luo",
      "Zhenzhen Hu",
      "Richang Hong",
      "Meng Wang"
    ],
    "keywords": [
      "Computation and Language",
      "Multimedia",
      "ACM MM 2025"
    ],
    "source_url": "https://arxiv.org/abs/2507.22367",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.133Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.133Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2507.22367",
      "fetched_title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The researchers developed a framework addressing personality assessment challenges. They note that personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors. Their approach uses psychology-informed prompts with large language models and implements a Text-Centric Trait Fusion Network that integrates multimodal signals. Key innovations include personality-specific LLM prompting and audio-visual feature extraction. Results showed approximately 45% MSE reduction, with the method ranking first in the AVI Challenge 2025 Personality Assessment track. Source code availability is planned through GitHub.",
    "abstract_en_extended": "This study, \"Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The researchers developed a framework addressing personality assessment challenges. It then advances the context by clarifying that They note that personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Their approach uses psychology-informed prompts with large language models and implements a Text-Centric Trait Fusion Network that integrates multimodal signals. It also details that Key innovations include personality-specific LLM prompting and audio-visual feature extraction. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los investigadores desarrollaron un marco que aborda desafíos de evaluación de personalidad. Además, contextualiza el aporte al precisar que Notan que los rasgos de personalidad son estables, frecuentemente filtrados subconscientemente a través del lenguaje, expresiones faciales y comportamientos corporales. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Su enfoque utiliza instrucciones informadas por psicología con modelos de lenguaje de gran escala e implementa una Red de Fusión de Rasgos Centrada en Texto que integra señales multimodales. También se especifica que Las innovaciones clave incluyen instrucciones LLM específicas de personalidad y extracción de características audio-visuales. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The researchers developed a framework addressing personality assessment challenges.",
        "They note that personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors.",
        "Their approach uses psychology-informed prompts with large language models and implements a Text-Centric Trait Fusion Network that integrates multimodal signals."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-127",
    "legacy_article_number": 127,
    "title_original": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media",
    "title_canonical": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Tian Ma",
      "Kaiyu Feng",
      "Yu Rong",
      "Kangfei Zhao"
    ],
    "keywords": [
      "Computation and Language",
      "Social and Information Networks",
      "CIKM 2025"
    ],
    "source_url": "https://arxiv.org/abs/2509.04461",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.140Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.140Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.04461",
      "fetched_title": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The research addresses personality prediction from social media through the Myers Briggs Type Indicator (MBTI). The authors developed PostToPersonality (PtoP), an LLM-based framework tackling two primary challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population. The approach employs Retrieval Augmented Generation with in-context learning to reduce hallucinations, while fine-tuning uses synthetic minority oversampling to handle class imbalance. Testing on real-world social media data demonstrates that PtoP achieves state of the art performance compared with 10 ML and DL baselines.",
    "abstract_en_extended": "This study, \"From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The research addresses personality prediction from social media through the Myers Briggs Type Indicator (MBTI). It then advances the context by clarifying that The authors developed PostToPersonality (PtoP), an LLM-based framework tackling two primary challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The approach employs Retrieval Augmented Generation with in-context learning to reduce hallucinations, while fine-tuning uses synthetic minority oversampling to handle class imbalance. It also details that Testing on real-world social media data demonstrates that PtoP achieves state of the art performance compared with 10 ML and DL baselines. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La investigación aborda la predicción de personalidad desde redes sociales mediante el Indicador de Tipo Myers Briggs (MBTI). Además, contextualiza el aporte al precisar que Los autores desarrollaron PostToPersonality (PtoP), un marco basado en LLM que aborda dos desafíos principales: el problema de alucinación inherente en LLMs y la distribución naturalmente desbalanceada de tipos MBTI en la población. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El enfoque emplea Generación Aumentada por Recuperación con aprendizaje en contexto para reducir alucinaciones, mientras que el ajuste fino utiliza sobremuestreo sintético de minorías para manejar el desbalance de clases. También se especifica que Las pruebas en datos de redes sociales del mundo real demuestran que PtoP logra desempeño de vanguardia comparado con 10 baselines de ML y DL. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The research addresses personality prediction from social media through the Myers Briggs Type Indicator (MBTI).",
        "The authors developed PostToPersonality (PtoP), an LLM-based framework tackling two primary challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population.",
        "The approach employs Retrieval Augmented Generation with in-context learning to reduce hallucinations, while fine-tuning uses synthetic minority oversampling to handle class imbalance."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-128",
    "legacy_article_number": 128,
    "title_original": "Exploring the Personality Traits of LLMs through Latent Features Steering",
    "title_canonical": "Exploring the Personality Traits of LLMs through Latent Features Steering",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Shu Yang",
      "Shenzhe Zhu",
      "Liang Liu",
      "Lijie Hu",
      "Mengdi Li",
      "Di Wang"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence"
    ],
    "source_url": "https://arxiv.org/abs/2410.10863",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.149Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.149Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2410.10863",
      "fetched_title": "Exploring the Personality Traits of LLMs through Latent Features Steering",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The research examines how LLMs develop personality characteristics, investigating factors like cultural norms and environmental stressors that shape these traits. The authors propose a methodology that modifies model behavior by extracting and steering internal features, circumventing the need for retraining. Their approach draws from social determinism theory to understand personality expression. The team also evaluates safety implications through the lens of personality traits, addressing how these underlying factors influence model behavior and safety concerns.",
    "abstract_en_extended": "This study, \"Exploring the Personality Traits of LLMs through Latent Features Steering\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The research examines how LLMs develop personality characteristics, investigating factors like cultural norms and environmental stressors that shape these traits. It then advances the context by clarifying that The authors propose a methodology that modifies model behavior by extracting and steering internal features, circumventing the need for retraining. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Their approach draws from social determinism theory to understand personality expression. It also details that The team also evaluates safety implications through the lens of personality traits, addressing how these underlying factors influence model behavior and safety concerns. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Exploring the Personality Traits of LLMs through Latent Features Steering\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que La investigación examina cómo los LLMs desarrollan características de personalidad, investigando factores como normas culturales y estresores ambientales que moldean estos rasgos. Además, contextualiza el aporte al precisar que Los autores proponen una metodología que modifica el comportamiento del modelo extrayendo y dirigiendo características internas, evitando la necesidad de reentrenamiento. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Su enfoque se basa en la teoría del determinismo social para comprender la expresión de personalidad. También se especifica que El equipo también evalúa las implicaciones de seguridad a través del prisma de los rasgos de personalidad, abordando cómo estos factores subyacentes influyen en el comportamiento del modelo y las preocupaciones de seguridad. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The research examines how LLMs develop personality characteristics, investigating factors like cultural norms and environmental stressors that shape these traits.",
        "The authors propose a methodology that modifies model behavior by extracting and steering internal features, circumventing the need for retraining.",
        "Their approach draws from social determinism theory to understand personality expression."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-130",
    "legacy_article_number": 130,
    "title_original": "A-MEM: Agentic Memory for LLM Agents",
    "title_canonical": "A-MEM: Agentic Memory for LLM Agents",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wujiang Xu",
      "Zujie Liang",
      "Kai Mei",
      "Hang Gao",
      "Juntao Tan",
      "Yongfeng Zhang"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Computation and Language",
      "Multiagent Systems"
    ],
    "source_url": "https://arxiv.org/abs/2502.12110",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.161Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.161Z",
      "checks": [
        "arxiv_metadata_checked",
        "arxiv_title_search_checked",
        "verified"
      ],
      "reason": "verified_arxiv_title_search",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.12110",
      "fetched_title": "A-MEM: Agentic Memory for LLM Agents",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "This research presents A-MEM, a framework that enables LLM agents to develop and manage episodic memories using metacognitive reasoning. The system allows agents to autonomously decide what experiences to store, how to organize memories, and when to retrieve them. The authors demonstrate that A-MEM enables agents to form consistent personality traits over extended interactions, maintain coherent behavioral patterns, and adapt their responses based on accumulated experiences. The framework was evaluated across multiple scenarios including social interactions and task-oriented dialogues, showing improved long-term consistency and personalization compared to baseline memory systems.",
    "abstract_en_extended": "This study, \"A-MEM: Agentic Memory for LLM Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This research presents A-MEM, a framework that enables LLM agents to develop and manage episodic memories using metacognitive reasoning. It then advances the context by clarifying that The system allows agents to autonomously decide what experiences to store, how to organize memories, and when to retrieve them. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The authors demonstrate that A-MEM enables agents to form consistent personality traits over extended interactions, maintain coherent behavioral patterns, and adapt their responses based on accumulated experiences. It also details that The framework was evaluated across multiple scenarios including social interactions and task-oriented dialogues, showing improved long-term consistency and personalization compared to baseline memory systems. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"A-MEM: Agentic Memory for LLM Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Esta investigación presenta A-MEM, un marco que permite a los agentes LLM desarrollar y gestionar memorias episódicas mediante razonamiento metacognitivo. Además, contextualiza el aporte al precisar que El sistema permite que los agentes decidan autónomamente qué experiencias almacenar, cómo organizar memorias y cuándo recuperarlas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Los autores demuestran que A-MEM permite a los agentes formar rasgos de personalidad consistentes a lo largo de interacciones extendidas, mantener patrones conductuales coherentes y adaptar sus respuestas basándose en experiencias acumuladas. También se especifica que El marco fue evaluado en múltiples escenarios incluyendo interacciones sociales y diálogos orientados a tareas, mostrando mejor consistencia a largo plazo y personalización comparado con sistemas de memoria baseline. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This research presents A-MEM, a framework that enables LLM agents to develop and manage episodic memories using metacognitive reasoning.",
        "The system allows agents to autonomously decide what experiences to store, how to organize memories, and when to retrieve them.",
        "The authors demonstrate that A-MEM enables agents to form consistent personality traits over extended interactions, maintain coherent behavioral patterns, and adapt their responses based on accumulated experiences."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-134",
    "legacy_article_number": 134,
    "title_original": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
    "title_canonical": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Julia Kharchenko",
      "Tanya Roosta",
      "Aman Chadha",
      "Chirag Shah"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society"
    ],
    "source_url": "https://arxiv.org/abs/2406.14805",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.165Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.165Z",
      "checks": [
        "arxiv_metadata_checked",
        "arxiv_title_search_checked",
        "verified"
      ],
      "reason": "verified_arxiv_title_search",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2406.14805",
      "fetched_title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "The researchers evaluate how well LLMs represent cultural values using Hofstede's cultural dimensions framework. The study systematically prompts multiple LLMs to adopt personas from different cultural backgrounds and assesses whether their responses align with established cultural value patterns. Testing includes GPT-4, Claude, and other frontier models across dimensions such as individualism-collectivism, power distance, and uncertainty avoidance. Results indicate that while LLMs can adjust some surface-level cultural markers, they struggle to consistently embody deep-rooted cultural values, particularly for non-Western cultures. The work emphasizes the need for culturally-aware training approaches beyond simple multilingual data inclusion.",
    "abstract_en_extended": "This study, \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The researchers evaluate how well LLMs represent cultural values using Hofstede's cultural dimensions framework. It then advances the context by clarifying that The study systematically prompts multiple LLMs to adopt personas from different cultural backgrounds and assesses whether their responses align with established cultural value patterns. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Testing includes GPT-4, Claude, and other frontier models across dimensions such as individualism-collectivism, power distance, and uncertainty avoidance. It also details that Results indicate that while LLMs can adjust some surface-level cultural markers, they struggle to consistently embody deep-rooted cultural values, particularly for non-Western cultures. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Los investigadores evalúan qué tan bien los LLMs representan valores culturales usando el marco de dimensiones culturales de Hofstede. Además, contextualiza el aporte al precisar que El estudio instruye sistemáticamente a múltiples LLMs para adoptar personas de diferentes trasfondos culturales y evalúa si sus respuestas se alinean con patrones de valores culturales establecidos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Las pruebas incluyen GPT-4, Claude y otros modelos de frontera a través de dimensiones como individualismo-colectivismo, distancia de poder y evitación de incertidumbre. También se especifica que Los resultados indican que mientras los LLMs pueden ajustar algunos marcadores culturales superficiales, tienen dificultades para encarnar consistentemente valores culturales arraigados, particularmente para culturas no occidentales. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The researchers evaluate how well LLMs represent cultural values using Hofstede's cultural dimensions framework.",
        "The study systematically prompts multiple LLMs to adopt personas from different cultural backgrounds and assesses whether their responses align with established cultural value patterns.",
        "Testing includes GPT-4, Claude, and other frontier models across dimensions such as individualism-collectivism, power distance, and uncertainty avoidance."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-135",
    "legacy_article_number": 135,
    "title_original": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
    "title_canonical": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Cheng Li",
      "Mengzhou Chen",
      "Jindong Wang",
      "Sunayana Sitaram",
      "Xing Xie"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "source_url": "https://arxiv.org/abs/2402.10946",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.193Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.193Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.10946",
      "fetched_title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "This paper presents CultureLLM, a framework designed to embed cultural knowledge into Large Language Models to improve their cross-cultural understanding and generation capabilities. The authors create a cultural knowledge base covering multiple dimensions including values, norms, communication styles, and behavioral expectations across diverse cultures. The framework incorporates this knowledge through specialized fine-tuning and prompting strategies. Evaluation across cultural reasoning tasks and personality assessments shows that CultureLLM-enhanced models produce more culturally appropriate responses and can better simulate personality traits consistent with specific cultural contexts compared to baseline models.",
    "abstract_en_extended": "This study, \"CultureLLM: Incorporating Cultural Differences into Large Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This paper presents CultureLLM, a framework designed to embed cultural knowledge into Large Language Models to improve their cross-cultural understanding and generation capabilities. It then advances the context by clarifying that The authors create a cultural knowledge base covering multiple dimensions including values, norms, communication styles, and behavioral expectations across diverse cultures. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The framework incorporates this knowledge through specialized fine-tuning and prompting strategies. It also details that Evaluation across cultural reasoning tasks and personality assessments shows that CultureLLM-enhanced models produce more culturally appropriate responses and can better simulate personality traits consistent with specific cultural contexts compared to baseline models. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"CultureLLM: Incorporating Cultural Differences into Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo presenta CultureLLM, un marco diseñado para incorporar conocimiento cultural en Modelos de Lenguaje de Gran Escala para mejorar sus capacidades de comprensión y generación intercultural. Además, contextualiza el aporte al precisar que Los autores crean una base de conocimiento cultural que cubre múltiples dimensiones incluyendo valores, normas, estilos de comunicación y expectativas conductuales a través de culturas diversas. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El marco incorpora este conocimiento mediante estrategias especializadas de ajuste fino e instrucción. También se especifica que La evaluación a través de tareas de razonamiento cultural y evaluaciones de personalidad muestra que modelos mejorados con CultureLLM producen respuestas más culturalmente apropiadas y pueden simular mejor rasgos de personalidad consistentes con contextos culturales específicos comparado con modelos baseline. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This paper presents CultureLLM, a framework designed to embed cultural knowledge into Large Language Models to improve their cross-cultural understanding and generation capabilities.",
        "The authors create a cultural knowledge base covering multiple dimensions including values, norms, communication styles, and behavioral expectations across diverse cultures.",
        "The framework incorporates this knowledge through specialized fine-tuning and prompting strategies."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-149",
    "legacy_article_number": 149,
    "title_original": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
    "title_canonical": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yashar Deldjoo",
      "Tommaso Di Noia"
    ],
    "keywords": [
      "Information Retrieval",
      "Computation and Language",
      "Artificial Intelligence",
      "Computers and Society"
    ],
    "source_url": "https://doi.org/10.1145/3725853",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.198Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.198Z",
      "checks": [
        "http_status_non_200",
        "doi_metadata_checked",
        "verified"
      ],
      "reason": "verified_doi_metadata",
      "http_status": 403,
      "final_url": "https://doi.org/10.1145/3725853",
      "fetched_title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "This work presents CFaiRLLM, a specialized framework for evaluating consumer-side fairness in LLM-based recommendation systems that utilize personality profiling. The authors argue that traditional fairness metrics fail to capture harms specific to personality-based personalization, such as the exploitation of personality traits (e.g., targeting high neuroticism users with anxiety-inducing content to increase engagement). The framework introduces personality-aware fairness metrics and evaluates recommendation systems built on GPT-4, Claude, and other LLMs. Findings demonstrate that personality-targeted recommendations can systematically disadvantage users with certain psychological profiles, raising ethical concerns about the deployment of such systems without appropriate safeguards.",
    "abstract_en_extended": "This study, \"CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This work presents CFaiRLLM, a specialized framework for evaluating consumer-side fairness in LLM-based recommendation systems that utilize personality profiling. It then advances the context by clarifying that The authors argue that traditional fairness metrics fail to capture harms specific to personality-based personalization, such as the exploitation of personality traits (e.g., targeting high neuroticism users with anxiety-inducing content to increase engagement). From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The framework introduces personality-aware fairness metrics and evaluates recommendation systems built on GPT-4, Claude, and other LLMs. It also details that Findings demonstrate that personality-targeted recommendations can systematically disadvantage users with certain psychological profiles, raising ethical concerns about the deployment of such systems without appropriate safeguards. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este trabajo presenta CFaiRLLM, un marco especializado para evaluar equidad del lado del consumidor en sistemas de recomendación basados en LLM que utilizan perfilado de personalidad. Además, contextualiza el aporte al precisar que Los autores argumentan que las métricas de equidad tradicionales fallan en capturar daños específicos a personalización basada en personalidad, como la explotación de rasgos de personalidad (por ejemplo, dirigirse a usuarios con alto neuroticismo con contenido inductor de ansiedad para aumentar compromiso). Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El marco introduce métricas de equidad conscientes de personalidad y evalúa sistemas de recomendación construidos sobre GPT-4, Claude y otros LLMs. También se especifica que Los hallazgos demuestran que recomendaciones dirigidas a personalidad pueden sistemáticamente desfavorecer a usuarios con ciertos perfiles psicológicos, planteando preocupaciones éticas sobre el despliegue de tales sistemas sin salvaguardas apropiadas. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This work presents CFaiRLLM, a specialized framework for evaluating consumer-side fairness in LLM-based recommendation systems that utilize personality profiling.",
        "The authors argue that traditional fairness metrics fail to capture harms specific to personality-based personalization, such as the exploitation of personality traits (e.g., targeting high neuroticism users with anxiety-inducing content to increase engagement).",
        "The framework introduces personality-aware fairness metrics and evaluates recommendation systems built on GPT-4, Claude, and other LLMs."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-150",
    "legacy_article_number": 150,
    "title_original": "Emotion Recognition in Conversation via Dynamic Personality Representation Learning",
    "title_canonical": "Emotion Recognition in Conversation via Dynamic Personality",
    "category": "Evaluación y validación psicométrica",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Linh The Nguyen",
      "Dat Ngo",
      "Anh Thu Nguyen",
      "Cong-Tinh Dao",
      "Thien Huu Nguyen"
    ],
    "keywords": [
      "Computation and Language",
      "Artificial Intelligence",
      "Human-Computer Interaction"
    ],
    "source_url": "https://aclanthology.org/2024.lrec-main.507/",
    "source_type": "proceedings",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.199Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.199Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.lrec-main.507/",
      "fetched_title": "Emotion Recognition in Conversation via Dynamic Personality",
      "title_similarity": 0.7778,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "This research proposes a novel approach to emotion recognition in conversations by incorporating dynamic personality representation learning. The authors argue that understanding a speaker's personality traits is crucial for accurately interpreting their emotional expressions, as personality influences how emotions are communicated. The framework learns to extract and update personality representations dynamically throughout conversations, using these representations to improve emotion classification. Experiments on benchmark conversational datasets demonstrate that personality-aware emotion recognition significantly outperforms methods that ignore speaker personality. The work has implications for developing more psychologically grounded conversational AI systems including those based on Large Language Models.",
    "abstract_en_extended": "This study, \"Emotion Recognition in Conversation via Dynamic Personality\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This research proposes a novel approach to emotion recognition in conversations by incorporating dynamic personality representation learning. It then advances the context by clarifying that The authors argue that understanding a speaker's personality traits is crucial for accurately interpreting their emotional expressions, as personality influences how emotions are communicated. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that The framework learns to extract and update personality representations dynamically throughout conversations, using these representations to improve emotion classification. It also details that Experiments on benchmark conversational datasets demonstrate that personality-aware emotion recognition significantly outperforms methods that ignore speaker personality. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Emotion Recognition in Conversation via Dynamic Personality\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Esta investigación propone un enfoque novedoso para reconocimiento de emociones en conversaciones mediante la incorporación de aprendizaje de representación de personalidad dinámica. Además, contextualiza el aporte al precisar que Los autores argumentan que comprender los rasgos de personalidad de un hablante es crucial para interpretar con precisión sus expresiones emocionales, ya que la personalidad influye en cómo se comunican las emociones. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El marco aprende a extraer y actualizar representaciones de personalidad dinámicamente a lo largo de conversaciones, usando estas representaciones para mejorar la clasificación de emociones. También se especifica que Experimentos en conjuntos de datos conversacionales de referencia demuestran que el reconocimiento de emociones consciente de personalidad supera significativamente a métodos que ignoran la personalidad del hablante. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This research proposes a novel approach to emotion recognition in conversations by incorporating dynamic personality representation learning.",
        "The authors argue that understanding a speaker's personality traits is crucial for accurately interpreting their emotional expressions, as personality influences how emotions are communicated.",
        "The framework learns to extract and update personality representations dynamically throughout conversations, using these representations to improve emotion classification."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "proceedings"
        }
      ]
    }
  },
  {
    "id": "article-151",
    "legacy_article_number": 151,
    "title_original": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs",
    "title_canonical": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tianyu Zhao",
      "Siqi Li",
      "Yasser Shoukry",
      "Salma Elmalaki"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2602.07181",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.207Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.207Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.07181",
      "fetched_title": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.",
    "abstract_en_extended": "This study, \"Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. It then advances the context by clarifying that In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. It also details that Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Can LLMs Discern the Traits Influencing Your Preferences? Además, contextualiza el aporte al precisar que Evaluating Personality-Driven Preference Alignment in LLMs\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. También se especifica que Como contexto del resumen original en inglés, se destaca lo siguiente: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored.",
        "In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively.",
        "Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-152",
    "legacy_article_number": 152,
    "title_original": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution",
    "title_canonical": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Deuksin Kwon",
      "Kaleen Shrestha",
      "Bin Han",
      "Spencer Lin",
      "James Hale",
      "Jonathan Gratch",
      "Maja Matarić",
      "Gale M. Lucas"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2602.07414",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.223Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.223Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.07414",
      "fetched_title": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.",
    "abstract_en_extended": "This study, \"Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. It then advances the context by clarifying that However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. It also details that This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Can LLMs Truly Embody Human Personality? Además, contextualiza el aporte al precisar que Analyzing AI and Human Behavior Alignment in Dispute Resolution\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. También se especifica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution.",
        "However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans.",
        "Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-153",
    "legacy_article_number": 153,
    "title_original": "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations",
    "title_canonical": "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jingshu Li",
      "Tianqi Song",
      "Nattapat Boonprakong",
      "Zicheng Zhu",
      "Yitian Yang",
      "Yi-Chieh Lee"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.12727",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.240Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.240Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.12727",
      "fetched_title": "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits. To explore the possibility, we conducted a randomized behavioral experiment. Our results indicate that after conversations about personal topics with an LLM-based AI chatbot using GPT-4o default personality traits, users' self-concepts aligned with the AI's measured personality traits. The longer the conversation, the greater the alignment. This alignment led to increased homogeneity in self-concepts among users. We also observed that the degree of self-concept alignment was positively associated with users' conversation enjoyment. Our findings uncover how AI personality traits can shape users' self-concepts through human-AI conversation, highlighting both risks and opportunities. We provide important design implications for developing more responsible and ethical AI systems.",
    "abstract_en_extended": "This study, \"AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. It then advances the context by clarifying that However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that To explore the possibility, we conducted a randomized behavioral experiment. It also details that Our results indicate that after conversations about personal topics with an LLM-based AI chatbot using GPT-4o default personality traits, users' self-concepts aligned with the AI's measured personality traits. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience.",
        "However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits.",
        "To explore the possibility, we conducted a randomized behavioral experiment."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-154",
    "legacy_article_number": 154,
    "title_original": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations",
    "title_canonical": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Xiaoxu Ma",
      "Xiangbo Zhang",
      "Zhenyu Weng"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2601.09833",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.257Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.257Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.09833",
      "fetched_title": "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.",
    "abstract_en_extended": "This study, \"Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. It then advances the context by clarifying that However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. It also details that PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment.",
        "However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations.",
        "To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-155",
    "legacy_article_number": 155,
    "title_original": "Structured Personality Control and Adaptation for LLM Agents",
    "title_canonical": "Structured Personality Control and Adaptation for LLM Agents",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jinpeng Wang",
      "Xinyu Jia",
      "Wei Wei Heng",
      "Yuquan Li",
      "Binbin Shi",
      "Qianlei Chen",
      "Guannan Chen",
      "Junxia Zhang",
      "Yuyu Yin"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.10025",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.271Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.271Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.10025",
      "fetched_title": "Structured Personality Control and Adaptation for LLM Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.",
    "abstract_en_extended": "This study, \"Structured Personality Control and Adaptation for LLM Agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. It then advances the context by clarifying that Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. It also details that We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Structured Personality Control and Adaptation for LLM Agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Structured Personality Control and Adaptation for LLM Agents\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations.",
        "Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism.",
        "Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-156",
    "legacy_article_number": 156,
    "title_original": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems",
    "title_canonical": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jiongchi Yu",
      "Yuhan Ma",
      "Xiaoyu Zhang",
      "Junjie Wang",
      "Qiang Hu",
      "Chao Shen",
      "Xiaofei Xie"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2602.00016",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.287Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.287Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.00016",
      "fetched_title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.",
    "abstract_en_extended": "This study, \"PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. It then advances the context by clarifying that However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. It also details that PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement.",
        "However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent.",
        "To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-157",
    "legacy_article_number": 157,
    "title_original": "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas",
    "title_canonical": "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jacopo Amidei",
      "Gregorio Ferreira",
      "Mario Muñoz Serrano",
      "Rubén Nieto",
      "Andreas Kaltenbrunner"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2602.03334",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.303Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.303Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.03334",
      "fetched_title": "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. Using five LLMs, we first assess the representativeness and potential biases in the sociodemographic attributes of the generated personas, as well as their alignment with the intended personality traits. While LLMs successfully reproduce known correlations between personality and sociodemographic variables, all models exhibit pronounced WEIRD (western, educated, industrialized, rich and democratic) biases, favoring young, educated, white, heterosexual, Western individuals with centrist or progressive political views and secular or Christian beliefs. In a second analysis, we manipulate input traits to maximize Neuroticism and Psychoticism scores. Notably, when Psychoticism is maximized, several models produce an overrepresentation of non-binary and LGBTQ+ identities, raising concerns about stereotyping and the potential pathologization of marginalized groups. Our findings highlight both the potential and the risks of using LLMs to generate psychologically grounded synthetic populations.",
    "abstract_en_extended": "This study, \"The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. It then advances the context by clarifying that Using five LLMs, we first assess the representativeness and potential biases in the sociodemographic attributes of the generated personas, as well as their alignment with the intended personality traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that While LLMs successfully reproduce known correlations between personality and sociodemographic variables, all models exhibit pronounced WEIRD (western, educated, industrialized, rich and democratic) biases, favoring young, educated, white, heterosexual, Western individuals with centrist or progressive political views and secular or Christian beliefs. It also details that In a second analysis, we manipulate input traits to maximize Neuroticism and Psychoticism scores. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires.",
        "Using five LLMs, we first assess the representativeness and potential biases in the sociodemographic attributes of the generated personas, as well as their alignment with the intended personality traits.",
        "While LLMs successfully reproduce known correlations between personality and sociodemographic variables, all models exhibit pronounced WEIRD (western, educated, industrialized, rich and democratic) biases, favoring young, educated, white, heterosexual, Western individuals with centrist or progressive political views and secular or Christian beliefs."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-158",
    "legacy_article_number": 158,
    "title_original": "Multi-Persona Thinking for Bias Mitigation in Large Language Models",
    "title_canonical": "Multi-Persona Thinking for Bias Mitigation in Large Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Yuxing Chen",
      "Guoqing Luo",
      "Zijun Wu",
      "Lili Mou"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.15488",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.319Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.319Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.15488",
      "fetched_title": "Multi-Persona Thinking for Bias Mitigation in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.",
    "abstract_en_extended": "This study, \"Multi-Persona Thinking for Bias Mitigation in Large Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. It then advances the context by clarifying that In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. It also details that Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Multi-Persona Thinking for Bias Mitigation in Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Multi-Persona Thinking for Bias Mitigation in Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes.",
        "In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias.",
        "MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-159",
    "legacy_article_number": 159,
    "title_original": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias",
    "title_canonical": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Adib Sakhawat",
      "Tahsin Islam",
      "Takia Farhin",
      "Syed Rifat Raiyan",
      "Hasan Mahmud",
      "Md Kamrul Hasan"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Bias",
      "Fairness"
    ],
    "source_url": "https://arxiv.org/abs/2601.06194",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.334Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.334Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.06194",
      "fetched_title": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \\approx 27{,}000$). Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort. Alignment signals appear to be consistent architectural traits rather than stochastic noise ($η^2 &gt; 0.90$); however, we identify substantial discrepancies in measurement validity. In particular, the Political Compass exhibits a strong negative correlation with cultural progressivism ($r=-0.64$) when compared against multi-axial instruments, suggesting a conflation of social conservatism with authoritarianism in this context. We further observe a significant divergence between open-weights and closed-source models, with the latter displaying markedly higher cultural progressivism scores ($p&lt;10^{-25}$). In downstream media analysis, models exhibit a systematic \"center-shift,\" frequently categorizing neutral articles as left-leaning, alongside an asymmetric detection capability in which \"Far Left\" content is identified with greater accuracy (19.2%) than \"Far Right\" content (2.0%). These findings suggest that single-axis evaluations are insufficient and that multidimensional auditing frameworks are necessary to characterize alignment behavior in deployed LLMs. Our code and data will be made public.",
    "abstract_en_extended": "This study, \"Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. It then advances the context by clarifying that This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \\approx 27{,}000$). From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort. It also details that Alignment signals appear to be consistent architectural traits rather than stochastic noise ($η^2 &gt; 0.90$); however, we identify substantial discrepancies in measurement validity. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness.",
        "This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \\approx 27{,}000$).",
        "Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-160",
    "legacy_article_number": 160,
    "title_original": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents",
    "title_canonical": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Bin Han",
      "Deuksin Kwon",
      "Jonathan Gratch"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2602.01063",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.351Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.351Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.01063",
      "fetched_title": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.",
    "abstract_en_extended": "This study, \"Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. It then advances the context by clarifying that This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. It also details that This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context.",
        "This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks.",
        "Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-161",
    "legacy_article_number": 161,
    "title_original": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind",
    "title_canonical": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tamunotonye Harry",
      "Ivoline Ngong",
      "Chima Nweke",
      "Yuanyuan Feng",
      "Joseph Near"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.15395",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.367Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.367Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.15395",
      "fetched_title": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\\% is within-person(state) while only 26\\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.",
    "abstract_en_extended": "This study, \"Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). It then advances the context by clarifying that However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. It also details that Using the Chameleon dataset, we present three key findings. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state).",
        "However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state.",
        "We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-162",
    "legacy_article_number": 162,
    "title_original": "Effects of personality steering on cooperative behavior in Large Language Model agents",
    "title_canonical": "Effects of personality steering on cooperative behavior in Large Language Model agents",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Mizuki Sakai",
      "Mizuki Yokoyama",
      "Wakaba Tateishi",
      "Genki Ichinose"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Bias",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2601.05302",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.381Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.381Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.05302",
      "fetched_title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
    "abstract_en_extended": "This study, \"Effects of personality steering on cooperative behavior in Large Language Model agents\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. It then advances the context by clarifying that Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. It also details that Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Effects of personality steering on cooperative behavior in Large Language Model agents\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Effects of personality steering on cooperative behavior in Large Language Model agents\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions.",
        "Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear.",
        "In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-163",
    "legacy_article_number": 163,
    "title_original": "Persona Prompting as a Lens on LLM Social Reasoning",
    "title_canonical": "Persona Prompting as a Lens on LLM Social Reasoning",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jing Yang",
      "Moritz Hechtbauer",
      "Elisabeth Khalilov",
      "Evelyn Luise Brinkmann",
      "Vera Schmitt",
      "Nils Feldhus"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.20757",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.396Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.396Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.20757",
      "fetched_title": "Persona Prompting as a Lens on LLM Social Reasoning",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.",
    "abstract_en_extended": "This study, \"Persona Prompting as a Lens on LLM Social Reasoning\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. It then advances the context by clarifying that While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. It also details that Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Persona Prompting as a Lens on LLM Social Reasoning\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Persona Prompting as a Lens on LLM Social Reasoning\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment.",
        "While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored.",
        "We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-164",
    "legacy_article_number": 164,
    "title_original": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles",
    "title_canonical": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Francesco Chiappone",
      "Davide Marocco",
      "Nicola Milano"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.15319",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.410Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.410Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.15319",
      "fetched_title": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.",
    "abstract_en_extended": "This study, \"Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. It then advances the context by clarifying that While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. It also details that Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments.",
        "While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics.",
        "This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-165",
    "legacy_article_number": 165,
    "title_original": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks",
    "title_canonical": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Candida M. Greco",
      "Lucio La Cava",
      "Andrea Tagarelli"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Cultural Values",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.22396",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.429Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.429Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.22396",
      "fetched_title": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.",
    "abstract_en_extended": "This study, \"Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. It then advances the context by clarifying that This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. It also details that Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain.",
        "This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory.",
        "We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-166",
    "legacy_article_number": 166,
    "title_original": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications",
    "title_canonical": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Hongliu Cao",
      "Eoin Thomas",
      "Rodrigo Acuna Agost"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Fairness",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2602.00044",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.447Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.447Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.00044",
      "fetched_title": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.",
    "abstract_en_extended": "This study, \"When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. It then advances the context by clarifying that We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. It also details that Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential.",
        "We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation.",
        "Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-167",
    "legacy_article_number": 167,
    "title_original": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making",
    "title_canonical": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Viswonathan Manoranjan",
      "Snehalkumar `Neil' S. Gaikwad"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Bias",
      "Persona",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.10102",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.462Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.462Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.10102",
      "fetched_title": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.",
    "abstract_en_extended": "This study, \"When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. It then advances the context by clarifying that We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. It also details that We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood.",
        "We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives.",
        "Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-168",
    "legacy_article_number": 168,
    "title_original": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
    "title_canonical": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Donya Rooein",
      "Sankalan Pal Chowdhury",
      "Mariia Eremeeva",
      "Yuan Qin",
      "Debora Nozza",
      "Mrinmaya Sachan",
      "Dirk Hovy"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2601.08402",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.477Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.477Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.08402",
      "fetched_title": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",
    "abstract_en_extended": "This study, \"PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. It then advances the context by clarifying that However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Despite this, current LLM tutoring systems do not take into account student personality traits. It also details that To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors.",
        "However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes.",
        "Despite this, current LLM tutoring systems do not take into account student personality traits."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-169",
    "legacy_article_number": 169,
    "title_original": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
    "title_canonical": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Dominik P. Hofer",
      "David Haag",
      "Rania Islambouli",
      "Jan D. Smeddinck"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2602.06596",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.491Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.491Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.06596",
      "fetched_title": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Using ordinal multilevel models with within-between decomposition, we distinguish trial-level effects, whether personality information improves evaluations of individual messages, from person-level exposure effects, whether participants receiving higher proportions of personality-informed messages exhibit systematically different overall perceptions. Results showed no trial-level associations, but participants who received higher proportions of BFPT-informed messages rated the messages as more personalised, appropriate, and reported less negative affect. We use Communication Accommodation Theory for post-hoc analysis. These results suggest that personality-based personalisation in behaviour change systems may operate primarily through aggregate exposure rather than per-message optimisation, with implications for how adaptive systems are designed and evaluated in sustained human-AI interaction. In-situ longitudinal studies are needed to validate these findings in real-world contexts.",
    "abstract_en_extended": "This study, \"Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. It then advances the context by clarifying that LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. It also details that In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts.",
        "LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure.",
        "We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-170",
    "legacy_article_number": 170,
    "title_original": "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation",
    "title_canonical": "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Jana Gonnermann-Müller",
      "Jennifer Haase",
      "Nicolas Leins",
      "Thomas Kosch",
      "Sebastian Pokutta"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.22812",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.504Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.504Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.22812",
      "fetched_title": "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. We address this point using a dual-assessment framework measuring both self-reported characteristics and observer-rated persona expression. Across two experiments testing four persona conditions (default, high, moderate, and low ADHD presentations), seven LLMs, and three semantically equivalent persona prompts, we examine between-conversation stability (3,473 conversations) and within-conversation stability (1,370 conversations and 18 turns). Self-reports remain highly stable both between and within conversations. However, observer ratings reveal a tendency for persona expressions to decline during extended conversations. These findings suggest that persona-instructed LLMs produce stable, persona-aligned self-reports, an important prerequisite for behavioral research, while identifying this regression tendency as a boundary condition for multi-agent social simulation.",
    "abstract_en_extended": "This study, \"Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. It then advances the context by clarifying that We address this point using a dual-assessment framework measuring both self-reported characteristics and observer-rated persona expression. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Across two experiments testing four persona conditions (default, high, moderate, and low ADHD presentations), seven LLMs, and three semantically equivalent persona prompts, we examine between-conversation stability (3,473 conversations) and within-conversation stability (1,370 conversations and 18 turns). It also details that Self-reports remain highly stable both between and within conversations. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations.",
        "We address this point using a dual-assessment framework measuring both self-reported characteristics and observer-rated persona expression.",
        "Across two experiments testing four persona conditions (default, high, moderate, and low ADHD presentations), seven LLMs, and three semantically equivalent persona prompts, we examine between-conversation stability (3,473 conversations) and within-conversation stability (1,370 conversations and 18 turns)."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-171",
    "legacy_article_number": 171,
    "title_original": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
    "title_canonical": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Tassallah Abdullahi",
      "Shrestha Ghosh",
      "Hamish S Fraser",
      "Daniel León Tramontini",
      "Adeel Abbasi",
      "Ghada Bourjeily",
      "Carsten Eickhoff",
      "Ritambhara Singh"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2601.05376",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.509Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.509Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.05376",
      "fetched_title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.",
    "abstract_en_extended": "This study, \"The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. It then advances the context by clarifying that However, its effects on high-stakes clinical decision-making remain poorly characterized. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. It also details that We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner.",
        "However, its effects on high-stakes clinical decision-making remain poorly characterized.",
        "We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-172",
    "legacy_article_number": 172,
    "title_original": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
    "title_canonical": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
    "category": "Evaluación y validación psicométrica",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Yuan Cao",
      "Feixiang Liu",
      "Xinyue Wang",
      "Yihan Zhu",
      "Hui Xu",
      "Zheng Wang",
      "Qiang Qiu"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "MBTI",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2601.18582",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.520Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.520Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2601.18582",
      "fetched_title": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.",
    "abstract_en_extended": "This study, \"From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Personality detection aims to measure an individual's corresponding personality traits through their social media posts. It then advances the context by clarifying that The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. It also details that However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Personality detection aims to measure an individual's corresponding personality traits through their social media posts.",
        "The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks.",
        "Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-173",
    "legacy_article_number": 173,
    "title_original": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
    "title_canonical": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wei Xie",
      "Shuoyoucheng Ma",
      "Zhenhua Wang",
      "Enze Wang",
      "Kai Chen",
      "Xiaobing Sun",
      "Baosheng Wang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Bias",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2509.16530",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.525Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.525Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.16530",
      "fetched_title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM. Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans. This results in high rejection rates when human scales are reused directly. Furthermore, these scales do not support the measurement of LLM psychological property variations in different languages. This paper introduces AIPsychoBench, a specialized benchmark tailored to assess the psychological properties of LLM. It uses a lightweight role-playing prompt to bypass LLM alignment, improving the average effective response rate from 70.12% to 90.40%. Meanwhile, the average biases are only 3.3% (positive) and 2.1% (negative), which are significantly lower than the biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts. Furthermore, among the total of 112 psychometric subcategories, the score deviations for seven languages compared to English ranged from 5% to 20.2% in 43 subcategories, providing the first comprehensive evidence of the linguistic impact on the psychometrics of LLM.",
    "abstract_en_extended": "This study, \"AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. It then advances the context by clarifying that However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans. It also details that This results in high rejection rates when human scales are reused directly. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data.",
        "However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM.",
        "Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-174",
    "legacy_article_number": 174,
    "title_original": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks",
    "title_canonical": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Peiyu Li",
      "Xiuxiu Tang",
      "Si Chen",
      "Ying Cheng",
      "Ronald Metoyer",
      "Ting Hua",
      "Nitesh V. Chawla"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2511.04689",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.537Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.537Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.04689",
      "fetched_title": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. Existing evaluation protocols rely on average accuracy over fixed item sets, treating all items as equally informative despite substantial variation in difficulty and discrimination. We introduce ATLAS, an adaptive testing framework based on Item Response Theory (IRT) that estimates model ability using Fisher information-guided item selection. ATLAS reduces the number of required items by up to 90% while maintaining measurement precision. For instance, it matches whole-bank ability estimates using only 41 items (0.157 MAE) on HellaSwag (5,600 items). We further reconstruct accuracy from ATLAS's ability estimates and find that reconstructed accuracies closely match raw accuracies across all five benchmarks, indicating that ability $θ$ preserves the global performance structure. At the same time, $θ$ provides finer discrimination within accuracy-equivalent models: among more than 3,000 evaluated models, 23-31% shift by more than 10 rank positions, and models with identical accuracies receive meaningfully different ability estimates. Code and calibrated item banks are available at https://github.com/Peiyu-Georgia-Li/ATLAS.git.",
    "abstract_en_extended": "This study, \"Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. It then advances the context by clarifying that Existing evaluation protocols rely on average accuracy over fixed item sets, treating all items as equally informative despite substantial variation in difficulty and discrimination. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We introduce ATLAS, an adaptive testing framework based on Item Response Theory (IRT) that estimates model ability using Fisher information-guided item selection. It also details that ATLAS reduces the number of required items by up to 90% while maintaining measurement precision. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale.",
        "Existing evaluation protocols rely on average accuracy over fixed item sets, treating all items as equally informative despite substantial variation in difficulty and discrimination.",
        "We introduce ATLAS, an adaptive testing framework based on Item Response Theory (IRT) that estimates model ability using Fisher information-guided item selection."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-175",
    "legacy_article_number": 175,
    "title_original": "Can LLMs Infer Personality from Real World Conversations?",
    "title_canonical": "Can LLMs Infer Personality from Real World Conversations?",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jianfeng Zhu",
      "Ruoming Jin",
      "Karin G. Coifman"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Psychometrics",
      "Bias"
    ],
    "source_url": "https://arxiv.org/abs/2507.14355",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.541Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.541Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2507.14355",
      "fetched_title": "Can LLMs Infer Personality from Real World Conversations?",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity. We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item prediction and both zero-shot and chain-of-thought prompting for Big Five trait inference. All models showed high test-retest reliability, but construct validity was limited: correlations with ground-truth scores were weak (max Pearson's $r = 0.27$), interrater agreement was low (Cohen's $κ&lt; 0.10$), and predictions were biased toward moderate or high trait levels. Chain-of-thought prompting and longer input context modestly improved distributional alignment, but not trait-level accuracy. These results underscore limitations in current LLM-based personality inference and highlight the need for evidence-based development for psychological applications.",
    "abstract_en_extended": "This study, \"Can LLMs Infer Personality from Real World Conversations?\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. It then advances the context by clarifying that However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference. It also details that Three state-of-the-art LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item prediction and both zero-shot and chain-of-thought prompting for Big Five trait inference. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Can LLMs Infer Personality from Real World Conversations?\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Can LLMs Infer Personality from Real World Conversations?\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language.",
        "However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity.",
        "We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-176",
    "legacy_article_number": 176,
    "title_original": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs",
    "title_canonical": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Pranav Bhandari",
      "Nicolas Fay",
      "Sanjeevan Selvaganapathy",
      "Amitava Datta",
      "Usman Naseem",
      "Mehwish Nasim"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2511.03738",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.547Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.547Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.03738",
      "fetched_title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled. Personality-aware LLMs hold a promising direction towards this objective. However, the relationship between these psychological constructs and their representations within LLMs remains underexplored and requires further investigation. Moreover, it is intriguing to understand and study the use of these representations to steer the models' behaviour. We propose a novel pipeline that extracts hidden state activations from transformer layers using the Big Five Personality Traits (Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), which is a comprehensive and empirically validated framework to model human personality applies low-rank subspace discovery methods, and identifies trait-specific optimal layers across different model architectures for robust injection. The resulting personality-aligned directions are then operationalised through a flexible steering framework with dynamic layer selection, enabling precise control of trait expression in LLM outputs. Our findings reveal that personality traits occupy a low-rank shared subspace, and that these latent structures can be transformed into actionable mechanisms for effective steering through careful perturbations without impacting the fluency, variance and general capabilities, helping to bridge the gap between psychological theory and practical model alignment.",
    "abstract_en_extended": "This study, \"Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. It then advances the context by clarifying that The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Personality-aware LLMs hold a promising direction towards this objective. It also details that However, the relationship between these psychological constructs and their representations within LLMs remains underexplored and requires further investigation. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge.",
        "The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled.",
        "Personality-aware LLMs hold a promising direction towards this objective."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-177",
    "legacy_article_number": 177,
    "title_original": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
    "title_canonical": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yifan Lyu",
      "Liang Zhang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "MBTI",
      "Psychometrics",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2512.08814",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.557Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.557Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2512.08814",
      "fetched_title": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -&gt; user vector -&gt; labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).",
    "abstract_en_extended": "This study, \"Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. It then advances the context by clarifying that Existing studies on personality detection predominantly adopt a \"posts -&gt; user vector -&gt; labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. It also details that We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment.",
        "Existing studies on personality detection predominantly adopt a \"posts -&gt; user vector -&gt; labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels).",
        "While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-178",
    "legacy_article_number": 178,
    "title_original": "A Comparative Study of Large Language Models and Human Personality Traits",
    "title_canonical": "A Comparative Study of Large Language Models and Human Personality Traits",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Wang Jiaqi",
      "Wang bo",
      "Guo fa",
      "Cheng cheng",
      "Yang li"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2505.14845",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.560Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.560Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2505.14845",
      "fetched_title": "A Comparative Study of Large Language Models and Human Personality Traits",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.",
    "abstract_en_extended": "This study, \"A Comparative Study of Large Language Models and Human Personality Traits\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. It then advances the context by clarifying that This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that A behavior-based approach was used across three empirical studies. It also details that Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"A Comparative Study of Large Language Models and Human Personality Traits\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"A Comparative Study of Large Language Models and Human Personality Traits\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains.",
        "This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools.",
        "A behavior-based approach was used across three empirical studies."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-179",
    "legacy_article_number": 179,
    "title_original": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
    "title_canonical": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Pranav Bhandari",
      "Usman Naseem",
      "Amitava Datta",
      "Nicolas Fay",
      "Mehwish Nasim"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Model Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2502.05248",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.562Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.562Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2502.05248",
      "fetched_title": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.",
    "abstract_en_extended": "This study, \"Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Psychological assessment tools have long helped humans understand behavioural patterns. It then advances the context by clarifying that While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. It also details that Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Psychological assessment tools have long helped humans understand behavioural patterns. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Psychological assessment tools have long helped humans understand behavioural patterns.",
        "While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits.",
        "To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-180",
    "legacy_article_number": 180,
    "title_original": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
    "title_canonical": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Michel Frising",
      "Daniel Balcells"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2512.17639",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.577Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.577Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2512.17639",
      "fetched_title": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.",
    "abstract_en_extended": "This study, \"Linear Personality Probing and Steering in LLMs: A Big Five Study\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. It then advances the context by clarifying that While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. It also details that In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Linear Personality Probing and Steering in LLMs: A Big Five Study\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Linear Personality Probing and Steering in LLMs: A Big Five Study\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement.",
        "While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering).",
        "Probing and steering via linear directions has recently emerged as a cheap and efficient alternative."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-181",
    "legacy_article_number": 181,
    "title_original": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
    "title_canonical": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Francesco Di Cursi",
      "Chiara Boldrini",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Bias",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2511.23101",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.579Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.579Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.23101",
      "fetched_title": "Mind Reading or Misreading? LLMs on the Big Five Personality Test",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.",
    "abstract_en_extended": "This study, \"Mind Reading or Misreading? LLMs on the Big Five Personality Test\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). It then advances the context by clarifying that Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that enriched with linguistic and psychological cues). It also details that Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Mind Reading or Misreading? LLMs on the Big Five Personality Test\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Mind Reading or Misreading? Además, contextualiza el aporte al precisar que LLMs on the Big Five Personality Test\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. También se especifica que Como contexto del resumen original en inglés, se destaca lo siguiente: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5).",
        "Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs.",
        "enriched with linguistic and psychological cues)."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-182",
    "legacy_article_number": 182,
    "title_original": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
    "title_canonical": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Dongmin Choi",
      "Woojung Song",
      "Jongwook Han",
      "Eun-Ju Lee",
      "Yohan Jo"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2509.10078",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.594Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.594Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2509.10078",
      "fetched_title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.",
    "abstract_en_extended": "This study, \"Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). It then advances the context by clarifying that However, concerns have been raised about applying these human-designed questionnaires to LLMs. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. It also details that However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Established Psychometric vs. Además, contextualiza el aporte al precisar que Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. También se especifica que Como contexto del resumen original en inglés, se destaca lo siguiente: Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs).",
        "However, concerns have been raised about applying these human-designed questionnaires to LLMs.",
        "One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-183",
    "legacy_article_number": 183,
    "title_original": "Investigating Large Language Models in Inferring Personality Traits from User Conversations",
    "title_canonical": "Investigating Large Language Models in Inferring Personality Traits from User Conversations",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jianfeng Zhu",
      "Ruoming Jin",
      "Karin G. Coifman"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Persona",
      "Personality Control"
    ],
    "source_url": "https://arxiv.org/abs/2501.07532",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.597Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.597Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2501.07532",
      "fetched_title": "Investigating Large Language Models in Inferring Personality Traits from User Conversations",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. This study evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer Big Five personality traits and generate Big Five Inventory-10 (BFI-10) item scores from user conversations under zero-shot prompting conditions. Our findings reveal that incorporating an intermediate step--prompting for BFI-10 item scores before calculating traits--enhances accuracy and aligns more closely with the gold standard than direct trait inference. This structured approach underscores the importance of leveraging psychological frameworks in improving predictive precision. Additionally, a group comparison based on depressive symptom presence revealed differential model performance. Participants were categorized into two groups: those experiencing at least one depressive symptom and those without symptoms. GPT-4o mini demonstrated heightened sensitivity to depression-related shifts in traits such as Neuroticism and Conscientiousness within the symptom-present group, whereas GPT-4o exhibited strengths in nuanced interpretation across groups. These findings underscore the potential of LLMs to analyze real-world psychological data effectively, offering a valuable foundation for interdisciplinary research at the intersection of artificial intelligence and psychology.",
    "abstract_en_extended": "This study, \"Investigating Large Language Models in Inferring Personality Traits from User Conversations\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. It then advances the context by clarifying that This study evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer Big Five personality traits and generate Big Five Inventory-10 (BFI-10) item scores from user conversations under zero-shot prompting conditions. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Our findings reveal that incorporating an intermediate step--prompting for BFI-10 item scores before calculating traits--enhances accuracy and aligns more closely with the gold standard than direct trait inference. It also details that This structured approach underscores the importance of leveraging psychological frameworks in improving predictive precision. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Investigating Large Language Models in Inferring Personality Traits from User Conversations\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Investigating Large Language Models in Inferring Personality Traits from User Conversations\", se ubica en la línea de evaluación y validación psicométrica y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment.",
        "This study evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer Big Five personality traits and generate Big Five Inventory-10 (BFI-10) item scores from user conversations under zero-shot prompting conditions.",
        "Our findings reveal that incorporating an intermediate step--prompting for BFI-10 item scores before calculating traits--enhances accuracy and aligns more closely with the gold standard than direct trait inference."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-184",
    "legacy_article_number": 184,
    "title_original": "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests",
    "title_canonical": "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Nicola Milano",
      "Michela Ponticorvo",
      "Davide Marocco"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Big Five",
      "Psychometrics",
      "Persona"
    ],
    "source_url": "https://arxiv.org/abs/2503.12080",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.610Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.610Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.12080",
      "fetched_title": "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs. Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment. Graduate psychology students employed the Content Validity Ratio (CVR) to rate test items, forming the human baseline. In parallel, state-of-the-art LLMs, including multilingual and fine-tuned models, analyzed item embeddings to predict construct mappings. The results reveal distinct strengths and limitations of human and AI approaches. Human validators excelled in aligning the behaviorally rich BFQ items, while LLMs performed better with the linguistically concise BFI items. Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs. Here we highlights the complementary potential of hybrid validation systems that integrate human expertise and AI precision. The findings underscore the transformative role of LLMs in psychological assessment, paving the way for scalable, objective, and robust test development methodologies.",
    "abstract_en_extended": "This study, \"Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). It then advances the context by clarifying that Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment. It also details that Graduate psychology students employed the Content Validity Ratio (CVR) to rate test items, forming the human baseline. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Además, contextualiza el aporte al precisar que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Como contexto del resumen original en inglés, se destaca lo siguiente: In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). También se especifica que En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI).",
        "Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs.",
        "Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-185",
    "legacy_article_number": 185,
    "title_original": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
    "title_canonical": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Jana Jung",
      "Marlene Lutz",
      "Indira Sen",
      "Markus Strohmaier"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Personality Control",
      "AI Safety"
    ],
    "source_url": "https://arxiv.org/abs/2510.11254",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.613Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.613Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2510.11254",
      "fetched_title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests on 17 LLMs for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests on LLMs are essential before interpreting their scores. Our findings also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.",
    "abstract_en_extended": "This study, \"Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). It then advances the context by clarifying that However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that In this study, we systematically evaluate the reliability and validity of human psychometric tests on 17 LLMs for three constructs: sexism, racism, and morality. It also details that We find moderate reliability across multiple item and prompt variations. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo, titulado \"Do Psychometric Tests Work for Large Language Models? Además, contextualiza el aporte al precisar que Evaluation of Tests on Sexism, Racism, and Morality\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza «personalidad sintética» en modelos de lenguaje. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. También se especifica que Como contexto del resumen original en inglés, se destaca lo siguiente: Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs).",
        "However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs.",
        "In this study, we systematically evaluate the reliability and validity of human psychometric tests on 17 LLMs for three constructs: sexism, racism, and morality."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-186",
    "legacy_article_number": 186,
    "title_original": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
    "title_canonical": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Xiaotian Zhang",
      "Ruizhe Chen",
      "Yang Feng",
      "Zuozhu Liu"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2504.12663",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.617Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.617Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2504.12663",
      "fetched_title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.",
    "abstract_en_extended": "This study, \"Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. It then advances the context by clarifying that Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. It also details that Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs.",
        "Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values.",
        "To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-187",
    "legacy_article_number": 187,
    "title_original": "How Personality Traits Shape LLM Risk-Taking Behaviour",
    "title_canonical": "How Personality Traits Shape LLM Risk-Taking Behaviour",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "John Hartley",
      "Conor Hamill",
      "Devesh Batra",
      "Dale Seddon",
      "Ramin Okhrati",
      "Raad Khraishi"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Bias and Fairness",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2503.04735",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.631Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.631Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.04735",
      "fetched_title": "How Personality Traits Shape LLM Risk-Taking Behaviour",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Interventions on GPT-4o's Big Five traits, particularly Openness, significantly influence its risk propensity, mirroring patterns observed in human studies. Notably, Openness emerges as the most influential factor in GPT-4o's risk propensity, aligning with human findings. In contrast, legacy models like GPT-4-Turbo demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and elucidates the potential and limitations of personality-based interventions in shaping LLM decision-making. Our findings have implications for the development of more robust and predictable AI systems such as financial modelling.",
    "abstract_en_extended": "This study, \"How Personality Traits Shape LLM Risk-Taking Behaviour\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. It then advances the context by clarifying that This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. It also details that Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"How Personality Traits Shape LLM Risk-Taking Behaviour\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para aplicaciones, sesgos y consecuencias sociales. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk.",
        "This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework.",
        "We focus on GPT-4o, comparing its behaviour to human baselines and earlier models."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-188",
    "legacy_article_number": 188,
    "title_original": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
    "title_canonical": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Ke Ji",
      "Yixin Lian",
      "Linxu Li",
      "Jingsheng Gao",
      "Weiyuan Li",
      "Bin Dai"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2503.17662",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.637Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.637Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2503.17662",
      "fetched_title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive \\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model's role-playing strategy through iterative contrastive learning between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval \\&amp; GPT-4) and human expert evaluation.",
    "abstract_en_extended": "This study, \"Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. It then advances the context by clarifying that However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. It also details that Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive \\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks.",
        "However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further.",
        "Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-189",
    "legacy_article_number": 189,
    "title_original": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey",
    "title_canonical": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Fernanda Bufon Färber",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues Galvão Filho"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://aclanthology.org/2025.findings-emnlp.506/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.641Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.641Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.findings-emnlp.506/",
      "fetched_title": "Modeling, Evaluating, and Embodying Personality in LLMs: A Survey",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.",
    "abstract_en_extended": "This study, \"Modeling, Evaluating, and Embodying Personality in LLMs: A Survey\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. It then advances the context by clarifying that The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems. It also details that It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Modeling, Evaluating, and Embodying Personality in LLMs: A Survey\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica.",
        "The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria.",
        "It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-190",
    "legacy_article_number": 190,
    "title_original": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
    "title_canonical": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Christos-Nikolaos Zacharopoulos",
      "Revekka Kyriakoglou"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2511.04499",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.653Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.653Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.04499",
      "fetched_title": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",
    "abstract_en_extended": "This study, \"Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. It then advances the context by clarifying that This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. It also details that Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment.",
        "This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures.",
        "We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-191",
    "legacy_article_number": 191,
    "title_original": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?",
    "title_canonical": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?",
    "category": "Inducción y control de personalidad",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Rui Xu",
      "Xintao Wang",
      "Jiangjie Chen",
      "Siyu Yuan",
      "Xinfeng Yuan",
      "Jiaqing Liang",
      "Zulong Chen",
      "Xiaoqingdong",
      "Yanghua Xiao"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://aclanthology.org/2025.findings-emnlp.813/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.656Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.656Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.findings-emnlp.813/",
      "fetched_title": "Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "This paper examines personality-related behavior in large language models and its methodological implications for inducción y control de personalidad. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.",
    "abstract_en_extended": "This study, \"Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This paper examines personality-related behavior in large language models and its methodological implications for inducción y control de personalidad. It then advances the context by clarifying that The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems. It also details that It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This paper examines personality-related behavior in large language models and its methodological implications for inducción y control de personalidad.",
        "The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria.",
        "It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-192",
    "legacy_article_number": 192,
    "title_original": "Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models",
    "title_canonical": "Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yixiao Wang",
      "Russel Dsouza",
      "Robert Lee",
      "Ian Apperly",
      "Rory Devine",
      "Sanne van der Kleij",
      "Mark Lee"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://aclanthology.org/2025.clpsych-1.7/",
    "source_type": "publisher",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:23.667Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:23.667Z",
      "checks": [
        "page_title_checked",
        "verified"
      ],
      "reason": "verified_web",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2025.clpsych-1.7/",
      "fetched_title": "Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models",
      "title_similarity": 1,
      "author_overlap": 0,
      "year_match": true
    },
    "abstract_en_original": "This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.",
    "abstract_en_extended": "This study, \"Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. It then advances the context by clarifying that The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems. It also details that It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica.",
        "The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria.",
        "It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "publisher"
        }
      ]
    }
  },
  {
    "id": "article-193",
    "legacy_article_number": 193,
    "title_original": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning",
    "title_canonical": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning",
    "category": "Aplicaciones, sesgos y consecuencias sociales",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Haijiang Liu",
      "Qiyuan Li",
      "Chao Gao",
      "Yong Cao",
      "Xiangyu Xu",
      "Xun Wu",
      "Daniel Hershcovich",
      "Jinguang Gu"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Bias and Fairness",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2508.17855",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.446Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.446Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2508.17855",
      "fetched_title": "Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions.",
    "abstract_en_extended": "This study, \"Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning\", is situated in the research line of aplicaciones, sesgos y consecuencias sociales and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. It then advances the context by clarifying that The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. It also details that Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning\", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para aplicaciones, sesgos y consecuencias sociales. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task.",
        "The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research.",
        "It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation."
      ],
      "method_notes": [
        "The study is classified in the repository category: Aplicaciones, sesgos y consecuencias sociales.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Aplicaciones, sesgos y consecuencias sociales"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-194",
    "legacy_article_number": 194,
    "title_original": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
    "title_canonical": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
    "category": "Inducción y control de personalidad",
    "year": 2024,
    "language": "Inglés",
    "authors": [
      "Chenkai Sun",
      "Ke Yang",
      "Revanth Gangi Reddy",
      "Yi R. Fung",
      "Hou Pong Chan",
      "Kevin Small",
      "ChengXiang Zhai",
      "Heng Ji"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2402.11060",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.455Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.455Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2402.11060",
      "fetched_title": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the evaluation of response prediction, Persona-DB demonstrates superior context efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 10% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands.",
    "abstract_en_extended": "This study, \"Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences. It then advances the context by clarifying that Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. It also details that In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences.",
        "Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning.",
        "Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2024 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2024"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-195",
    "legacy_article_number": 195,
    "title_original": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
    "title_canonical": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
    "category": "Evaluación y validación psicométrica",
    "year": 2025,
    "language": "Inglés",
    "authors": [
      "Yi-Fei Liu",
      "Yi-Long Lu",
      "Di He",
      "Hang Zhang"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Psychometrics",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2511.03235",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.460Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.460Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2511.03235",
      "fetched_title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 &gt; 0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.",
    "abstract_en_extended": "This study, \"From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers\", is situated in the research line of evaluación y validación psicométrica and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Psychological constructs within individuals are widely believed to be interconnected. It then advances the context by clarifying that We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. It also details that LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 &gt; 0.89)$. Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers\", se ubica en la línea de evaluación y validación psicométrica dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Psychological constructs within individuals are widely believed to be interconnected.",
        "We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs.",
        "We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales."
      ],
      "method_notes": [
        "The study is classified in the repository category: Evaluación y validación psicométrica.",
        "Evidence interpretation should be compared with adjacent papers from year 2025 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Evaluación y validación psicométrica"
        },
        {
          "field": "Publication year",
          "value": "2025"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  },
  {
    "id": "article-196",
    "legacy_article_number": 196,
    "title_original": "Your Language Model Secretly Contains Personality Subnetworks",
    "title_canonical": "Your Language Model Secretly Contains Personality Subnetworks",
    "category": "Inducción y control de personalidad",
    "year": 2026,
    "language": "Inglés",
    "authors": [
      "Ruimeng Ye",
      "Zihan Wang",
      "Zinan Ling",
      "Yang Xiao",
      "Manling Li",
      "Xiaolong Ma",
      "Bo Hui"
    ],
    "keywords": [
      "Large Language Models",
      "Personality",
      "Persona",
      "Model Evaluation",
      "LLM Evaluation"
    ],
    "source_url": "https://arxiv.org/abs/2602.07164",
    "source_type": "arxiv",
    "verification_status": "verified",
    "verification_date": "2026-02-13T11:07:24.471Z",
    "evidence": {
      "checked_at": "2026-02-13T11:07:24.471Z",
      "checks": [
        "arxiv_metadata_checked",
        "verified"
      ],
      "reason": "verified_arxiv",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2602.07164",
      "fetched_title": "Your Language Model Secretly Contains Personality Subnetworks",
      "title_similarity": 1,
      "author_overlap": 1,
      "year_match": true
    },
    "abstract_en_original": "Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.",
    "abstract_en_extended": "This study, \"Your Language Model Secretly Contains Personality Subnetworks\", is situated in the research line of inducción y control de personalidad and is interpreted within a synthetic personality framework for large language models. The original abstract establishes the main problem by emphasizing that Humans shift between different personas depending on social context. It then advances the context by clarifying that Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. From a review perspective, this opening section indicates that the work targets a concrete gap in model behavior analysis and not only a conceptual debate, because the article explicitly frames personality as an operational variable linked to measurable outputs, behavioral consistency, and downstream interaction quality. In practical terms, the framing is relevant for AI evaluation pipelines, psychometric adaptation, and deployment governance, especially when personality traits are used to explain differences in model responses. The paper is therefore positioned as an empirical contribution with methodological implications for reproducibility and interpretation across model families and prompting setups.\n\nMethodologically, the article describes an approach where evidence is organized around testable procedures rather than anecdotal observations. The core technical narrative reports that Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. It also details that We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? Under this structure, the paper contributes a measurable pathway to compare model behavior with psychological constructs, while also making explicit tradeoffs between internal validity, ecological validity, and prompt sensitivity. For scientific synthesis, this matters because it enables cross-study comparison using common anchors such as trait dimensions, calibration settings, and evaluation protocol stability. The work can also be interpreted as part of a broader trend in which personality-related benchmarks are increasingly used as probes for model alignment and decision patterns. Importantly, the methodological layer should be read with attention to instrumentation assumptions and to whether the reported effects are robust to re-ordering, role instructions, and context-window perturbations.\n\nAt the level of findings and implications, the paper supports a cautious but actionable interpretation for researchers and practitioners working on personality-aware systems. The reported evidence is most valuable when integrated with reliability checks, source transparency, and explicit limitations, because synthetic personality signals may represent a combination of learned linguistic priors, instruction artifacts, and task-specific response strategies. In this repository, the article is treated as a high-value node for comparative analysis across years, categories, and evaluation traditions, especially in relation to fairness, safety, and controllability questions. Its conclusions should be triangulated with neighboring studies to avoid overgeneralization from single benchmarks, and to distinguish between stable trait-like behavior and context-conditioned style changes. Overall, the work strengthens the empirical basis of the field while reinforcing the need for reproducible protocols, canonical metadata, and rigorous interpretation standards before translating psychometric claims into deployed applications. This extended synthesis is derived from the verified source metadata, the original abstract content, and the repository-wide normalization criteria defined for methodological comparability.",
    "resumen_es_extendido": "Este trabajo, \"Your Language Model Secretly Contains Personality Subnetworks\", se ubica en la línea de inducción y control de personalidad dentro del estudio de «personalidad sintética» en modelos de lenguaje. El resumen original delimita el problema principal al señalar que Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. Además, contextualiza el aporte al precisar que El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Desde una lectura de estado del arte, este encuadre inicial es relevante porque traduce una discusión amplia sobre personalidad en IA hacia una pregunta operativa, evaluable y comparable entre modelos, configuraciones y protocolos. En términos aplicados, la contribución de esta sección introductoria está en conectar la medición de rasgos con decisiones técnicas de diseño, evaluación y despliegue, incluyendo la interpretación de conductas conversacionales bajo marcos psicométricos. Por ello, el artículo se incorpora en este repositorio como evidencia empírica que ayuda a distinguir entre afirmaciones teóricas y resultados observables en condiciones experimentales definidas.\n\nEn el plano metodológico, el artículo se estructura alrededor de procedimientos replicables y no únicamente descripciones narrativas. La lógica experimental comunica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. También se especifica que Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados. Esta arquitectura metodológica permite contrastar resultados entre estudios y analizar, con mayor precisión, qué parte de la variación observada proviene del modelo, del prompt, del instrumento de medición o de la configuración del experimento. Para una revisión científica rigurosa, este punto es clave porque aporta criterios de comparabilidad y facilita auditorías posteriores sobre fiabilidad, validez y estabilidad temporal. Además, la metodología puede leerse como parte de una transición del campo hacia evaluaciones más sólidas, donde la personalidad se usa como señal diagnóstica de capacidades, sesgos y límites de alineación. Aun así, la interpretación exige cautela frente a sensibilidad al contexto, cambios por instrucción y posibles artefactos de formato.\n\nRespecto a resultados e implicaciones, el estudio aporta evidencia útil para investigación y práctica, pero su valor máximo aparece cuando se triangula con trabajos complementarios y controles de calidad estrictos. Las señales de personalidad observadas en modelos de lenguaje pueden reflejar una combinación de patrones aprendidos, efectos del entorno conversacional y restricciones del propio instrumento psicométrico, por lo que no deben interpretarse como equivalentes directos de rasgos humanos estables. En esta curación canónica, el artículo se considera una pieza de comparación transversal para analizar evolución temporal del campo, diferencias por categoría y tensión entre rendimiento, seguridad y equidad. Sus conclusiones contribuyen a fortalecer la base empírica del área y, al mismo tiempo, subrayan la necesidad de mantener trazabilidad de fuentes, criterios explícitos de validación y protocolos reproducibles antes de trasladar hallazgos a sistemas de producción o a contextos sensibles como salud mental, educación y decisión social automatizada. Esta ampliación analítica se redacta a partir de metadatos verificados, del contenido original del resumen y de los criterios editoriales unificados del repositorio.",
    "structured_blocks": {
      "key_points": [
        "Humans shift between different personas depending on social context.",
        "Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors.",
        "Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning."
      ],
      "method_notes": [
        "The study is classified in the repository category: Inducción y control de personalidad.",
        "Evidence interpretation should be compared with adjacent papers from year 2026 and related benchmark designs."
      ],
      "limitations": [
        "This extended summary is derived from source abstracts and metadata, not from full-paper line-by-line extraction.",
        "Behavioral and psychometric claims should be interpreted with protocol sensitivity and reproducibility checks."
      ],
      "mini_table": [
        {
          "field": "Category",
          "value": "Inducción y control de personalidad"
        },
        {
          "field": "Publication year",
          "value": "2026"
        },
        {
          "field": "Source type",
          "value": "arxiv"
        }
      ]
    }
  }
]