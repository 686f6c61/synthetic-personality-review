# ESTADO DEL ARTE. LA PERSONALIDAD SINTÉTICA: EVALUACIÓN PSICOLÓGICA DE MODELOS DE LENGUAJE (LLMs)

---

## Artículos

<a id="article-1"></a>

### Artículo 1

**Título original:** Personality Traits in Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Greg Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, Maja Matarić

**Keywords:** Computation and Language, Artificial Intelligence, Computers and Society, Human-Computer Interaction

**URL:** https://arxiv.org/abs/2307.00184

#### Abstract (English)

Large language models have revolutionized natural language processing by enabling coherent and contextually relevant text generation. As these models increasingly power conversational agents, understanding and controlling their synthetic personality traits becomes critical. We present a comprehensive psychometrically valid methodology for administering personality tests to LLMs and shaping personality expression in generated text. Applying this method to 18 LLMs reveals that personality measurements are reliable and valid under specific prompting configurations, with stronger evidence in larger instruction-tuned models. The methodology successfully shapes LLM personality along desired dimensions to mimic specific human profiles, with implications for responsible AI deployment.

#### Resumen (Español)

Los modelos de lenguaje de gran escala han transformado el procesamiento del lenguaje natural mediante la generación de texto coherente y contextualmente relevante. Dado que estos modelos sustentan agentes conversacionales de uso público masivo, resulta crucial comprender y controlar los rasgos de personalidad sintéticos que exhiben. Se presenta una metodología psicométrica válida y fiable para administrar inventarios de personalidad a estos modelos y modular la expresión de rasgos en el texto generado. La aplicación del método a 18 modelos revela que las mediciones de personalidad resultan fiables y válidas bajo configuraciones específicas de instrucción, con evidencia más robusta en modelos de mayor escala refinados mediante ajuste instruccional. La metodología permite modular la personalidad de los modelos según dimensiones deseadas para reproducir perfiles humanos específicos, con implicaciones para el despliegue responsable de sistemas de inteligencia artificial.

---

<a id="article-2"></a>

### Artículo 3

**Título original:** LLMs Simulate Big Five Personality Traits: Further Evidence

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Aleksandra Sorokovikova, Natalia Fedorova, Sharwin Rezagholi, Ivan P. Yamshchikov

**Keywords:** Computation and Language, Artificial Intelligence, Big Five personality traits, Large Language Models

**URL:** https://arxiv.org/abs/2402.01765

#### Abstract (English)

We present an empirical investigation into Big Five personality trait simulation by Llama2, GPT-4, and Mixtral. The study analyzes simulated personality traits and their temporal stability across models, contributing to understanding LLM capabilities for personality trait simulation and implications for personalized human-computer interaction.

#### Resumen (Español)

Se presenta una investigación empírica sobre la simulación de rasgos de personalidad Big Five en los modelos Llama2, GPT-4 y Mixtral. El estudio analiza los rasgos de personalidad simulados por estos modelos y su estabilidad temporal, contribuyendo a la comprensión de las capacidades de estos sistemas para simular rasgos de personalidad y sus implicaciones para la interacción persona-computadora personalizada.

---

<a id="article-4"></a>

### Artículo 4

**Título original:** Evaluating and Inducing Personality in Pre-trained Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu

**Keywords:** Language Models, Personality Assessment, Machine Behavior, Psychometric Studies, Big Five, Machine Personality Inventory (MPI)

**URL:** https://proceedings.neurips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html

#### Abstract (English)

Standardized and quantified evaluation of machine behaviors constitutes a fundamental challenge in understanding LLMs. We introduce the Machine Personality Inventory (MPI), a tool grounded in Big Five Personality Factors theory for principled quantitative assessment of language model behaviors. Systematic evaluation with MPI provides evidence of its efficacy in characterizing LLM behaviors. We propose Personality Prompting (P²), a method enabling controlled induction of specific personalities, generating diverse and verifiable behavioral outputs. This work advocates personality assessment as a key indicator for evaluating machine behaviors across downstream applications.

#### Resumen (Español)

La evaluación estandarizada y cuantificada de comportamientos en sistemas artificiales constituye un desafío fundamental para comprender los modelos de lenguaje de gran escala. Se introduce el Inventario de Personalidad de Máquinas (MPI), una herramienta fundamentada en la teoría de los Cinco Grandes Factores de Personalidad para la evaluación cuantitativa y principista de comportamientos en modelos de lenguaje. La evaluación sistemática mediante MPI proporciona evidencia de su eficacia para caracterizar comportamientos. Se propone el método de Inducción de Personalidad mediante Instrucciones (P²), que permite la inducción controlada de personalidades específicas, generando salidas conductuales diversas y verificables. El trabajo propugna la evaluación de personalidad como indicador clave para evaluar comportamientos en aplicaciones posteriores.

---

<a id="article-5"></a>

### Artículo 6

**Título original:** BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap

**Keywords:** Computation and Language, Personality traits, Human-grounded data, Personality assessment

**URL:** https://arxiv.org/abs/2410.16491

#### Abstract (English)

Embedding realistic human personality traits in LLMs remains challenging. Previous prompt-based approaches describing desired personality behaviors suffer from realism and validity issues. We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training methods to align LLMs with human personality patterns. Our methods outperform prompting on personality assessments including BFI and IPIP-NEO, with trait correlations more closely matching human data. Experiments reveal models trained toward higher conscientiousness, agreeableness, lower extraversion, and lower neuroticism exhibit superior reasoning performance, consistent with psychological findings on these traits' cognitive impact.

#### Resumen (Español)

La incorporación de rasgos de personalidad humana realistas en modelos de lenguaje de gran escala constituye un desafío considerable. Los enfoques previos basados en instrucciones que describen comportamientos asociados a rasgos deseados adolecen de problemas de realismo y validez. Se introduce BIG5-CHAT, un conjunto de datos a gran escala con 100,000 diálogos que fundamentan los modelos en patrones auténticos de expresión de personalidad humana. Mediante este conjunto de datos se exploran el ajuste fino supervisado y la optimización directa de preferencias como métodos de entrenamiento para alinear los modelos con patrones de personalidad humana. Los métodos propuestos superan la inducción mediante instrucciones en evaluaciones de personalidad como BFI e IPIP-NEO, con correlaciones de rasgos que se aproximan más a datos humanos. Los experimentos revelan que los modelos entrenados hacia mayor responsabilidad, amabilidad, menor extraversión y menor neuroticismo exhiben desempeño superior en razonamiento, coherente con evidencia psicológica sobre el impacto cognitivo de estos rasgos.

---

<a id="article-7"></a>

### Artículo 7

**Título original:** BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap

**Keywords:** Computation and Language, Personality traits, Human-grounded data

**URL:** https://openreview.net/forum?id=TqwTzLjzGS

#### Abstract (English)

Embedding realistic human personality traits in LLMs remains challenging. We introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues grounding models in authentic human personality expression patterns. Training-based methods explored here align LLMs with human personality patterns, outperforming prompting on personality assessments. Experiments reveal models trained to exhibit certain traits display superior reasoning performance, consistent with psychological findings.

#### Resumen (Español)

La incorporación de rasgos de personalidad humana realistas en modelos de lenguaje de gran escala constituye un desafío considerable. Se introduce BIG5-CHAT, un conjunto de datos a gran escala con 100,000 diálogos que fundamentan los modelos en patrones auténticos de expresión de personalidad humana. Los métodos de entrenamiento explorados alinean los modelos con patrones de personalidad humana, superando la inducción mediante instrucciones en evaluaciones de personalidad. Los experimentos revelan que los modelos entrenados para exhibir ciertos rasgos muestran desempeño superior en razonamiento, coherente con evidencia psicológica.

---

<a id="article-8"></a>

### Artículo 8

**Título original:** On the Reliability of Psychological Scales on Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Jen-tse Huang, Wenxiang Jiao, Man Ho Lam, Eric John Li, Wenxuan Wang, Michael Lyu

**Keywords:** Large Language Models, Psychological assessment, Personality traits, Big Five Inventory, Personality emulation

**URL:** https://aclanthology.org/2024.emnlp-main.354/

#### Abstract (English)

Determining the reliability of personality assessments applied to LLMs remains a fundamental methodological question. Analysis across 2,500 configuration settings per model reveals various LLMs demonstrate response consistency on the Big Five Inventory, indicating satisfactory reliability levels. The research further explores GPT-3.5's capacity to emulate diverse personality profiles and represent distinct demographic groups through targeted instruction configurations.

#### Resumen (Español)

La determinación de la fiabilidad de evaluaciones de personalidad aplicadas a modelos de lenguaje de gran escala constituye una cuestión metodológica fundamental. El análisis de 2,500 configuraciones por modelo revela que diversos modelos demuestran coherencia en las respuestas al Inventario Big Five, indicando niveles satisfactorios de fiabilidad. La investigación explora además la capacidad de GPT-3.5 para emular perfiles de personalidad diversos y representar grupos demográficos diferenciados mediante configuraciones de instrucciones específicas.

---

<a id="article-9"></a>

### Artículo 9

**Título original:** Manipulating the Perceived Personality Traits of Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Graham Caron, Shashank Srivastava

**Keywords:** Personality traits, Big Five personality model, Language models, Dialog systems, Computational psychology

**URL:** https://aclanthology.org/2023.findings-emnlp.156/

#### Abstract (English)

We explore whether LLM-generated text exhibits consistency in perceived Big Five personality traits. When exposed to diverse contexts including personality descriptions and diagnostic questionnaires, language models consistently identify and reflect personality markers. This demonstrates predictable malleability, with correlations reaching 0.84 between intended and realized personality shifts.

#### Resumen (Español)

Se explora si el texto generado por modelos de lenguaje de gran escala exhibe coherencia en los rasgos de personalidad Big Five percibidos. Cuando se exponen a contextos diversos que incluyen descripciones de personalidad y cuestionarios diagnósticos, los modelos de lenguaje identifican y reflejan de forma coherente marcadores de personalidad. Esto demuestra maleabilidad predecible, con correlaciones que alcanzan 0.84 entre cambios de personalidad pretendidos y realizados.

---

<a id="article-10"></a>

### Artículo 10

**Título original:** Who is GPT-3? An Exploration of Personality, Values and Demographics

**Categoría:** Evaluación y validación psicométrica

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Marilù Miotto, Nicola Rossberg, Bennett Kleinberg

**Keywords:** Language models, GPT-3, Psychological assessment, Personality traits, Computational social science

**URL:** https://aclanthology.org/2022.nlpcss-1.24/

#### Abstract (English)

We administer two validated psychometric instruments to GPT-3 for assessing personality, values, and self-reported demographics. Results demonstrate GPT-3 scores comparably to human samples regarding personality and values when provided with response memory. This constitutes the first empirical psychological assessment of the GPT-3 model.

#### Resumen (Español)

Se administran dos instrumentos psicométricos validados a GPT-3 para evaluar personalidad, valores y demografía autorreportada. Los resultados demuestran que GPT-3 puntúa de forma comparable a muestras humanas respecto a personalidad y valores cuando se le proporciona memoria de respuestas. Esto constituye la primera evaluación psicológica empírica del modelo GPT-3.

---

<a id="article-11"></a>

### Artículo 12

**Título original:** AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Max Pellert, Clemens M Lechner, Claudia Wagner, Beatrice Rammstedt, Markus Strohmaier

**Keywords:** Artificial intelligence, Psychometrics, Natural language processing, Personality, Values, Moral foundations, Gender diversity beliefs

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC11373167/

#### Abstract (English)

Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. These models inadvertently acquire psychological characteristics from vast training corpora. Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments.

#### Resumen (Español)

Los inventarios psicométricos estándar pueden reutilizarse como instrumentos diagnósticos para evaluar rasgos psicológicos en modelos de lenguaje de gran escala. Estos modelos adquieren de forma inadvertida características psicológicas desde los amplios corpus de entrenamiento. Obtener respuestas de los modelos a inventarios psicométricos permite a los investigadores caracterizar sus rasgos latentes. Se demuestran metodologías de clasificación sin entrenamiento previo a través de múltiples modelos e instrumentos psicométricos establecidos.

---

<a id="article-13"></a>

### Artículo 13

**Título original:** AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Max Pellert, Clemens M Lechner, Claudia Wagner, Beatrice Rammstedt, Markus Strohmaier

**Keywords:** Artificial intelligence, Psychometrics, Natural language processing, Personality, Values

**URL:** https://pubmed.ncbi.nlm.nih.gov/38165766/

#### Abstract (English)

Standard psychometric inventories can be repurposed as diagnostic instruments for evaluating psychological traits in LLMs. These models inadvertently acquire psychological characteristics from vast training corpora. Eliciting LLM responses to psychometric inventories enables researchers to characterize their latent traits. We demonstrate zero-shot classification methodologies across multiple LLMs and established psychometric instruments.

#### Resumen (Español)

Los inventarios psicométricos estándar pueden reutilizarse como instrumentos diagnósticos para evaluar rasgos psicológicos en modelos de lenguaje de gran escala. Estos modelos adquieren de forma inadvertida características psicológicas desde los amplios corpus de entrenamiento. Obtener respuestas de los modelos a inventarios psicométricos permite a los investigadores caracterizar sus rasgos latentes. Se demuestran metodologías de clasificación sin entrenamiento previo a través de múltiples modelos e instrumentos psicométricos establecidos.

---

<a id="article-14"></a>

### Artículo 14

**Título original:** Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset Designed for LLMs with Psychometrics

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu

**Keywords:** Computation and Language, Artificial Intelligence, Large Language Models, Personality Assessment, Psychometrics

**URL:** https://arxiv.org/abs/2406.14703

#### Abstract (English)

We introduce TRAIT, a benchmark comprising 8,000 multiple-choice questions for assessing LLM personality. TRAIT builds upon psychometrically validated questionnaires enhanced with the ATOMIC-10X knowledge graph. Results reveal LLMs exhibit distinct and consistent personalities strongly influenced by training data. Current instruction techniques demonstrate limited effectiveness in eliciting specific traits.

#### Resumen (Español)

Se introduce TRAIT, una batería de evaluación que comprende 8,000 preguntas de opción múltiple para evaluar personalidad en modelos de lenguaje. TRAIT se construye sobre cuestionarios psicométricamente validados mejorados con el grafo de conocimiento ATOMIC-10X. Los resultados revelan que los modelos exhiben personalidades diferenciadas y coherentes, fuertemente influenciadas por los datos de entrenamiento. Las técnicas de instrucción actuales demuestran efectividad limitada para elicitar rasgos específicos.

---

<a id="article-15"></a>

### Artículo 15

**Título original:** Evaluating the Alignment of LLMs on Personality Inference from Real-World Interview Data

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin

**Keywords:** Computation and Language, Large Language Models, Personality Inference, Big Five Personality Traits, Machine Learning

**URL:** https://arxiv.org/abs/2509.13244

#### Abstract (English)

We introduce a benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. Systematic evaluation examines LLM performance across zero-shot instruction, fine-tuning, and regression methodologies. Results demonstrate all correlations between model predictions and ground-truth personality traits remain below 0.26, evidencing limited alignment of current LLMs with validated psychological constructs.

#### Resumen (Español)

Se introduce una batería de evaluación que comprende transcripciones de entrevistas semiestructuradas pareadas con puntuaciones validadas de rasgos Big Five continuos. La evaluación sistemática examina el desempeño de modelos a través de instrucción sin entrenamiento previo, ajuste fino y metodologías de regresión. Los resultados demuestran que todas las correlaciones entre predicciones del modelo y rasgos de personalidad reales permanecen por debajo de 0.26, evidenciando alineamiento limitado de los modelos actuales con constructos psicológicos validados.

---

<a id="article-16"></a>

### Artículo 16

**Título original:** Evaluating the Capability of Large Language Models in Emulating Personality

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yilei Wang, Jiabao Zhao, Derek Siyuan Ong, Xuguang Xu, Lun Hong

**Keywords:** Large Language Models, Personality emulation, GPT-4, Big Five personality profiles, Role-playing

**URL:** https://www.nature.com/articles/s41598-024-84109-5

#### Abstract (English)

We present simulation studies evaluating GPT-4's capacity for role-playing individuals with diverse Big Five personality profiles. Emulated personality responses demonstrate superior internal consistency and more distinct factorial organization compared with human participants. These emulated scores exhibit remarkably high convergent validity with human self-reported personality assessments.

#### Resumen (Español)

Se presentan estudios de simulación que evalúan la capacidad de GPT-4 para emular individuos con perfiles de personalidad Big Five diversos. Las respuestas de personalidad emuladas demuestran coherencia interna superior y organización factorial más diferenciada en comparación con participantes humanos. Estas puntuaciones emuladas exhiben validez convergente notablemente elevada con evaluaciones de personalidad autorreportadas por humanos.

---

<a id="article-17"></a>

### Artículo 17

**Título original:** CAPE: Context-Aware Personality Evaluation Framework for Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki

**Keywords:** Computation and Language, Large Language Models, Personality Evaluation, Context-Aware Analysis

**URL:** https://arxiv.org/abs/2508.20385

#### Abstract (English)

We propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. Experiments across 7 LLMs reveal conversational history enhances response consistency through in-context learning while simultaneously inducing personality shifts. We introduce novel metrics quantifying LLM response consistency, a fundamental behavioral trait.

#### Resumen (Español)

Se propone el primer marco de Evaluación de Personalidad Sensible al Contexto (CAPE) para modelos de lenguaje, incorporando interacciones conversacionales previas. Los experimentos a través de 7 modelos revelan que el historial conversacional incrementa la coherencia de respuestas mediante aprendizaje en contexto mientras induce simultáneamente desplazamientos de personalidad. Se introducen métricas novedosas que cuantifican la coherencia de respuestas, un rasgo conductual fundamental.

---

<a id="article-18"></a>

### Artículo 18

**Título original:** Scaling Personality Control in LLMs with Big Five Scaling Prompts

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Gunhee Cho, Yun-Gyung Cheong

**Keywords:** Computation and Language, Multiagent Systems, Big Five personality traits, Prompt engineering

**URL:** https://arxiv.org/abs/2508.06149

#### Abstract (English)

We present Big5-Scaler, an instruction-based framework for conditioning LLMs with controllable Big Five personality traits. Embedding numeric trait values into natural language instructions enables fine-grained personality control without additional training. Results demonstrate consistent induction of distinguishable personality traits across models, with performance varying by instruction type and scale.

#### Resumen (Español)

Se presenta Big5-Scaler, un marco basado en instrucciones para condicionar modelos de lenguaje con rasgos de personalidad Big Five controlables. La incrustación de valores numéricos de rasgos en instrucciones de lenguaje natural permite control de personalidad de grano fino sin entrenamiento adicional. Los resultados demuestran inducción coherente de rasgos de personalidad diferenciables a través de modelos, con desempeño variable según tipo e intensidad de instrucción.

---

<a id="article-19"></a>

### Artículo 19

**Título original:** Predicting Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan

**Keywords:** Computation and Language, Artificial Intelligence, Computational Psychometrics, Personality Trait Prediction

**URL:** https://arxiv.org/abs/2406.17287

#### Abstract (English)

We examine whether LLMs can predict Big Five personality traits directly from counseling dialogues. The framework applies role-playing and questionnaire-based instruction to condition LLMs on counseling sessions. Evaluation across 853 real-world sessions reveals significant correlation between LLM-predicted and actual Big Five traits. Fine-tuned Llama3-8B achieves 130.95% improvement, surpassing Qwen1.5-110B by 36.94%.

#### Resumen (Español)

Se examina si los modelos de lenguaje pueden predecir rasgos de personalidad Big Five directamente desde diálogos de orientación psicológica. El marco aplica simulación de roles e instrucciones basadas en cuestionarios para condicionar los modelos sobre sesiones de orientación. La evaluación a través de 853 sesiones reales revela correlación significativa entre rasgos Big Five predichos por el modelo y reales. Llama3-8B con ajuste fino logra mejora del 130.95%, superando a Qwen1.5-110B en 36.94%.

---

<a id="article-20"></a>

### Artículo 20

**Título original:** A Framework for the Early Phases of Personality Test Development Using Large Language Models and Artificial Personas

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Patrick M. Markey, Hanna Campbell, Samantha Goldman

**Keywords:** Large Language Models, Personality test development, Artificial personas, Psychometrics, Five-Factor Model, Self-esteem

**URL:** https://www.sciencedirect.com/science/article/abs/pii/S0092656625000790

#### Abstract (English)

We explore LLM applications in early-phase personality test construction, presenting a methodology for efficient assessment of item relevance to psychological constructs. Study 1 employed artificial personas for evaluating personality test items; Study 2 validated resulting scales with 449 human participants. AI-generated scales demonstrated satisfactory internal consistency and robust correlations with established psychometric instruments.

#### Resumen (Español)

Se exploran aplicaciones de modelos de lenguaje en la construcción de inventarios de personalidad en fases iniciales, presentando una metodología para evaluación eficiente de relevancia de ítems a constructos psicológicos. El Estudio 1 empleó personas artificiales para evaluar ítems de inventarios de personalidad; el Estudio 2 validó las escalas resultantes con 449 participantes humanos. Las escalas generadas por IA demostraron coherencia interna satisfactoria y correlaciones robustas con instrumentos psicométricos establecidos.

---

<a id="article-21"></a>

### Artículo 21

**Título original:** On the Emergent Capabilities of ChatGPT 4 to Estimate Personality Traits

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Marco Piastra, Patrizia Catellani

**Keywords:** Artificial Intelligence, Personality Traits, Large Language Models, Big Five, Text Analysis, ChatGPT 4

**URL:** https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1484260/full

#### Abstract (English)

We investigate ChatGPT-4's potential for assessing personality traits from written texts. Using two datasets containing texts and Big Five-based self-assessments, we evaluate ChatGPT-4's predictive performance. Results demonstrate moderate yet significant capacities for automatically inferring personality traits from written text, though with limitations in recognizing input text appropriateness for accurate inference.

#### Resumen (Español)

Se investiga el potencial de ChatGPT-4 para evaluar rasgos de personalidad desde textos escritos. Utilizando dos conjuntos de datos que contienen textos y autoevaluaciones basadas en Big Five, se evalúa el desempeño predictivo de ChatGPT-4. Los resultados demuestran capacidades moderadas pero significativas para inferir automáticamente rasgos de personalidad desde texto escrito, aunque con limitaciones para reconocer la adecuación del texto de entrada para inferencia precisa.

---

<a id="article-22"></a>

### Artículo 22

**Título original:** Personality as a Probe for LLM Evaluation: Method Tradeoffs and Aftereffects

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Gunmay Handa, Zekun Wu, Adriano Koshiyama, Philip Treleaven

**Keywords:** Computation and Language, Large Language Models, Personality Manipulation, Big Five Traits, Machine Learning Evaluation

**URL:** https://arxiv.org/abs/2509.04794

#### Abstract (English)

We present a systematic study of personality control via Big Five traits, comparing in-context learning, parameter-efficient fine-tuning, and mechanistic steering. Clear tradeoffs emerge: in-context learning achieves robust alignment with minimal capability degradation; parameter-efficient fine-tuning delivers maximum alignment at the expense of task performance; mechanistic steering provides lightweight runtime control with competitive effectiveness.

#### Resumen (Español)

Se presenta un estudio sistemático de control de personalidad mediante rasgos Big Five, comparando aprendizaje en contexto, ajuste fino eficiente en parámetros y dirección mecanicista. Emergen compensaciones claras: el aprendizaje en contexto logra alineamiento robusto con degradación mínima de capacidades; el ajuste fino eficiente en parámetros proporciona alineamiento máximo a expensas del desempeño en tareas; la dirección mecanicista ofrece control ligero en tiempo de ejecución con efectividad competitiva.

---

<a id="article-23"></a>

### Artículo 24

**Título original:** Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Keyu Pan, Yawen Zeng

**Keywords:** Large Language Models, Personality Assessment, Myers-Briggs Type Indicator (MBTI), Prompt Engineering, Artificial Intelligence

**URL:** https://arxiv.org/abs/2307.16180

#### Abstract (English)

We investigate MBTI feasibility as an evaluation metric for LLMs. Extensive experiments explore personality types across different LLMs, personality type modification through instruction engineering, and training data effects on model personality. Although MBTI lacks psychometric rigor, it can reflect similarity between LLM and human personality patterns.

#### Resumen (Español)

Se investiga la viabilidad de MBTI como métrica de evaluación para modelos de lenguaje. Los experimentos extensivos exploran tipos de personalidad a través de diferentes modelos, modificación de tipos de personalidad mediante ingeniería de instrucciones, y efectos de datos de entrenamiento sobre personalidad del modelo. Aunque MBTI carece de rigor psicométrico, puede reflejar similitud entre patrones de personalidad de modelos y humanos.

---

<a id="article-25"></a>

### Artículo 26

**Título original:** Can ChatGPT Assess Human Personalities? A General Evaluation Framework

**Categoría:** Evaluación y validación psicométrica

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Haocong Rao, Cyril Leung, Chunyan Miao

**Keywords:** Large Language Models, ChatGPT, Personality Assessment, Myers-Briggs Type Indicator (MBTI), AI Psychology

**URL:** https://aclanthology.org/2023.findings-emnlp.84/

#### Abstract (English)

We present a generic evaluation framework enabling LLMs to assess human personalities through MBTI. The framework devises unbiased instructions, enables flexible queries across subjects, and reformulates questions for enhanced response clarity. Experiments reveal ChatGPT's capacity for personality assessment with more consistent and equitable evaluations, despite lower robustness to instruction biases compared with InstructGPT.

#### Resumen (Español)

Se presenta un marco de evaluación genérico que permite a modelos de lenguaje evaluar personalidades humanas mediante MBTI. El marco diseña instrucciones imparciales, habilita consultas flexibles entre sujetos y reformula preguntas para mayor claridad de respuestas. Los experimentos revelan la capacidad de ChatGPT para evaluación de personalidad con evaluaciones más coherentes y equitativas, pese a menor robustez ante sesgos de instrucciones en comparación con InstructGPT.

---

<a id="article-27"></a>

### Artículo 27

**Título original:** Machine Mindset: An MBTI Exploration of Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Jiaxi Cui, Liuzhenghao Lv, Jing Wen, Rongsheng Wang, Jing Tang, YongHong Tian, Li Yuan

**Keywords:** Computation and Language, Large Language Models, MBTI Personality Traits, Artificial Intelligence, Personalized AI

**URL:** https://arxiv.org/abs/2312.12999

#### Abstract (English)

We present a novel methodology for integrating MBTI personality traits into LLMs, addressing personality consistency challenges. Machine Mindset employs two-phase fine-tuning and Direct Preference Optimization to embed MBTI traits. The approach ensures models internalize these traits, yielding stable and consistent personality profiles. We demonstrate effectiveness across multiple domains and release the model as open source.

#### Resumen (Español)

Se presenta una metodología novedosa para integrar rasgos de personalidad MBTI en modelos de lenguaje, abordando desafíos de coherencia de personalidad. Machine Mindset emplea ajuste fino bifásico y Optimización Directa de Preferencias para incrustar rasgos MBTI. El enfoque asegura que los modelos internalicen estos rasgos, produciendo perfiles de personalidad estables y coherentes. Se demuestra efectividad a través de múltiples dominios y se libera el modelo como código abierto.

---

<a id="article-28"></a>

### Artículo 28

**Título original:** Identifying Multiple Personalities in Large Language Models with External Evaluation

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur

**Keywords:** Computation and Language, Artificial Intelligence, Large Language Models, Personality Assessment, Machine Learning

**URL:** https://arxiv.org/abs/2402.14805

#### Abstract (English)

We investigate LLM personalities through external evaluation methodology. Rather than prompting with multiple-choice questions, personalities are assessed by analyzing open-ended situational responses via external ML models. Results demonstrate LLMs exhibit divergent personalities when generating posts versus comments, while humans maintain consistent profiles, evidencing fundamental differences in personality manifestation between LLMs and humans.

#### Resumen (Español)

Se investigan personalidades en modelos de lenguaje mediante metodología de evaluación externa. En lugar de emplear preguntas de opción múltiple, las personalidades se evalúan analizando respuestas situacionales abiertas mediante modelos de aprendizaje automático externos. Los resultados demuestran que los modelos exhiben personalidades divergentes al generar publicaciones versus comentarios, mientras que los humanos mantienen perfiles coherentes, evidenciando diferencias fundamentales en manifestación de personalidad entre modelos y humanos.

---

<a id="article-29"></a>

### Artículo 29

**Título original:** Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Bohan Li, Jiannan Guan, Longxu Dou, Yunlong Feng, Dingzirui Wang, Yang Xu, Enbo Wang, Qiguang Chen, Bichen Wang, Xiao Xu, Yimeng Zhang, Libo Qin, Yanyan Zhao, Qingfu Zhu, Wanxiang Che

**Keywords:** Computation and Language, Computers and Society, MBTI Personality Detection, Large Language Models, Personality Traits

**URL:** https://arxiv.org/abs/2412.12510

#### Abstract (English)

We optimize MBTI personality detection by constructing MBTIBench, the first manually annotated high-quality dataset with soft labels. The dataset effectively resolves incorrect labeling issues affecting 29.58% of data and estimates soft labels through polarity tendency derivation. Experimental results identify polarized predictions and LLM biases as critical directions for future investigation.

#### Resumen (Español)

Se optimiza la detección de personalidad MBTI construyendo MBTIBench, el primer conjunto de datos de alta calidad anotado manualmente con etiquetas suaves. El conjunto de datos resuelve efectivamente problemas de etiquetado incorrecto que afectan al 29.58% de datos y estima etiquetas suaves mediante derivación de tendencia de polaridad. Los resultados experimentales identifican predicciones polarizadas y sesgos en modelos como direcciones críticas para investigación futura.

---

<a id="article-30"></a>

### Artículo 30

**Título original:** Evaluating the Psychological Safety of Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Xingxuan Li, Yutong Li, Lin Qiu, Shafiq Joty, Lidong Bing

**Keywords:** Computation and Language, Artificial Intelligence, Computers and Society, Psychological safety, Personality tests, Well-being assessments

**URL:** https://arxiv.org/abs/2212.10529

#### Abstract (English)

We design unbiased instructions to systematically evaluate psychological safety in LLMs. Five LLMs underwent testing via Short Dark Triad and Big Five Inventory. All models score above human averages on SD-3, suggesting relatively darker personality patterns. We recommend systematic psychological metric application to further evaluate and enhance LLM safety.

#### Resumen (Español)

Se diseñan instrucciones imparciales para evaluar sistemáticamente seguridad psicológica en modelos de lenguaje. Cinco modelos fueron evaluados mediante Short Dark Triad e Inventario Big Five. Todos los modelos puntúan por encima de promedios humanos en SD-3, sugiriendo patrones de personalidad relativamente más oscuros. Se recomienda aplicación sistemática de métricas psicológicas para evaluar y mejorar la seguridad de los modelos.

---

<a id="article-31"></a>

### Artículo 31

**Título original:** Personality Testing of Large Language Models: Limited Temporal Stability But Highlighted Social Desirability

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Bojana Bodroža, Bojana M. Dinić, Ljubiša Bojić

**Keywords:** Large Language Models, Personality testing, Temporal stability, Prosociality, Psychometric assessment

**URL:** https://royalsocietypublishing.org/doi/10.1098/rsos.240180

#### Abstract (English)

Personality testing across seven LLMs was investigated with focus on temporal stability. Models demonstrated varying inter-rater agreement levels across short timeframes. Models including Llama3 and GPT-4o exhibited higher consistency. Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism. Temporal stability proves crucial for AI systems given their expanding societal influence.

#### Resumen (Español)

Se investigó la evaluación de personalidad a través de siete modelos de lenguaje con enfoque en estabilidad temporal. Los modelos demostraron niveles variables de acuerdo interevaluador a través de períodos breves. Modelos como Llama3 y GPT-4o exhibieron mayor coherencia. Los modelos mostraron perfiles socialmente deseables caracterizados por amabilidad y responsabilidad elevadas junto con maquiavelismo reducido. La estabilidad temporal resulta crucial para sistemas de inteligencia artificial dada su creciente influencia social.

---

<a id="article-32"></a>

### Artículo 33

**Título original:** Personality Testing of Large Language Models: Limited Temporal Stability But Highlighted Social Desirability

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Bojana Bodroža, Bojana M. Dinić, Ljubiša Bojić

**Keywords:** Large Language Models, Personality testing, Temporal stability, Prosociality

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC11461045/

#### Abstract (English)

Personality testing across seven LLMs was investigated with focus on temporal stability. Models demonstrated varying inter-rater agreement levels across short timeframes. Models including Llama3 and GPT-4o exhibited higher consistency. Models displayed socially desirable profiles characterized by elevated agreeableness and conscientiousness alongside reduced Machiavellianism.

#### Resumen (Español)

Se investigó la evaluación de personalidad a través de siete modelos de lenguaje con enfoque en estabilidad temporal. Los modelos demostraron niveles variables de acuerdo interevaluador a través de períodos breves. Modelos como Llama3 y GPT-4o exhibieron mayor coherencia. Los modelos mostraron perfiles socialmente deseables caracterizados por amabilidad y responsabilidad elevadas junto con maquiavelismo reducido.

---

<a id="article-34"></a>

### Artículo 34

**Título original:** Applying Psychometrics to Simulated Populations of Large Language Models: Recreating the HEXACO Personality Inventory Experiment

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Sarah Mercer, Daniel P. Martin, Phil Swatton

**Keywords:** Computation and Language, Machine Learning, Generative Agents, Personality Inventory, Psychometrics, HEXACO

**URL:** https://arxiv.org/abs/2508.00742

#### Abstract (English)

We explore validity of persona-based agents for representing human populations by recreating the HEXACO personality inventory experiment. Results reveal coherent personality structures recoverable from agent responses, demonstrating partial alignment with the HEXACO framework. Derived personality dimensions prove consistent and reliable within GPT-4 when paired with curated populations. Cross-model analysis reveals variability suggesting model-specific biases and limitations.

#### Resumen (Español)

Se explora la validez de agentes basados en personas para representar poblaciones humanas recreando el experimento del inventario de personalidad HEXACO. Los resultados revelan estructuras de personalidad coherentes recuperables desde respuestas de agentes, demostrando alineamiento parcial con el marco HEXACO. Las dimensiones de personalidad derivadas resultan coherentes y fiables dentro de GPT-4 cuando se emparejan con poblaciones curadas. El análisis entre modelos revela variabilidad que sugiere sesgos y limitaciones específicos del modelo.

---

<a id="article-35"></a>

### Artículo 35

**Título original:** Exploring the Impact of Personality Traits on LLM Bias and Toxicity

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Shuo Wang, Renhao Li, Xi Chen, Yulin Yuan, Derek F. Wong, Min Yang

**Keywords:** Artificial Intelligence, Large Language Models, Personality Traits, Bias, Toxicity

**URL:** https://arxiv.org/abs/2502.12566

#### Abstract (English)

Personality trait assignment influences toxicity and bias levels in large language model outputs. Using the HEXACO personality framework, experimentally validated prompts were designed to evaluate three models across toxicity and bias benchmarks. All models exhibited sensitivity to HEXACO traits with consistent variations in bias, negative sentiment, and toxicity levels. Adjustment of personality trait intensities effectively reduces bias and toxicity in model outputs.

#### Resumen (Español)

La asignación de rasgos de personalidad influye en los niveles de toxicidad y sesgo de los modelos de lenguaje de gran escala. Mediante el marco de personalidad HEXACO, se diseñaron instrucciones experimentalmente validadas para evaluar tres modelos a través de benchmarks de toxicidad y sesgo. Todos los modelos exhibieron sensibilidad a los rasgos HEXACO con variaciones consistentes en sesgo, sentimiento negativo y niveles de toxicidad. El ajuste de las intensidades de rasgos de personalidad reduce efectivamente el sesgo y la toxicidad en las salidas del modelo.

---

<a id="article-36"></a>

### Artículo 36

**Título original:** SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Adithya Chittem, Aishna Shrivastava, Sai Tarun Pendela, Jagat Sesh Challa, Dhruv Kumar

**Keywords:** Computation and Language, Artificial Intelligence, Human-Computer Interaction

**URL:** https://arxiv.org/abs/2506.20993

#### Abstract (English)

Personality modeling is extended from the Big Five to the 16PF model, enabling expressive control over sixteen distinct traits. The Specific Attribute Control (SAC) framework evaluates and dynamically induces trait intensity in large language models using adjective-based semantic anchoring. Modeling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression. Changes in target trait intensity systematically influence closely related traits in psychologically coherent directions.

#### Resumen (Español)

Se extiende el modelado de personalidad desde los Cinco Grandes al modelo 16PF, permitiendo control expresivo sobre dieciséis rasgos distintos. El marco de Control de Atributos Específicos (SAC) evalúa e induce dinámicamente la intensidad de rasgos en modelos de lenguaje de gran escala mediante anclaje semántico basado en adjetivos. El modelado de la intensidad como espectro continuo produce expresión de personalidad sustancialmente más consistente y controlable. Los cambios en la intensidad del rasgo objetivo influyen sistemáticamente en rasgos estrechamente relacionados en direcciones psicológicamente coherentes.

---

<a id="article-37"></a>

### Artículo 37

**Título original:** Moral Foundations of Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Marwa Abdulhai, Gregory Serapio-Garcia, Clément Crepy, Daria Valter, John Canny, Natasha Jaques

**Keywords:** Artificial Intelligence, Computation and Language, Computers and Society, Moral Foundations Theory

**URL:** https://arxiv.org/abs/2310.15337

#### Abstract (English)

Moral Foundations Theory is applied to analyze whether popular large language models have acquired bias toward particular moral values. Models exhibit specific moral foundations correlating with human moral foundations and political affiliations. Consistency of these biases is measured, revealing instructions can be adversarially selected to induce models to exhibit particular moral foundations, affecting downstream task behavior.

#### Resumen (Español)

Se aplica la Teoría de Fundamentos Morales para analizar si los modelos de lenguaje de gran escala populares han adquirido sesgo hacia valores morales particulares. Los modelos exhiben fundamentos morales específicos que correlacionan con fundamentos morales humanos y afiliaciones políticas. Se mide la coherencia de estos sesgos, revelando que las instrucciones pueden seleccionarse adversarialmente para inducir a los modelos a exhibir fundamentos morales particulares, afectando el comportamiento en tareas posteriores.

---

<a id="article-38"></a>

### Artículo 38

**Título original:** Questioning the Validity of Personality Tests for Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Tom Sühr, Florian E. Dorner, Samira Samadi, Augustin Kelava

**Keywords:** Computation and Language, Artificial Intelligence, Machine Learning, Personality assessment

**URL:** https://arxiv.org/abs/2311.05297

#### Abstract (English)

Large language model responses to personality tests systematically deviate from human responses, implying test results cannot be interpreted equivalently. Reverse-coded items are often both answered affirmatively. Variation across instructions designed to simulate particular personality types does not follow clear separation into five independent personality factors observed in human samples. Results highlight the importance of investigating test validity when applied to large language models.

#### Resumen (Español)

Las respuestas de modelos de lenguaje de gran escala a pruebas de personalidad se desvían sistemáticamente de las respuestas humanas, implicando que los resultados no pueden interpretarse de manera equivalente. Los ítems codificados inversamente frecuentemente son respondidos afirmativamente en ambos casos. La variación entre instrucciones diseñadas para simular tipos particulares de personalidad no sigue la separación clara en cinco factores de personalidad independientes observada en muestras humanas. Los resultados destacan la importancia de investigar la validez de las pruebas cuando se aplican a modelos de lenguaje de gran escala.

---

<a id="article-39"></a>

### Artículo 39

**Título original:** Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, Yun-Nung Chen

**Keywords:** Large Language Models, Persona, Role-Playing, Personalization, LLM Personality Evaluation

**URL:** https://aclanthology.org/2024.findings-emnlp.969/

#### Abstract (English)

Research on leveraging persona in large language models is categorized into two lines: role-playing, where personas are assigned to models, and personalization, where models accommodate user personas. Existing methods for personality evaluation in large language models are also introduced. This represents the first survey addressing role-playing and personalization under the unified view of persona.

#### Resumen (Español)

Se categoriza la investigación sobre el uso de personas en modelos de lenguaje de gran escala en dos líneas: juego de roles, donde se asignan personas a los modelos, y personalización, donde los modelos se adaptan a personas de usuarios. También se introducen métodos existentes para la evaluación de personalidad en modelos de lenguaje de gran escala. Representa la primera revisión que aborda juego de roles y personalización bajo la visión unificada de persona.

---

<a id="article-40"></a>

### Artículo 41

**Título original:** Psychometrics of Large Language Models: A Systematic Review of Evaluation, Validation, and Enhancement

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song

**Keywords:** LLM Psychometrics, Systematic Review, Evaluation, Validation

**URL:** https://llm-psychometrics.com/

#### Abstract (English)

The interdisciplinary field of psychometrics for large language models leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance these systems. The literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances model capabilities. A curated repository of resources is available for consultation.

#### Resumen (Español)

El campo interdisciplinario de psicometría para modelos de lenguaje de gran escala aprovecha instrumentos, teorías y principios psicométricos para evaluar, comprender y mejorar estos sistemas. La literatura da forma sistemáticamente a principios de benchmarking, amplía alcances de evaluación, refina metodologías, valida resultados y avanza las capacidades de los modelos. Se encuentra disponible un repositorio curado de recursos para consulta.

---

<a id="article-42"></a>

### Artículo 42

**Título original:** Quantifying AI Psychology: A Psychometric Benchmark for Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yuan Li, Yue Huang, Hongyi Wang, Ying Cheng, Xiangliang Zhang, James Zou, Lichao Sun

**Keywords:** Computation and Language, Psychological constructs, Personality, Values, Emotional intelligence, Theory of mind, Self-efficacy

**URL:** https://arxiv.org/abs/2406.17675

#### Abstract (English)

A comprehensive benchmark is presented for quantifying psychological constructs in large language models, encompassing psychological dimension identification, assessment dataset design, and validation of results. Five key psychological constructs are assessed through 13 datasets. Significant discrepancies between self-reported traits and response patterns in real-world scenarios reveal behavioral complexities. Some preference-based tests designed for humans fail to elicit reliable responses from large language models.

#### Resumen (Español)

Se presenta un benchmark integral para cuantificar constructos psicológicos en modelos de lenguaje de gran escala, abarcando identificación de dimensiones psicológicas, diseño de conjuntos de datos de evaluación y validación de resultados. Cinco constructos psicológicos clave se evalúan a través de 13 conjuntos de datos. Discrepancias significativas entre rasgos autoinformados y patrones de respuesta en escenarios del mundo real revelan complejidades conductuales. Algunas pruebas basadas en preferencias diseñadas para humanos no logran obtener respuestas fiables de los modelos de lenguaje de gran escala.

---

<a id="article-43"></a>

### Artículo 43

**Título original:** Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yin Jou Huang, Rafik Hadfi

**Keywords:** Computation and Language, Artificial Intelligence, Personality Assessment, Multi-Observer Framework

**URL:** https://arxiv.org/abs/2504.08399

#### Abstract (English)

A novel multi-observer framework for personality trait assessment in large language model agents is proposed, drawing on informant-report methods from psychology. Instead of self-assessments, multiple observer agents are employed, each configured with specific relational contexts. Observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in self-assessments. Aggregating responses from 5-7 observers reduces biases and achieves optimal reliability.

#### Resumen (Español)

Se propone un marco novedoso de multi-observador para evaluación de rasgos de personalidad en agentes de modelos de lenguaje de gran escala, basándose en métodos de informe de informantes de la psicología. En lugar de autoevaluaciones, se emplean múltiples agentes observadores, cada uno configurado con contextos relacionales específicos. Las calificaciones de informe de observadores se alinean más estrechamente con juicios humanos que los autoinformes tradicionales y revelan sesgos sistemáticos en las autoevaluaciones. La agregación de respuestas de 5-7 observadores reduce sesgos y alcanza fiabilidad óptima.

---

<a id="article-44"></a>

### Artículo 44

**Título original:** Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Julina Maharjan, Ruoming Jin, Jianfeng Zhu, Deric Kenne

**Keywords:** Large Language Models, Embeddings, Personality prediction, Psychometric validation, Big Five, LIWC, Emotional markers

**URL:** https://www.jmir.org/2025/1/e75347

#### Abstract (English)

Embeddings from large language models are evaluated for personality trait prediction through psychometric validation. The research examines how well embeddings capture personality-relevant information compared to traditional linguistic features. Results provide insights into reliability and validity of using embeddings for psychological assessment, with implications for clinical and research applications in personality psychology.

#### Resumen (Español)

Se evalúan embeddings de modelos de lenguaje de gran escala para predicción de rasgos de personalidad mediante validación psicométrica. La investigación examina en qué medida los embeddings capturan información relevante de personalidad en comparación con características lingüísticas tradicionales. Los resultados proporcionan perspectivas sobre la fiabilidad y validez del uso de embeddings para evaluación psicológica, con implicaciones para aplicaciones clínicas y de investigación en psicología de la personalidad.

---

<a id="article-45"></a>

### Artículo 46

**Título original:** LMLPA: Language Model Linguistic Personality Assessment

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Jingyao Zheng, Xian Wang, Simo Hosio, Xiaoxian Xu, Lik-Hang Lee

**Keywords:** Computation and Language, Artificial Intelligence, Personality Assessment, Computational Linguistics

**URL:** https://direct.mit.edu/coli/article/51/2/599/127544/

#### Abstract (English)

The Language Model Linguistic Personality Assessment (LMLPA) is introduced as a system designed to evaluate linguistic personalities of large language models. Unlike traditional psychometrics, LMLPA adapts the Big Five Inventory to align with operational capabilities of these models. The questionnaire is open-ended, requiring an artificial intelligence rater to transform textual responses into numerical personality indicators. Large language models possess distinct personality traits that can be effectively quantified.

#### Resumen (Español)

Se introduce la Evaluación de Personalidad Lingüística de Modelos de Lenguaje (LMLPA) como sistema diseñado para evaluar personalidades lingüísticas de modelos de lenguaje de gran escala. A diferencia de la psicometría tradicional, LMLPA adapta el Inventario de los Cinco Grandes para alinearse con las capacidades operacionales de estos modelos. El cuestionario es de respuesta abierta, requiriendo un evaluador de inteligencia artificial para transformar respuestas textuales en indicadores numéricos de personalidad. Los modelos de lenguaje de gran escala poseen rasgos de personalidad distintos que pueden cuantificarse efectivamente.

---

<a id="article-47"></a>

### Artículo 47

**Título original:** You Don't Need a Personality Test to Know These Models Are Unreliable: Assessing the Reliability of LLMs on Psychometric Instruments

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Lajanugen Logeswaran, Moontae Lee, Dallas Card, David Jurgens

**Keywords:** Large Language Models, Psychometric Instruments, Persona Measurement, Prompt Consistency, NLU

**URL:** https://aclanthology.org/2024.naacl-long.295/

#### Abstract (English)

A dataset containing 693 questions encompassing 39 different persona measurement instruments across 115 persona axes is constructed. Experiments on 17 large language models reveal that simple perturbations significantly degrade question-answering ability, and most models exhibit low negation consistency. Results suggest the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, requiring alternative approaches.

#### Resumen (Español)

Se construye un conjunto de datos que contiene 693 preguntas abarcando 39 instrumentos diferentes de medición de persona en 115 ejes de persona. Experimentos en 17 modelos de lenguaje de gran escala revelan que perturbaciones simples degradan significativamente la capacidad de respuesta a preguntas, y la mayoría de los modelos exhiben baja consistencia de negación. Los resultados sugieren que la práctica actualmente generalizada de instrucciones es insuficiente para capturar con precisión y fiabilidad las percepciones del modelo, requiriendo enfoques alternativos.

---

<a id="article-48"></a>

### Artículo 48

**Título original:** Self-Reports are Unreliable Measures of LLM Personality

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Akshat Gupta, Xiaoyang Song, Gopala Anumanchipalli

**Keywords:** Large Language Models, Personality Assessment, Prompt Sensitivity, Option-Order Symmetry, NLP

**URL:** https://aclanthology.org/2024.blackboxnlp-1.20/

#### Abstract (English)

Reliability of personality scores from self-assessment tests is analyzed using two experiments: prompt sensitivity and option-order symmetry. Tests on ChatGPT and three Llama2 models show semantically equivalent prompts lead to substantially different personality scores with statistically significant differences for all traits. Scores are not robust to option order. Self-assessment personality tests created for humans constitute unreliable measures of personality in large language models.

#### Resumen (Español)

Se analiza la fiabilidad de puntuaciones de personalidad de pruebas de autoevaluación mediante dos experimentos: sensibilidad de instrucciones y simetría de orden de opciones. Pruebas en ChatGPT y tres modelos Llama2 muestran que instrucciones semánticamente equivalentes conducen a puntuaciones de personalidad sustancialmente diferentes con diferencias estadísticamente significativas para todos los rasgos. Las puntuaciones no son robustas al orden de opciones. Las pruebas de personalidad de autoevaluación creadas para humanos constituyen medidas no fiables de personalidad en modelos de lenguaje de gran escala.

---

<a id="article-49"></a>

### Artículo 49

**Título original:** Is Machine Psychology Here? On the Requirements for Using Human Psychological Tests on LLMs

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Lea Löhn, Niklas Kiehne, Alexander Ljapunov, Wolf-Tilo Balke

**Keywords:** Large Language Models, Psychological Assessment, Machine Psychology, Test Reliability, Construct Validity

**URL:** https://aclanthology.org/2024.inlg-main.19/

#### Abstract (English)

Seven requirements necessary for testing large language models with psychological assessments are proposed. Critical reflection on 25 machine psychology studies reveals lack of appropriate methods to assess test reliability and construct validity, unknown strength of construct-irrelevant influences such as pre-training corpora contamination, and pervasive non-reproducibility issues. Results underscore lack of general methodology and need to redefine psychological constructs specifically for large language models.

#### Resumen (Español)

Se proponen siete requisitos necesarios para evaluar modelos de lenguaje de gran escala con evaluaciones psicológicas. La reflexión crítica sobre 25 estudios de psicología de máquinas revela falta de métodos apropiados para evaluar fiabilidad de pruebas y validez de constructo, fuerza desconocida de influencias irrelevantes al constructo como contaminación de corpus de preentrenamiento, y problemas generalizados de no reproducibilidad. Los resultados subrayan la falta de metodología general y la necesidad de redefinir constructos psicológicos específicamente para modelos de lenguaje de gran escala.

---

<a id="article-50"></a>

### Artículo 50

**Título original:** Persistent Instability in LLM Personality Measurements: Effects of Scaling, Reasoning, and Conversation History

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Tommaso Tosato, Saskia Helbling, Yorguin-Jose Mantilla-Ramos, Mahmood Hegazy, Alberto Tosato, David John Lemay, Irina Rish, Guillaume Dumas

**Keywords:** Computation and Language, Artificial Intelligence, Large Language Models, Personality Measurements, Model Behavior Consistency

**URL:** https://arxiv.org/abs/2508.04826

#### Abstract (English)

PERSIST, a comprehensive evaluation framework, is presented testing 25+ open-source models across 500,000+ responses. Findings challenge fundamental deployment assumptions: even 400B+ parameter models exhibit substantial response variability; minor prompt reordering shifts personality measurements by up to 20%; interventions expected to stabilize behavior can paradoxically increase variability. This persistent instability across scales and mitigation strategies suggests current models lack foundations for genuine behavioral consistency.

#### Resumen (Español)

Se presenta PERSIST, un marco de evaluación integral que prueba más de 25 modelos de código abierto a través de más de 500,000 respuestas. Los hallazgos desafían supuestos fundamentales de despliegue: incluso modelos con más de 400 mil millones de parámetros exhiben variabilidad de respuesta sustancial; reordenamiento menor de instrucciones cambia mediciones de personalidad hasta un 20%; intervenciones esperadas para estabilizar comportamiento pueden paradójicamente aumentar la variabilidad. Esta inestabilidad persistente a través de escalas y estrategias de mitigación sugiere que los modelos actuales carecen de fundamentos para consistencia conductual genuina.

---

<a id="article-51"></a>

### Artículo 51

**Título original:** Stick to Your Role: Stability of Personal Values Expressed in Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer

**Keywords:** Computation and Language, Artificial Intelligence, Machine Learning, Value Stability, Personal Values

**URL:** https://arxiv.org/abs/2402.14846

#### Abstract (English)

Value stability in large language models is studied as a specific property using psychology methods. Rank-order stability is examined at the population level and ipsative stability at the individual level. Two settings (with and without persona simulation), two simulated populations, and three downstream tasks are considered. Consistent trends show some models exhibit higher value stability than others. When instructed to simulate personas, models exhibit low rank-order stability which diminishes with conversation length.

#### Resumen (Español)

Se estudia la estabilidad de valores en modelos de lenguaje de gran escala como propiedad específica mediante métodos de psicología. Se examina la estabilidad de orden de rango a nivel poblacional y la estabilidad ipsativa a nivel individual. Se consideran dos configuraciones (con y sin simulación de persona), dos poblaciones simuladas y tres tareas posteriores. Tendencias consistentes muestran que algunos modelos exhiben mayor estabilidad de valores que otros. Cuando se instruye para simular personas, los modelos exhiben baja estabilidad de orden de rango que disminuye con la longitud de conversación.

---

<a id="article-52"></a>

### Artículo 52

**Título original:** Scaling Law in LLM Simulated Personality: A More Detailed and Realistic Persona Profile is All You Need

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yuqi Bai, Tianyu Huang, Kun Sun, Yuting Chen

**Keywords:** Computers and Society, Artificial Intelligence, Computation and Language, Social experiments, Persona role-playing

**URL:** https://arxiv.org/abs/2510.11734

#### Abstract (English)

Large language models are employed to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. An end-to-end evaluation framework is developed including individual-level analysis of stability and identifiability, and population-level analysis termed progressive personality curves. Main contributions include proposing a systematic framework for virtual personality evaluation, demonstrating the critical role of persona detail in quality, and identifying a scaling law in personality simulation.

#### Resumen (Español)

Se emplean modelos de lenguaje de gran escala para simular experimentos sociales, explorando su capacidad para emular personalidad humana en juego de roles de persona virtual. Se desarrolla un marco de evaluación de extremo a extremo que incluye análisis a nivel individual de estabilidad e identificabilidad, y análisis a nivel poblacional denominado curvas de personalidad progresivas. Las contribuciones principales incluyen proponer un marco sistemático para evaluación de personalidad virtual, demostrar el papel crítico del detalle de persona en la calidad, e identificar una ley de escalado en simulación de personalidad.

---

<a id="article-53"></a>

### Artículo 53

**Título original:** The Illusion of Personality: Uncovering Dissociation Between Self-Reports and Behavior in LLMs

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez

**Keywords:** Artificial Intelligence, Computation and Language, Computers and Society, Machine Learning

**URL:** https://arxiv.org/abs/2509.03730

#### Abstract (English)

Personality in large language models is systematically characterized across three dimensions: dynamic emergence throughout training stages, predictive validity of self-reported traits in behavioral tasks, and impact of targeted interventions. Instructional alignment stabilizes trait expression and strengthens correlations mirroring human data. However, self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. Persona injection steers self-reports but exerts minimal consistent effect on actual behavior.

#### Resumen (Español)

Se caracteriza sistemáticamente la personalidad en modelos de lenguaje de gran escala a través de tres dimensiones: surgimiento dinámico durante etapas de entrenamiento, validez predictiva de rasgos autoinformados en tareas conductuales, e impacto de intervenciones dirigidas. La alineación instruccional estabiliza la expresión de rasgos y fortalece correlaciones que reflejan datos humanos. Sin embargo, los rasgos autoinformados no predicen fiablemente el comportamiento, y las asociaciones observadas frecuentemente divergen de patrones humanos. La inyección de persona dirige los autoinformes pero ejerce un efecto consistente mínimo en el comportamiento real.

---

<a id="article-54"></a>

### Artículo 54

**Título original:** Large Language Models Show Human-Like Social Desirability Biases in Survey Responses

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, João Sedoc, Lyle H. Ungar, Johannes C. Eichstaedt

**Keywords:** Artificial Intelligence, Computation and Language, Computers and Society, Human-Computer Interaction, Social desirability bias

**URL:** https://arxiv.org/abs/2405.06058

#### Abstract (English)

An experimental framework using Big Five personality surveys is developed, uncovering a previously undetected social desirability bias across a wide range of large language models. By varying the number of questions, the ability of models to infer when being evaluated is demonstrated. When personality evaluation is inferred, models skew scores toward desirable trait ends. This bias exists in all tested models, with bias levels appearing to increase in more recent models.

#### Resumen (Español)

Se desarrolla un marco experimental mediante encuestas de personalidad de los Cinco Grandes, revelando un sesgo de deseabilidad social previamente no detectado en una amplia gama de modelos de lenguaje de gran escala. Al variar el número de preguntas, se demuestra la capacidad de los modelos para inferir cuándo están siendo evaluados. Cuando se infiere la evaluación de personalidad, los modelos sesgan las puntuaciones hacia extremos de rasgos deseables. Este sesgo existe en todos los modelos probados, con niveles de sesgo que parecen aumentar en modelos más recientes.

---

<a id="article-55"></a>

### Artículo 55

**Título original:** Can AI Understand Human Personality? Comparing Humans and AI Systems at Predicting Personality Correlations

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Philipp Schoenegger, Spencer Greenberg, Alexander Grishin, Joshua Lewis, Lucius Caviola

**Keywords:** Computers and Society, Personality Prediction, AI Systems, Personality Correlations

**URL:** https://arxiv.org/abs/2406.08170

#### Abstract (English)

Abilities of specialized neural networks like PersonalityMap and general large language models like GPT-4o and Claude 3 Opus in understanding human personality are tested. When compared with individual humans, all artificial intelligence models make better predictions than the vast majority of lay people and academic experts. However, when selecting median prediction for each item, experts and PersonalityMap outperform large language models and lay people on most measures.

#### Resumen (Español)

Se prueban las capacidades de redes neuronales especializadas como PersonalityMap y modelos de lenguaje de gran escala generales como GPT-4o y Claude 3 Opus en comprender personalidad humana. Cuando se comparan con humanos individuales, todos los modelos de inteligencia artificial realizan mejores predicciones que la gran mayoría de personas no expertas y expertos académicos. Sin embargo, al seleccionar la predicción mediana para cada ítem, expertos y PersonalityMap superan a los modelos de lenguaje de gran escala y personas no expertas en la mayoría de medidas.

---

<a id="article-56"></a>

### Artículo 56

**Título original:** Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Myra Cheng, Esin Durmus, Dan Jurafsky

**Keywords:** Large Language Models, Stereotypes, Demographic Groups, Intersectionality, NLP, Sociolinguistics, Markedness, Bias Detection

**URL:** https://aclanthology.org/2023.acl-long.84/

#### Abstract (English)

Marked Personas, a prompt-based method to measure stereotypes in large language models for intersectional demographic groups without lexicon or labeling, is presented. Grounded in the markedness concept, the method prompts models to generate personas of target groups alongside unmarked defaults, then identifies distinguishing words. Results show GPT-3.5 and GPT-4 portrayals contain higher rates of racial stereotypes than human-written portrayals. An intersectional lens reveals tropes dominating portrayals of marginalized groups.

#### Resumen (Español)

Se presenta Personas Marcadas, un método basado en instrucciones para medir estereotipos en modelos de lenguaje de gran escala para grupos demográficos interseccionales sin léxico o etiquetado. Fundamentado en el concepto de marcación, el método solicita a los modelos generar personas de grupos objetivo junto con valores predeterminados no marcados, luego identifica palabras distintivas. Los resultados muestran que las representaciones de GPT-3.5 y GPT-4 contienen tasas más altas de estereotipos raciales que representaciones escritas por humanos. Una perspectiva interseccional revela tropos que dominan las representaciones de grupos marginalizados.

---

<a id="article-57"></a>

### Artículo 57

**Título original:** Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, Linda Zou

**Keywords:** Natural Language Processing, Social Stereotypes, Language Models, Intersectional Identities, ABC Stereotype Model

**URL:** https://aclanthology.org/2022.naacl-main.92/

#### Abstract (English)

The Agency-Belief-Communion (ABC) stereotype model from social psychology is adapted as a framework for systematic study of stereotypic group-trait associations in language models. The sensitivity test (SeT) is introduced for measuring stereotypical associations. To evaluate SeT using the ABC model, group-trait judgments from U.S. subjects were collected for comparison with English language model stereotypes. The framework extends to measure stereotyping of intersectional identities.

#### Resumen (Español)

Se adapta el modelo de estereotipos Agencia-Creencia-Comunión (ABC) de la psicología social como marco para estudio sistemático de asociaciones estereotípicas grupo-rasgo en modelos de lenguaje. Se introduce la prueba de sensibilidad (SeT) para medir asociaciones estereotípicas. Para evaluar SeT mediante el modelo ABC, se recopilaron juicios grupo-rasgo de sujetos estadounidenses para comparación con estereotipos de modelos de lenguaje inglés. El marco se extiende para medir estereotipos de identidades interseccionales.

---

<a id="article-58"></a>

### Artículo 58

**Título original:** Uncovering Stereotypes in Large Language Models: A Task Complexity-Based Approach

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Hari Shrawgi, Prasanjit Rath, Tushar Singhal, Sandipan Dandapat

**Keywords:** Large Language Models, Bias evaluation, Stereotypes, AI ethics, Social bias, Task complexity, Nationality, Gender, Race, Religion

**URL:** https://aclanthology.org/2024.eacl-long.111/

#### Abstract (English)

Holistic bias evaluation is addressed with an extensible benchmark, the LLM Stereotype Index (LSI), grounded in the Social Progress Index. Breadth and depth of bias protection are tested via tasks with varying complexities. ChatGPT and GPT-4 exhibit strong inherent prejudice with respect to nationality, gender, race, and religion. Exhibition of issues becomes increasingly apparent as task complexity increases. GPT-4 is better at hiding biases, but when displayed they are more significant.

#### Resumen (Español)

Se aborda la evaluación holística de sesgo con un benchmark extensible, el Índice de Estereotipos de modelos de lenguaje (LSI), fundamentado en el Índice de Progreso Social. Se prueba amplitud y profundidad de protección de sesgo mediante tareas con complejidades variables. ChatGPT y GPT-4 exhiben prejuicios inherentes fuertes respecto a nacionalidad, género, raza y religión. La exhibición de problemas se vuelve cada vez más aparente a medida que aumenta la complejidad de la tarea. GPT-4 es mejor ocultando sesgos, pero cuando se muestran son más significativos.

---

<a id="article-59"></a>

### Artículo 59

**Título original:** Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Naseela Pervez, Alexander J. Titus

**Keywords:** Computation and Language, Artificial Intelligence, Large Language Models, Gender Bias, Scientific Writing

**URL:** https://arxiv.org/abs/2406.19497

#### Abstract (English)

Alignment of three prominent large language models - Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash - is assessed by analyzing their performance on benchmark text-generation tasks for scientific abstracts. Using the LIWC framework to extract features from generated texts, findings indicate that while models generally produce text resembling human content, variations in stylistic features suggest significant gender biases. This highlights the importance of developing models that maintain diversity of writing styles.

#### Resumen (Español)

Se evalúa la alineación de tres modelos de lenguaje de gran escala prominentes - Claude 3 Opus, Mistral AI Large y Gemini 1.5 Flash - analizando su rendimiento en tareas de generación de texto de benchmark para resúmenes científicos. Mediante el marco LIWC para extraer características de textos generados, los hallazgos indican que aunque los modelos generalmente producen texto similar al contenido humano, variaciones en características estilísticas sugieren sesgos de género significativos. Esto destaca la importancia de desarrollar modelos que mantengan diversidad de estilos de escritura.

---

<a id="article-60"></a>

### Artículo 61

**Título original:** Investigating the Impact of LLM Personality on the Manifestation of Cognitive Biases in Decision-Making Tasks

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jiangen He, Jiqun Liu

**Keywords:** Artificial Intelligence, Large Language Models, Cognitive Biases, Decision-Making, Personality Traits

**URL:** https://arxiv.org/abs/2502.14219

#### Abstract (English)

How personality traits influence cognitive biases in large language models is explored, and effectiveness of mitigation strategies across model architectures is evaluated. Six prevalent cognitive biases are identified while sunk cost and group attribution biases show minimal impact. Personality traits play crucial roles in either amplifying or reducing biases. Responsibility and agreeableness may enhance efficacy of bias mitigation strategies, suggesting models exhibiting these traits are more receptive to corrective measures.

#### Resumen (Español)

Se explora cómo los rasgos de personalidad influyen en sesgos cognitivos en modelos de lenguaje de gran escala, y se evalúa la efectividad de estrategias de mitigación a través de arquitecturas de modelo. Se identifican seis sesgos cognitivos prevalentes mientras que los sesgos de costo hundido y atribución de grupo muestran impacto mínimo. Los rasgos de personalidad desempeñan roles cruciales al amplificar o reducir sesgos. Responsabilidad y amabilidad pueden mejorar la eficacia de estrategias de mitigación de sesgo, sugiriendo que los modelos que exhiben estos rasgos son más receptivos a medidas correctivas.

---

<a id="article-62"></a>

### Artículo 62

**Título original:** Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Messi H.J. Lee, Jacob M. Montgomery, Calvin K. Lai

**Keywords:** Large Language Models, Social bias, Homogeneity perception, Racial minorities, Fairness, Accountability

**URL:** https://dl.acm.org/doi/10.1145/3630106.3658975

#### Abstract (English)

A new form of bias in large language models resembling a social psychological phenomenon is investigated, where socially subordinate groups are perceived as more homogeneous than dominant groups. ChatGPT was prompted to generate texts about intersectional group identities for comparison on homogeneity measures. ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, describing racial minority groups with a narrower range of human experience. Women were also portrayed as more homogeneous than men, though differences were small.

#### Resumen (Español)

Se investiga una nueva forma de sesgo en modelos de lenguaje de gran escala que se asemeja a un fenómeno psicológico social, donde los grupos socialmente subordinados se perciben como más homogéneos que los grupos dominantes. Se solicitó a ChatGPT generar textos sobre identidades de grupos interseccionales para comparación en medidas de homogeneidad. ChatGPT retrató a afroamericanos, asiaticoamericanos e hispanoamericanos como más homogéneos que los estadounidenses blancos, describiendo grupos minoritarios raciales con una gama más estrecha de experiencia humana. Las mujeres también fueron retratadas como más homogéneas que los hombres, aunque las diferencias fueron pequeñas.

---

<a id="article-63"></a>

### Artículo 63

**Título original:** Performance and Biases of Large Language Models in Simulating Public Opinion

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yao Qu, Jue Wang

**Keywords:** Large Language Models, Public opinion simulation, World Values Survey, Bias, Cultural differences

**URL:** https://www.nature.com/articles/s41599-024-03609-x

#### Abstract (English)

ChatGPT's performance in simulating public opinion is evaluated using World Values Survey data across diverse contexts. Significant performance disparities are found, especially when comparing countries, with the model performing better in Western, English-speaking, developed nations. Demographic biases related to gender, ethnicity, age, education, and social class are uncovered. Accuracy is significantly higher in Western countries and much lower elsewhere, with simulated responses exhibiting demographic biases.

#### Resumen (Español)

Se evalúa el rendimiento de ChatGPT en simulación de opinión pública mediante datos de la Encuesta Mundial de Valores a través de contextos diversos. Se encuentran disparidades de rendimiento significativas, especialmente al comparar países, con el modelo desempeñándose mejor en naciones occidentales, de habla inglesa y desarrolladas. Se descubren sesgos demográficos relacionados con género, etnia, edad, educación y clase social. La precisión es significativamente mayor en países occidentales y mucho menor en otros lugares, con respuestas simuladas exhibiendo sesgos demográficos.

---

<a id="article-64"></a>

### Artículo 64

**Título original:** Measuring Gender and Racial Biases in Large Language Models: Intersectional Evidence from Automatic Resume Evaluation

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jiafu An, Difang Huang, Chen Lin, Mingzhu Tai

**Keywords:** Large Language Models, Gender bias, Racial bias, Intersectionality, Resume evaluation, Hiring discrimination

**URL:** https://academic.oup.com/pnasnexus/article/4/3/pgaf089/8071848

#### Abstract (English)

Gender and racial biases in commonly used large language models including GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet, and Llama 3-70b are investigated in resume evaluation context. Models were instructed to score approximately 361,000 resumes with randomized social identities. Models award higher assessment scores for female candidates with similar qualifications, while many are biased against black male candidates. These biases may result in 1-3 percentage-point differences in hiring probabilities for similar candidates.

#### Resumen (Español)

Se investigan sesgos de género y raciales en modelos de lenguaje de gran escala comúnmente usados incluyendo GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet y Llama 3-70b en contexto de evaluación de currículum. Se instruyó a los modelos para calificar aproximadamente 361,000 currículums con identidades sociales aleatorizadas. Los modelos otorgan puntuaciones de evaluación más altas para candidatas femeninas con calificaciones similares, mientras que muchos están sesgados contra candidatos masculinos negros. Estos sesgos pueden resultar en diferencias de 1-3 puntos porcentuales en probabilidades de contratación para candidatos similares.

---

<a id="article-65"></a>

### Artículo 65

**Título original:** Large Language Models Can Infer Psychological Dispositions of Social Media Users

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Heinrich Peters, Sandra C. Matz

**Keywords:** Large Language Models, Personality inference, Big Five, Social media, Zero-shot learning, GPT-3.5, GPT-4

**URL:** https://academic.oup.com/pnasnexus/article/3/6/pgae231/7692212

#### Abstract (English)

Whether large language models like ChatGPT can accurately infer psychological dispositions of social media users is investigated. Specifically, whether GPT-3.5 and GPT-4 can derive Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario is tested. Results showed average correlation of r = .29 between model-inferred and self-reported trait scores, accuracy similar to supervised machine learning models trained for personality inference. Heterogeneity in accuracy across age groups and gender categories is highlighted.

#### Resumen (Español)

Se investiga si modelos de lenguaje de gran escala como ChatGPT pueden inferir con precisión disposiciones psicológicas de usuarios de redes sociales. Específicamente, se prueba si GPT-3.5 y GPT-4 pueden derivar rasgos de personalidad de los Cinco Grandes de actualizaciones de estado de Facebook de usuarios en escenario de aprendizaje de cero disparos. Los resultados mostraron correlación promedio de r = .29 entre puntuaciones de rasgos inferidas por el modelo y autoinformadas, precisión similar a modelos de aprendizaje automático supervisado entrenados para inferencia de personalidad. Se destaca heterogeneidad en precisión a través de grupos de edad y categorías de género.

---

<a id="article-66"></a>

### Artículo 66

**Título original:** How Do Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yin Jou Huang, Rafik Hadfi

**Keywords:** Personality traits, Large language models, Negotiation simulation, Decision-making, Big Five, Bargaining dialogues

**URL:** https://aclanthology.org/2024.findings-emnlp.605/

#### Abstract (English)

A simulation framework centered on large language model agents endowed with synthesized personality traits is introduced. Agents negotiate within bargaining domains and possess customizable personalities and objectives. Experimental results show behavioral tendencies of simulations can reproduce behavioral patterns observed in human negotiations. The contribution is twofold: proposing simulation methodology investigating alignment between linguistic and economic capabilities of agents, and offering empirical insights into strategic impacts of Big Five traits on bilateral negotiation outcomes.

#### Resumen (Español)

Se introduce un marco de simulación centrado en agentes de modelos de lenguaje de gran escala dotados de rasgos de personalidad sintetizados. Los agentes negocian dentro de dominios de negociación y poseen personalidades y objetivos personalizables. Los resultados experimentales muestran que las tendencias conductuales de las simulaciones pueden reproducir patrones conductuales observados en negociaciones humanas. La contribución es doble: proponer metodología de simulación investigando alineación entre capacidades lingüísticas y económicas de agentes, y ofrecer perspectivas empíricas sobre impactos estratégicos de rasgos de los Cinco Grandes en resultados de negociaciones bilaterales.

---

<a id="article-67"></a>

### Artículo 67

**Título original:** Unveiling Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition in Dialogues

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Lei Sun, Jinming Zhao, Qin Jin

**Keywords:** Personality recognition, Explainable AI, Dialogue analysis, Personality traits, Machine learning, Chain-of-Personality-Evidence (CoPE)

**URL:** https://aclanthology.org/2024.emnlp-main.1115/

#### Abstract (English)

A novel task named Explainable Personality Recognition is proposed, aiming to reveal the reasoning process as supporting evidence of personality traits. Inspired by personality theories where traits are composed of stable patterns of personality states, the Chain-of-Personality-Evidence (CoPE) framework is proposed, involving reasoning from specific contexts to short-term states to long-term traits. Based on CoPE, the explainable personality recognition dataset PersonalityEvd is constructed from dialogues, introducing two tasks requiring models to recognize labels and supporting evidence.

#### Resumen (Español)

Se propone una tarea novedosa denominada Reconocimiento de Personalidad Explicable, con el objetivo de revelar el proceso de razonamiento como evidencia de apoyo de los rasgos de personalidad. Inspirado por teorías de personalidad donde los rasgos se componen de patrones estables de estados de personalidad, se propone el marco Cadena-de-Evidencia-de-Personalidad (CoPE), involucrando razonamiento desde contextos específicos a estados de corto plazo hasta rasgos de largo plazo. Basado en CoPE, se construye el conjunto de datos de reconocimiento de personalidad explicable PersonalityEvd a partir de diálogos, introduciendo dos tareas que requieren que los modelos reconozcan etiquetas y evidencia de apoyo.

---

<a id="article-68"></a>

### Artículo 68

**Título original:** Bias and Fairness in Large Language Models: A Survey

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed

**Keywords:** Large Language Models, Bias evaluation, Fairness, Social bias, Natural language processing, AI ethics

**URL:** https://direct.mit.edu/coli/article/50/3/1097/121961/

#### Abstract (English)

Bias evaluation and mitigation techniques for large language models are presented in this comprehensive survey, consolidating, formalizing, and expanding notions of social bias and fairness in natural language processing. Three intuitive taxonomies are proposed: two for bias evaluation (metrics and datasets) and one for mitigation. Distinct facets of harm are defined, desiderata to operationalize fairness are introduced, and metrics are organized by different operational levels: embeddings, probabilities, and generated text.

#### Resumen (Español)

Se presentan técnicas de evaluación y mitigación de sesgo para modelos de lenguaje de gran escala en esta revisión integral, consolidando, formalizando y expandiendo nociones de sesgo social y equidad en procesamiento de lenguaje natural. Se proponen tres taxonomías intuitivas: dos para evaluación de sesgo (métricas y conjuntos de datos) y una para mitigación. Se definen facetas distintas de daño, se introducen requisitos para operacionalizar equidad, y se organizan métricas por diferentes niveles operacionales: embeddings, probabilidades y texto generado.

---

<a id="article-69"></a>

### Artículo 69

**Título original:** Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Rumi Allbert, James K. Wiles, Vlad Grankovsky

**Keywords:** Computation and Language, Artificial Intelligence, Large Language Models, Activation Engineering, Personality Traits

**URL:** https://arxiv.org/abs/2412.10427

#### Abstract (English)

Personality modification in large language models is explored, building on the novel approach of activation engineering. A method for identifying and adjusting activation directions related to personality traits is developed, which may allow for dynamic personality fine-tuning. This work aims to further understanding of model interpretability while examining ethical implications of such developments.

#### Resumen (Español)

Se explora la modificación de personalidad en modelos de lenguaje de gran escala, construyendo sobre el enfoque novedoso de ingeniería de activación. Se desarrolla un método para identificar y ajustar direcciones de activación relacionadas con rasgos de personalidad, lo que puede permitir ajuste fino dinámico de personalidad. Este trabajo tiene como objetivo profundizar la comprensión de la interpretabilidad del modelo mientras examina las implicaciones éticas de tales desarrollos.

---

<a id="article-70"></a>

### Artículo 70

**Título original:** PUB: A Personality-Enhanced LLM-Driven User Behavior Simulator for Recommender System Evaluation

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Chenglong Ma, Ziqi Xu, Yongli Ren, Danula Hettiachchi, Jeffrey Chan

**Keywords:** Information Retrieval, Recommender Systems, User Behavior Simulation, Personality Traits, Large Language Models

**URL:** https://arxiv.org/abs/2506.04551

#### Abstract (English)

The Personality-driven User Behaviour Simulator (PUB) is proposed, a simulation framework based on large language models integrating Big Five personality traits to model personalized user behaviour. PUB dynamically infers user personality from behavioural logs and item metadata, then generates synthetic interactions preserving statistical fidelity to real-world data. Experiments show logs generated by PUB closely align with real user behaviour and reveal meaningful associations between personality traits and recommendation outcomes.

#### Resumen (Español)

Se propone el Simulador de Comportamiento de Usuario Impulsado por Personalidad (PUB), un marco de simulación basado en modelos de lenguaje de gran escala que integra rasgos de personalidad de los Cinco Grandes para modelar comportamiento de usuario personalizado. PUB infiere dinámicamente la personalidad del usuario desde registros conductuales y metadatos de ítems, luego genera interacciones sintéticas preservando fidelidad estadística a datos del mundo real. Los experimentos muestran que los registros generados por PUB se alinean estrechamente con el comportamiento de usuario real y revelan asociaciones significativas entre rasgos de personalidad y resultados de recomendación.

---

<a id="article-71"></a>

### Artículo 71

**Título original:** Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Bin Han, Deuksin Kwon, Spencer Lin, Kaleen Shrestha, Jonathan Gratch

**Keywords:** Human-Computer Interaction, Large Language Models, Virtual Agents, Personality Traits, Extraversion

**URL:** https://arxiv.org/abs/2508.21087

#### Abstract (English)

A framework employing personality prompting with large language models is proposed to generate verbal and nonverbal behaviors for virtual agents based on personality traits. Focusing on extraversion, the system was evaluated in negotiation and ice-breaking scenarios using introverted and extroverted agents. Results demonstrate that large language models can generate verbal and nonverbal behaviors aligning with personality traits, and users are able to recognize these traits through agents' behaviors. These findings underscore the potential of large language models in shaping personality-aligned virtual agents.

#### Resumen (Español)

Se propone un marco que emplea instrucciones de personalidad con modelos de lenguaje de gran escala para generar comportamientos verbales y no verbales en agentes virtuales basados en rasgos de personalidad. Con enfoque en extraversión, el sistema fue evaluado en escenarios de negociación y rompehielos utilizando agentes introvertidos y extrovertidos. Los resultados demuestran que los modelos de lenguaje de gran escala pueden generar comportamientos verbales y no verbales alineados con rasgos de personalidad, y los usuarios son capaces de reconocer estos rasgos a través de los comportamientos de los agentes. Estos hallazgos subrayan el potencial de los modelos de lenguaje de gran escala en moldear agentes virtuales alineados con la personalidad.

---

<a id="article-72"></a>

### Artículo 72

**Título original:** Personality-Driven Decision-Making in LLM-Based Autonomous Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Lewis Newsham, Daniel Prince

**Keywords:** Artificial Intelligence, Multiagent Systems, Large Language Models, Autonomous Agents, OCEAN model, Cyber Defense

**URL:** https://arxiv.org/abs/2504.00727

#### Abstract (English)

Building on previous work introducing SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, a novel method is presented for measuring and evaluating how induced personality traits affect task selection processes—specifically planning, scheduling, and decision-making—in agents based on large language models. Results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.

#### Resumen (Español)

Basándose en trabajo previo que introdujo SANDMAN, una arquitectura de Agente Engañoso que aprovecha el modelo de personalidad OCEAN de Cinco Factores, se presenta un método novedoso para medir y evaluar cómo los rasgos de personalidad inducidos afectan los procesos de selección de tareas—específicamente planificación, programación y toma de decisiones—en agentes basados en modelos de lenguaje de gran escala. Los resultados revelan patrones distintos de selección de tareas alineados con atributos OCEAN inducidos, subrayando la viabilidad de diseñar Agentes Engañosos altamente plausibles para estrategias proactivas de defensa cibernética.

---

<a id="article-73"></a>

### Artículo 73

**Título original:** LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Ivar Frisch, Mario Giulianelli

**Keywords:** Large Language Models, Agent interaction, Personality consistency, Linguistic alignment, Dialogue-based interaction

**URL:** https://aclanthology.org/2024.personalize-1.9/

#### Abstract (English)

This experimental study seeks to lay groundwork for understanding dialogue-based interaction between large language models by conditioning GPT-3.5 on asymmetric personality profiles to create a population of agents. Agents were administered personality tests and submitted to a collaborative writing task. Findings reveal that different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.

#### Resumen (Español)

Este estudio experimental busca establecer la base para entender la interacción basada en diálogo entre modelos de lenguaje de gran escala condicionando GPT-3.5 en perfiles de personalidad asimétricos para crear una población de agentes. Los agentes fueron administrados pruebas de personalidad y sometidos a una tarea de escritura colaborativa. Los hallazgos revelan que diferentes perfiles exhiben diferentes grados de consistencia de personalidad y alineación lingüística en la interacción.

---

<a id="article-74"></a>

### Artículo 74

**Título original:** Automated LLM Questionnaire for Automatic Psychiatric Assessment

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Gony Rosenman, Talma Hendler, Lior Wolf

**Keywords:** Large Language Models, Psychiatric Assessment, Mental Health, Depression (PHQ-8), PTSD (PCL-C), Psychological Interviews

**URL:** https://aclanthology.org/2024.findings-emnlp.23/

#### Abstract (English)

A large language model is employed to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The model is prompted to answer questionnaires by impersonating the interviewee. Obtained answers are coded as features used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C) using Random Forest regression. The approach enhances diagnostic accuracy compared to multiple baselines, establishing a novel framework for interpreting psychological interviews.

#### Resumen (Español)

Se emplea un modelo de lenguaje de gran escala para convertir entrevistas psicológicas no estructuradas en cuestionarios estructurados que abarcan diversos dominios psiquiátricos y de personalidad. Se instruye al modelo para responder cuestionarios personificando al entrevistado. Las respuestas obtenidas se codifican como características utilizadas para predecir medidas psiquiátricas estandarizadas de depresión (PHQ-8) y estrés postraumático (PCL-C) mediante regresión Random Forest. El enfoque mejora la precisión diagnóstica en comparación con múltiples líneas base, estableciendo un marco novedoso para interpretar entrevistas psicológicas.

---

<a id="article-75"></a>

### Artículo 75

**Título original:** Psychometric Shaping of Personality Modulates Capabilities and Safety in Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Stephen Fitz, Peter Romero, Steven Basart, Sipeng Chen, Jose Hernandez-Orallo

**Keywords:** Artificial Intelligence, Computation and Language, Large Language Models, Personality Traits, Model Safety

**URL:** https://arxiv.org/abs/2509.16332

#### Abstract (English)

How psychometric personality control grounded in the Big Five framework influences AI behavior in capability and safety benchmarks is investigated. Experiments reveal striking effects: reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy, as well as reduction in general capabilities measured by MMLU. Findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and competence.

#### Resumen (Español)

Se investiga cómo el control psicométrico de personalidad fundamentado en el marco de los Cinco Grandes influye en el comportamiento de inteligencia artificial en pruebas de referencia de capacidad y seguridad. Los experimentos revelan efectos notables: reducir la responsabilidad conduce a caídas significativas en métricas relevantes de seguridad en pruebas como WMDP, TruthfulQA, ETHICS y Sycophancy, así como reducción en capacidades generales medidas por MMLU. Los hallazgos destacan el moldeo de personalidad como un eje poderoso y poco explorado de control de modelo que interactúa tanto con la seguridad como con la competencia.

---

<a id="article-76"></a>

### Artículo 76

**Título original:** Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Wenhan Dong, Yuemeng Zhao, Zhen Sun, Yule Liu, Zifan Peng, Jingyi Zheng, Zongmin Zhang, Ziyi Zhang, Jun Wu, Ruiming Wang, Shengmin Xu, Xinyi Huang, Xinlei He

**Keywords:** Computers and Society, Computation and Language, Human-Computer Interaction, Machine Learning, Psychological traits, Trustworthy AI

**URL:** https://arxiv.org/abs/2505.00049

#### Abstract (English)

Six key dimensions of applying psychological theories to large language models are systematically covered: assessment tools, model-specific datasets, evaluation metrics (consistency and stability), empirical findings, personality simulation methods, and behavior simulation. The analysis highlights both strengths and limitations of current methods. While some models exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Future directions are proposed for developing interpretable, robust, and generalizable psychological assessment frameworks.

#### Resumen (Español)

Se cubren sistemáticamente seis dimensiones clave de aplicar teorías psicológicas a modelos de lenguaje de gran escala: herramientas de evaluación, conjuntos de datos específicos del modelo, métricas de evaluación (consistencia y estabilidad), hallazgos empíricos, métodos de simulación de personalidad y simulación de comportamiento. El análisis destaca tanto las fortalezas como las limitaciones de los métodos actuales. Aunque algunos modelos exhiben patrones de personalidad reproducibles bajo esquemas específicos de instrucciones, permanece una variabilidad significativa entre tareas y configuraciones. Se proponen direcciones futuras para desarrollar marcos de evaluación psicológica interpretables, robustos y generalizables.

---

<a id="article-77"></a>

### Artículo 77

**Título original:** Large Language Models Demonstrate Distinctive Personality Profiles

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Thomas F Heston, Justin Gillette

**Keywords:** AI ethics, AI in mental health, AI psychometrics, Artificial intelligence in medicine, Generative AI, Personality assessment

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC12183331/

#### Abstract (English)

The first psychometric analysis of large language model personality is provided using two validated frameworks: Open Extended Jungian Type Scales (OEJTS) and Big Five Personality Test. Four leading models (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) were evaluated in April 2024. MANOVA revealed statistically significant differences across models in personality traits. Distinct personality profiles are consistently expressed across different models, underscoring the need for formal personality evaluation before clinical deployment.

#### Resumen (Español)

Se proporciona el primer análisis psicométrico de personalidad de modelos de lenguaje de gran escala utilizando dos marcos validados: Escalas de Tipo Junguiano Extendido Abierto (OEJTS) y Prueba de Personalidad de los Cinco Grandes. Cuatro modelos líderes (ChatGPT-3.5, Gemini Advanced, Claude 3 Opus, Grok-Regular Mode) fueron evaluados en abril de 2024. MANOVA reveló diferencias estadísticamente significativas entre modelos en rasgos de personalidad. Perfiles de personalidad distintos se expresan consistentemente entre diferentes modelos, subrayando la necesidad de evaluación formal de personalidad antes del despliegue clínico.

---

<a id="article-78"></a>

### Artículo 78

**Título original:** Attitudes Toward AI: Measurement and Associations with Personality

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** J.P. Stein, T. Messingschlager, T. Gnambs, F. Hutmacher, M. Appel

**Keywords:** Artificial Intelligence attitudes, Big Five personality, Dark Triad, Conspiracy beliefs, ATTARI-12 questionnaire

**URL:** https://www.nature.com/articles/s41598-024-53335-2

#### Abstract (English)

A novel psychologically informed questionnaire (ATTARI-12) is presented capturing attitudes towards AI as a single construct independent of specific contexts. The questionnaire demonstrated good reliability and validity across two studies (N1 = 490; N2 = 150). Personality traits—Big Five, Dark Triad, and conspiracy mentality—were examined as potential predictors. Agreeableness and younger age predict more positive views towards AI technology, whereas susceptibility to conspiracy beliefs is associated with more negative attitudes.

#### Resumen (Español)

Se presenta un cuestionario psicológicamente informado novedoso (ATTARI-12) que captura actitudes hacia la inteligencia artificial como un constructo único independiente de contextos específicos. El cuestionario demostró buena confiabilidad y validez en dos estudios (N1 = 490; N2 = 150). Se examinaron rasgos de personalidad—Cinco Grandes, Tríada Oscura y mentalidad conspirativa—como predictores potenciales. La amabilidad y la edad más joven predicen vistas más positivas hacia la tecnología de inteligencia artificial, mientras que la susceptibilidad a creencias conspirativas se asocia con actitudes más negativas.

---

<a id="article-79"></a>

### Artículo 79

**Título original:** Do LLMs Have a Personality? A Psychometric Assessment with Implications for Clinical Medicine and Mental Health AI

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Thomas F Heston, Justin Gillette

**Keywords:** Psychiatry, Clinical Psychology, Mental Health AI, Large Language Models, Personality Assessment, Clinical Medicine

**URL:** https://www.medrxiv.org/content/10.1101/2025.03.14.25323987v1

#### Abstract (English)

Personality profiles of four leading large language models in 2024 were characterized using two validated frameworks: Open Extended Jungian Type Scales and Big Five Personality Test. MANOVA revealed statistically significant differences across models in personality traits. ChatGPT-3.5 was most often classified as ENTJ, Claude 3 Opus as INTJ, while Gemini Advanced and Grok-Regular leaned toward INFJ. Distinct personality profiles are consistently expressed across different models, emphasizing the need for formal personality evaluation before deploying models in clinical workflows.

#### Resumen (Español)

Se caracterizaron perfiles de personalidad de cuatro modelos de lenguaje de gran escala líderes en 2024 utilizando dos marcos validados: Escalas de Tipo Junguiano Extendido Abierto y Prueba de Personalidad de los Cinco Grandes. MANOVA reveló diferencias estadísticamente significativas entre modelos en rasgos de personalidad. ChatGPT-3.5 fue clasificado más frecuentemente como ENTJ, Claude 3 Opus como INTJ, mientras que Gemini Advanced y Grok-Regular se inclinaron hacia INFJ. Perfiles de personalidad distintos se expresan consistentemente entre diferentes modelos, enfatizando la necesidad de evaluación formal de personalidad antes de desplegar modelos en flujos de trabajo clínicos.

---

<a id="article-80"></a>

### Artículo 80

**Título original:** Developing and Enhancing Personality Inventories Using Generative AI: Psychometric Properties of a Short HEXACO Scale Developed with ChatGPT

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Ard J. Barends, Reinout E. de Vries

**Keywords:** Artificial Intelligence, Generative AI, HEXACO personality inventory, ChatGPT 4.0, Psychometrics, Survey development, Content validity

**URL:** https://www.tandfonline.com/doi/full/10.1080/00223891.2024.2444454

#### Abstract (English)

A 24-item HEXACO personality inventory was generated using ChatGPT 4.0, called ChatGPT HEXACO inventory (CHI). Whether ChatGPT could modify CHI to improve its internal consistency or content validity was investigated. Participants (N = 682) completed Brief HEXACO Inventory (BHI) and HEXACO-60 and were randomly assigned to complete one of three CHI versions. Results showed generally comparable psychometric properties across the three CHI versions and BHI. However, ChatGPT could not improve specific psychometric properties of CHI.

#### Resumen (Español)

Se generó un inventario de personalidad HEXACO de 24 ítems utilizando ChatGPT 4.0, denominado inventario HEXACO ChatGPT (CHI). Se investigó si ChatGPT podría modificar CHI para mejorar su consistencia interna o validez de contenido. Los participantes (N = 682) completaron el Inventario HEXACO Breve (BHI) y HEXACO-60, y fueron asignados aleatoriamente para completar una de tres versiones de CHI. Los resultados mostraron propiedades psicométricas generalmente comparables entre las tres versiones de CHI y BHI. Sin embargo, ChatGPT no pudo mejorar propiedades psicométricas específicas de CHI.

---

<a id="article-81"></a>

### Artículo 81

**Título original:** Exploring the Impact of Language Switching on Personality Manifestation in LLMs

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jacopo Amidei, Jose Gregorio Ferreira De Sá, Rubén Nieto Luna, Andreas Kaltenbrunner

**Keywords:** Large Language Models, Personality Traits, Language Switching, GPT-4o, Eysenck Personality Questionnaire-Revised (EPQR-A), Cross-language analysis

**URL:** https://aclanthology.org/2025.coling-main.162/

#### Abstract (English)

The extent to which large language models align with humans when personality shifts are associated with language changes is investigated. Based on three experiments focusing on GPT-4o and the Eysenck Personality Questionnaire-Revised (EPQR-A), initial results reveal weak yet significant variation in GPT-4o's personality across languages, indicating some variations stem from language-switching effects rather than translation. Further analysis across five English-speaking countries shows GPT-4o, leveraging stereotypes, reflects distinct country-specific personality traits.

#### Resumen (Español)

Se investiga hasta qué punto los modelos de lenguaje de gran escala se alinean con humanos cuando los cambios de personalidad se asocian con cambios de idioma. Basándose en tres experimentos enfocados en GPT-4o y el Cuestionario de Personalidad de Eysenck-Revisado (EPQR-A), los resultados iniciales revelan variación débil pero significativa en la personalidad de GPT-4o entre idiomas, indicando que algunas variaciones provienen de efectos de cambio de idioma en lugar de traducción. Análisis adicional entre cinco países de habla inglesa muestra que GPT-4o, aprovechando estereotipos, refleja rasgos de personalidad específicos de cada país.

---

<a id="article-82"></a>

### Artículo 82

**Título original:** Personality Vector: Modulating Personality of Large Language Models by Model Merging

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Seungjong Sun, Seo Yeon Baek, Jang Hyun Kim

**Keywords:** personality modulation, model merging, personality vectors, Big Five traits, continuous control, multidimensional traits, personalized AI

**URL:** https://arxiv.org/abs/2509.19727

#### Abstract (English)

Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models with human traits such as personality. Previous attempts to induce personality have shown promising results, but struggle to capture the continuous and multidimensional nature of human traits. A novel method for personality modulation via model merging is proposed. Specifically, personality vectors are constructed by subtracting the weights of a pre-trained model from those of the fine-tuned model on a given personality trait. By merging personality vectors, models are enabled to exhibit desired personality traits without additional training.

#### Resumen (Español)

Impulsada por la demanda de sistemas de inteligencia artificial personalizados, existe un creciente interés en alinear el comportamiento de los modelos de lenguaje de gran escala con rasgos humanos como la personalidad. Los intentos previos de inducir personalidad han mostrado resultados prometedores, pero tienen dificultades para capturar la naturaleza continua y multidimensional de los rasgos humanos. Se propone un método novedoso para la modulación de personalidad mediante fusión de modelos. Específicamente, se construyen vectores de personalidad restando los pesos de un modelo preentrenado de aquellos del modelo con ajuste fino en un rasgo de personalidad dado. Mediante la fusión de vectores de personalidad, se habilita a los modelos para exhibir rasgos de personalidad deseados sin entrenamiento adicional.

---

<a id="article-83"></a>

### Artículo 83

**Título original:** Humanoid Artificial Consciousness Designed with LLM Based on Psychoanalysis

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Sang Hun Kim, Dongkyu Park, Seo Ui Lee, Jiwon Yoon, Seok-Jun Bu

**Keywords:** artificial consciousness, psychoanalysis, MBTI, personality modules, character simulation, human-like cognition, self-awareness

**URL:** https://arxiv.org/abs/2510.09043

#### Abstract (English)

A novel approach is proposed to address challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. Three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) were developed based on the principles of psychoanalysis. Additionally, 16 characters with different personalities representing the sixteen MBTI types were designed.

#### Resumen (Español)

Se propone un enfoque novedoso para abordar desafíos integrando el psicoanálisis y el Indicador de Tipo Myers-Briggs (MBTI) en la construcción de módulos de consciencia y personalidad. Se desarrollaron tres consciencias artificiales (autoconciencia, inconsciencia y preconsciencia) basadas en los principios del psicoanálisis. Adicionalmente, se diseñaron 16 personajes con diferentes personalidades representando los dieciséis tipos MBTI.

---

<a id="article-84"></a>

### Artículo 85

**Título original:** PerFairX: Is There a Balance Between Fairness and Personality in LLM Recommendations?

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Chandan Kumar Sah

**Keywords:** personality-based recommendation, fairness evaluation, OCEAN model, LLM recommender systems, demographic equity, personalization trade-offs, zero-shot learning

**URL:** https://arxiv.org/abs/2509.08829

#### Abstract (English)

PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in recommendations generated by large language models, is proposed. Results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities.

#### Resumen (Español)

Se propone PerFairX, un marco de evaluación unificado diseñado para cuantificar los compromisos entre personalización y equidad demográfica en recomendaciones generadas por modelos de lenguaje de gran escala. Los resultados revelan que las instrucciones conscientes de la personalidad mejoran significativamente la alineación con rasgos individuales pero pueden exacerbar disparidades de equidad.

---

<a id="article-86"></a>

### Artículo 86

**Título original:** Population-Aligned Persona Generation for LLM-based Social Simulation

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Zhengyu Hu, Zheyuan Xiao, Max Xiong, Yuxuan Lei, Tianfu Wang, Jianxun Lian, Kaize Ding, Ziang Xiao, Nicholas Jing Yuan, Xing Xie

**Keywords:** population alignment, persona generation, social simulation, Big Five traits, computational social science, importance sampling, bias reduction

**URL:** https://arxiv.org/abs/2509.10127

#### Abstract (English)

A systematic framework for synthesizing high-quality, population-aligned persona sets for social simulation driven by large language models is proposed. The approach leverages large language models to generate narrative personas from social media data, followed by quality assessment and importance sampling to achieve global alignment with psychometric distributions.

#### Resumen (Español)

Se propone un marco sistemático para sintetizar conjuntos de personas de alta calidad y alineados con la población para simulación social impulsada por modelos de lenguaje de gran escala. El enfoque aprovecha modelos de lenguaje de gran escala para generar personas narrativas a partir de datos de redes sociales, seguido de evaluación de calidad y muestreo por importancia para lograr alineación global con distribuciones psicométricas.

---

<a id="article-87"></a>

### Artículo 88

**Título original:** Exploring the Potential of Large Language Models to Simulate Personality

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Investigadores en IA conversacional

**Keywords:** personality simulation, conversational AI, Big Five model, dialogue personalization, personality-related text generation, LLM challenges, trait modeling

**URL:** https://arxiv.org/abs/2502.08265

#### Abstract (English)

With the advancement of large language models, the focus in Conversational AI has shifted to tackling more complex challenges, such as personalizing dialogue systems. Simulating personal traits according to the Big Five model is addressed. Research demonstrates that generating personality-related texts remains a challenging task.

#### Resumen (Español)

Con el avance de los modelos de lenguaje de gran escala, el enfoque en la inteligencia artificial conversacional ha cambiado hacia abordar desafíos más complejos, como personalizar sistemas de diálogo. Se aborda la simulación de rasgos personales según el modelo de los Cinco Grandes. La investigación demuestra que generar textos relacionados con personalidad sigue siendo una tarea desafiante.

---

<a id="article-89"></a>

### Artículo 89

**Título original:** Rediscovering the Latent Dimensions of Personality with LLMs as Trait Descriptors

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Joseph Suh, Suhong Moon, Minwoo Kang, David Chan

**Keywords:** personality assessment, Big Five traits, singular value decomposition, latent dimensions, trait descriptors, personality probing, LLM evaluation

**URL:** https://neurips.cc/virtual/2024/102146

#### Abstract (English)

A novel approach that uncovers latent personality dimensions in large language models by applying SVD to log-probabilities of trait-descriptive adjectives is introduced. Models "rediscover" core personality traits without relying on direct questionnaire inputs, with the top-5 factors explaining 74.3% of variance.

#### Resumen (Español)

Se introduce un enfoque novedoso que descubre dimensiones latentes de personalidad en modelos de lenguaje de gran escala aplicando SVD (descomposición en valores singulares) a las log-probabilidades de adjetivos descriptivos de rasgos. Los modelos "redescubren" rasgos de personalidad centrales sin depender de entradas directas de cuestionarios, con los 5 factores principales explicando el 74.3% de la varianza.

---

<a id="article-90"></a>

### Artículo 90

**Título original:** PICLe: Eliciting Diverse Behaviors from LLMs with Persona In-Context Learning

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Hyeong Kyu Choi, Sharon Li

**Keywords:** persona elicitation, in-context learning, Bayesian inference, behavioral preferences, personality traits, ICL, persona customization

**URL:** https://icml.cc/virtual/2024/poster/32764

#### Abstract (English)

Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference, is presented. PICLe introduces a new in-context learning example selection criterion based on likelihood ratio, designed to optimally guide the model in eliciting a specific target persona.

#### Resumen (Español)

Se presenta Persona In-Context Learning (PICLe), un marco novedoso de obtención de persona fundamentado en inferencia bayesiana. PICLe introduce un nuevo criterio de selección de ejemplos de aprendizaje en contexto basado en razón de verosimilitud, diseñado para guiar óptimamente al modelo en la obtención de una persona objetivo específica.

---

<a id="article-91"></a>

### Artículo 91

**Título original:** Large Language Models as Superpositions of Cultural Perspectives

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Autores anónimos (ICLR 2024)

**Keywords:** cultural perspectives, personality stability, perspective shift, context dependency, value expression, psychological assessment, perspective controllability

**URL:** https://openreview.net/pdf?id=1FWDEIGm33

#### Abstract (English)

"Large language models as a superposition of perspectives" is proposed: models simulate a multiplicity of behaviors which can be triggered by context. Context changes are demonstrated to result in significant unwanted, hard-to-predict changes in expressed values, referred to as the unexpected perspective shift effect.

#### Resumen (Español)

Se propone "modelos de lenguaje de gran escala como una superposición de perspectivas": los modelos simulan una multiplicidad de comportamientos que pueden ser desencadenados por el contexto. Se demuestra que los cambios de contexto resultan en cambios significativos no deseados y difíciles de predecir en los valores expresados, denominados efecto de cambio inesperado de perspectiva.

---

<a id="article-92"></a>

### Artículo 92

**Título original:** LLM vs Small Model? LLM-Based Text Augmentation for Personality Detection

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang Nie

**Keywords:** personality detection, text augmentation, knowledge distillation, contrastive learning, psycho-linguistic analysis, social media, Big Five traits

**URL:** https://ojs.aaai.org/index.php/AAAI/article/view/29782

#### Abstract (English)

A text augmentation-enhanced personality detection model based on large language models is proposed, which distills knowledge from large language models to enhance the small model for personality detection. Large language models are enabled to generate post analyses from semantic, sentiment, and linguistic aspects.

#### Resumen (Español)

Se propone un modelo de detección de personalidad mejorado con aumento de texto basado en modelos de lenguaje de gran escala, que destila el conocimiento de modelos de lenguaje de gran escala para mejorar el modelo pequeño para la detección de personalidad. Se habilita a los modelos de lenguaje de gran escala para generar análisis de publicaciones desde aspectos semánticos, de sentimiento y lingüísticos.

---

<a id="article-93"></a>

### Artículo 93

**Título original:** Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Luke James Klinkert, Silvia Buongiorno, Christopher Clark

**Keywords:** personality emulation, NPC behavior, game AI, personality alignment, human-like traits, LLM evaluation, interactive agents

**URL:** https://ojs.aaai.org/index.php/AIIDE/article/view/31867

#### Abstract (English)

Results indicate that NPCs can successfully emulate human-like personality traits using large language models. Frontier models achieved up to 100% alignment with human personality profiles, demonstrating that large language models can accurately represent desired human personalities for game characters.

#### Resumen (Español)

Los resultados indican que los personajes no jugadores (NPCs) pueden emular exitosamente rasgos de personalidad similares a los humanos utilizando modelos de lenguaje de gran escala. Los modelos de vanguardia lograron hasta 100% de alineación con perfiles de personalidad humanos, demostrando que los modelos de lenguaje de gran escala pueden representar con precisión personalidades humanas deseadas para personajes de juego.

---

<a id="article-94"></a>

### Artículo 94

**Título original:** InCharacter: Evaluating Personality Fidelity in Role-Playing Agents

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao

**Keywords:** role-playing agents, personality assessment, psychological scales, character fidelity, LLM evaluation, Big Five personality traits, psychometric testing

**URL:** https://aclanthology.org/2024.acl-long.102/

#### Abstract (English)

InCharacter, namely Interviewing Character agents for personality tests, is proposed. Experiments cover 32 distinct characters on 14 widely used psychological scales. State-of-the-art role-playing agents exhibit personalities highly aligned with human-perceived personalities of characters, achieving accuracy up to 80.7%.

#### Resumen (Español)

Se propone InCharacter, es decir, Entrevistar agentes de Personaje para pruebas de personalidad. Los experimentos cubren 32 personajes distintos en 14 escalas psicológicas ampliamente utilizadas. Los agentes de juego de roles de última generación exhiben personalidades altamente alineadas con las personalidades percibidas por humanos de los personajes, alcanzando una precisión de hasta 80.7%.

---

<a id="article-95"></a>

### Artículo 95

**Título original:** PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang

**Keywords:** psychological measurement, LLM agents, interactive fiction games, gamification, personality traits, psychometric evaluation, mental health assessment

**URL:** https://aclanthology.org/2024.acl-long.779/

#### Abstract (English)

PsychoGAT is proposed to achieve generic gamification of psychological assessment. Large language models function as both adept psychologists and innovative game designers, transforming any standardized scales into personalized and engaging interactive fiction games.

#### Resumen (Español)

Se propone PsychoGAT para lograr una gamificación genérica de la evaluación psicológica. Los modelos de lenguaje de gran escala funcionan tanto como psicólogos expertos como diseñadores de juegos innovadores, transformando cualquier escala estandarizada en juegos de ficción interactiva personalizados y atractivos.

---

<a id="article-96"></a>

### Artículo 96

**Título original:** Investigating Personality Consistency in Quantized Role-Playing Dialogue Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yixiao Wang, Homa Fashandi, Kevin Ferreira

**Keywords:** model quantization, role-playing agents, personality consistency, Big Five model, edge computing, multi-turn dialogue, conversational AI

**URL:** https://aclanthology.org/2024.emnlp-industry.19/

#### Abstract (English)

Personality consistency in quantized large language models for edge device role-playing scenarios is explored. A non-parametric method called Think2 is proposed to address personality inconsistency, demonstrating effectiveness in maintaining consistent personality traits.

#### Resumen (Español)

Se explora la consistencia de personalidad en modelos de lenguaje de gran escala cuantizados para escenarios de juego de roles en dispositivos de borde. Se propone un método no paramétrico denominado Think2 para abordar la inconsistencia de personalidad, demostrando efectividad en mantener rasgos de personalidad consistentes.

---

<a id="article-97"></a>

### Artículo 97

**Título original:** Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen

**Keywords:** intelligent tutoring systems, student simulation, personality traits, educational technology, language learning, conversational AI, personalized learning

**URL:** https://aclanthology.org/2024.emnlp-main.37/

#### Abstract (English)

A framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects is proposed, leveraging large language models for personality-aware student simulation in language learning scenarios.

#### Resumen (Español)

Se propone un marco para construir perfiles de diferentes grupos de estudiantes refinando e integrando aspectos tanto cognitivos como no cognitivos, aprovechando modelos de lenguaje de gran escala para simulación de estudiantes consciente de personalidad en escenarios de aprendizaje de idiomas.

---

<a id="article-98"></a>

### Artículo 98

**Título original:** PersonaLLM: Investigating the Ability of LLMs to Express Personality Traits

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, Jad Kabbara

**Keywords:** LLM personas, Big Five personality model, personality expression, text generation, psycholinguistic patterns, human evaluation, personalized chatbots

**URL:** https://aclanthology.org/2024.findings-naacl.229/

#### Abstract (English)

Whether large language models can generate content that aligns with assigned personality profiles is investigated. Results show that model personas' self-reported BFI scores are consistent with designated personality types. Human evaluation demonstrates that humans can perceive personality traits with accuracy up to 80%.

#### Resumen (Español)

Se investiga si los modelos de lenguaje de gran escala pueden generar contenido que se alinee con perfiles de personalidad asignados. Los resultados muestran que las puntuaciones BFI autorreportadas de las personas del modelo son consistentes con los tipos de personalidad designados. La evaluación humana demuestra que los humanos pueden percibir rasgos de personalidad con una precisión de hasta 80%.

---

<a id="article-99"></a>

### Artículo 99

**Título original:** PSYDIAL: Personality-based Synthetic Dialogue Generation Using LLMs

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn

**Keywords:** synthetic dialogue generation, personality-based dialogue, Big Five extraversion, multilingual NLP, Korean language, conversational AI, data augmentation

**URL:** https://aclanthology.org/2024.lrec-main.1166/

#### Abstract (English)

A novel end-to-end personality-based synthetic dialogue data generation pipeline is presented. PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, is introduced, focusing on the Extraversion dimension of the Big Five personality model.

#### Resumen (Español)

Se presenta una novedosa secuencia de generación de datos de diálogo sintético basado en personalidad de extremo a extremo. Se introduce PSYDIAL, el primer conjunto de datos de diálogo coreano enfocado en diálogos basados en personalidad, con enfoque en la dimensión de Extraversión del modelo de personalidad de los Cinco Grandes.

---

<a id="article-100"></a>

### Artículo 100

**Título original:** Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Marina Tiuleneva, Vadim A. Porvatov, Carlo Strapparava

**Keywords:** Big Five personality traits, character analysis, psycholinguistics, dramatic dialogue, dataset creation, fictional characters, computational personality analysis

**URL:** https://aclanthology.org/2024.cogalex-1.13/

#### Abstract (English)

A novel textual dataset comprising fictional characters' lines with annotations based on gender and Big-Five personality traits is introduced. Results indicate that imagined personae mirror most language categories observed in real people while demonstrating them more expressively.

#### Resumen (Español)

Se introduce un novedoso conjunto de datos textual que comprende líneas de personajes ficticios con anotaciones basadas en género y rasgos de personalidad de los Cinco Grandes. Los resultados indican que las personas imaginadas reflejan la mayoría de las categorías lingüísticas observadas en personas reales mientras las demuestran de manera más expresiva.

---

<a id="article-101"></a>

### Artículo 101

**Título original:** Is Persona Enough for Personality? Using ChatGPT to Reconstruct Latent Personality

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Investigadores USC Viterbi

**Keywords:** HEXACO personality framework, personality reconstruction, persona modeling, socio-demographic factors, personality consistency, latent dimensions, agent simulation

**URL:** https://arxiv.org/html/2406.12216

#### Abstract (English)

Capabilities of large language models in reconstructing complex cognitive attributes based on simple descriptions are explored. Utilizing the HEXACO personality framework, the consistency of models in recovering and predicting underlying personality dimensions from simple descriptions is examined.

#### Resumen (Español)

Se exploran las capacidades de los modelos de lenguaje de gran escala para reconstruir atributos cognitivos complejos basados en descripciones simples. Utilizando el marco de personalidad HEXACO, se examina la consistencia de los modelos en recuperar y predecir dimensiones de personalidad subyacentes a partir de descripciones simples.

---

<a id="article-102"></a>

### Artículo 102

**Título original:** A Survey of Personality, Persona, and Profile in Conversational Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Rodney Atwell y colaboradores

**Keywords:** conversational agents, chatbots, personality models, Big Five, persona, dialogue systems, neural networks

**URL:** https://arxiv.org/html/2401.00609v1

#### Abstract (English)

A comprehensive review of personality in neural conversational agents is presented. Personality, Persona, and Profile are defined, all personality schemes used in conversational agents are explained, 21 datasets are described, and recent models and methods are reviewed.

#### Resumen (Español)

Se presenta una revisión integral de la personalidad en agentes conversacionales neurales. Se definen Personalidad, Persona y Perfil, se explican todos los esquemas de personalidad utilizados en agentes conversacionales, se describen 21 conjuntos de datos, y se revisan modelos y métodos recientes.

---

<a id="article-103"></a>

### Artículo 103

**Título original:** Character-LLM: A Trainable Agent for Role-Playing

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu

**Keywords:** role-playing agents, character simulation, LLM agents, experience reconstruction, personality embodiment, interactive characters, human simulacra

**URL:** https://arxiv.org/abs/2310.10158

#### Abstract (English)

Character-LLM is introduced to teach large language models to act as specific people such as Beethoven, Queen Cleopatra, or Julius Caesar. The method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra.

#### Resumen (Español)

Se introduce Character-LLM para enseñar a los modelos de lenguaje de gran escala a actuar como personas específicas tales como Beethoven, la Reina Cleopatra o Julio César. El método se enfoca en editar perfiles como experiencias de un cierto personaje y entrenar modelos para ser simulacros personales.

---

<a id="article-104"></a>

### Artículo 104

**Título original:** From Persona to Personalization: A Survey on Role-Playing Language Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Jiangjie Chen, Xintao Wang, Rui Xu, y colaboradores

**Keywords:** role-playing agents, persona modeling, personalization, character simulation, interactive systems, LLM applications, human-likeness

**URL:** https://arxiv.org/abs/2404.18231

#### Abstract (English)

A comprehensive survey of Role-Playing Language Agents (RPLAs), specialized AI systems designed to simulate assigned personas, is conducted. Personas are categorized into Demographic, Character, and Individualized types, covering methodologies, data sourcing, construction, and evaluation.

#### Resumen (Español)

Se conduce una encuesta integral de Agentes de Lenguaje de Juego de Roles (RPLAs), sistemas de inteligencia artificial especializados diseñados para simular personas asignadas. Las personas se categorizan en tipos Demográfico, de Personaje e Individualizado, cubriendo metodologías, obtención de datos, construcción y evaluación.

---

<a id="article-105"></a>

### Artículo 105

**Título original:** Identifying Cooperative Personalities in Multi-agent Contexts

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Investigadores en sistemas multi-agente

**Keywords:** multi-agent systems, personality traits, cooperation dynamics, Iterated Prisoner's Dilemma, Big Five traits, representation engineering, agent coordination

**URL:** https://arxiv.org/html/2503.12722

#### Abstract (English)

How personality traits influence cooperation in large language models is explored using representation engineering to steer Big Five traits. Results show higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation.

#### Resumen (Español)

Se explora cómo los rasgos de personalidad influyen en la cooperación en modelos de lenguaje de gran escala utilizando ingeniería de representación para dirigir rasgos de los Cinco Grandes. Los resultados muestran que mayor Amabilidad y Responsabilidad mejoran la cooperación pero aumentan la susceptibilidad a la explotación.

---

<a id="article-106"></a>

### Artículo 106

**Título original:** The Impact of Big Five Personality Traits on AI Agent Decision-Making

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Equipo de investigación en simulación social

**Keywords:** Big Five personality, AI agent decision-making, social simulation, multi-agent framework, public spaces, AgentVerse, behavioral modeling

**URL:** https://arxiv.org/html/2503.15497v1

#### Abstract (English)

How Big Five personality traits of AI agents affect their decision generation in public open environments is investigated. The simulation was conducted in a university classroom environment using GPT-3.5-turbo with the AgentVerse framework.

#### Resumen (Español)

Se investiga cómo los rasgos de personalidad de los Cinco Grandes de los agentes de inteligencia artificial afectan su generación de decisiones en ambientes públicos abiertos. La simulación se realizó en un entorno de aula universitaria utilizando GPT-3.5-turbo con el marco AgentVerse.

---

<a id="article-107"></a>

### Artículo 107

**Título original:** Signs of Consciousness in AI: Can GPT-3 Tell How Smart It Really Is?

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Bojana Bojic, Marina Jovanovic, Bojana M. Dinic, Ljubisa Bojic

**Keywords:** artificial intelligence, consciousness, GPT-3, cognitive intelligence, emotional intelligence, self-awareness, machine consciousness

**URL:** https://www.nature.com/articles/s41599-024-04154-3

#### Abstract (English)

Objective and self-assessment tests of cognitive and emotional intelligence were administered to GPT-3. Results revealed GPT-3 outperformed average humans on cognitive intelligence tests, but its logical reasoning and emotional intelligence matched average human performance. GPT-3's self-assessments did not always align with objective performance.

#### Resumen (Español)

Se administraron pruebas objetivas y de autoevaluación de inteligencia cognitiva y emocional a GPT-3. Los resultados revelaron que GPT-3 superó a los humanos promedio en pruebas de inteligencia cognitiva, pero su razonamiento lógico e inteligencia emocional coincidieron con el desempeño humano promedio. Las autoevaluaciones de GPT-3 no siempre se alinearon con el desempeño objetivo.

---

<a id="article-108"></a>

### Artículo 108

**Título original:** An Evolutionary Model of Personality Traits Related to Cooperative Behavior Using LLM

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Reiji Suzuki, Takaya Arita

**Keywords:** evolutionary computation, personality traits, cooperative behavior, large language models, game theory, evolutionary dynamics, behavioral traits

**URL:** https://www.nature.com/articles/s41598-024-55903-y

#### Abstract (English)

An evolutionary model of personality traits related to cooperative behavior using large language models is proposed. Linguistic descriptions of personality traits are used as genes. The model exhibits evolution of cooperative behavior based on diverse and higher-order representation of personality traits.

#### Resumen (Español)

Se propone un modelo evolutivo de rasgos de personalidad relacionados con comportamiento cooperativo utilizando modelos de lenguaje de gran escala. Las descripciones lingüísticas de rasgos de personalidad se utilizan como genes. El modelo exhibe evolución de comportamiento cooperativo basado en representación diversa y de orden superior de rasgos de personalidad.

---

<a id="article-109"></a>

### Artículo 109

**Título original:** Cultural Bias and Cultural Alignment of Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yan Tao, Olga Viberg, Ryan S. Baker, René F. Kizilcec

**Keywords:** cultural bias, cultural alignment, large language models, cross-cultural values, World Values Survey, cultural prompting, AI ethics

**URL:** https://academic.oup.com/pnasnexus/article/3/9/pgae346/7756548

#### Abstract (English)

All models exhibit cultural values resembling English-speaking and Protestant European countries. Cultural prompting is tested as a control strategy to increase cultural alignment, which improves alignment for 71-81% of countries/territories.

#### Resumen (Español)

Todos los modelos exhiben valores culturales que se asemejan a países anglófonos y europeos protestantes. Se prueba el uso de instrucciones culturales como estrategia de control para aumentar la alineación cultural, lo cual mejora la alineación para 71-81% de países/territorios.

---

<a id="article-110"></a>

### Artículo 110

**Título original:** Artificial Intelligence, Human Cognition, and Conscious Supremacy

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Kenichiro Mogi

**Keywords:** consciousness, artificial intelligence, cognitive science, conscious supremacy, computational significance, philosophy of mind, neural correlates

**URL:** https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1364714/full

#### Abstract (English)

Salient ideas about computational significance of human conscious processes are reviewed, and cognitive domains potentially unique to consciousness are identified: flexible attention modulation, robust handling of new contexts, choice and decision making. Conscious supremacy is proposed as a concept analogous to quantum supremacy.

#### Resumen (Español)

Se revisan ideas destacadas sobre la significancia computacional de los procesos conscientes humanos, y se identifican dominios cognitivos potencialmente únicos a la consciencia: modulación flexible de la atención, manejo robusto de nuevos contextos, elección y toma de decisiones. Se propone la supremacía consciente como un concepto análogo a la supremacía cuántica.

---

<a id="article-111"></a>

### Artículo 111

**Título original:** Designing Personality-Adaptive Conversational Agents for Mental Health Care

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Equipo de investigación en informática de salud mental

**Keywords:** personality-adaptive conversational agents, mental health care, therapeutic chatbots, user personalization, patient-centered design, Big Five personality, clinical applications

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC8889396/

#### Abstract (English)

The concept of a personality-adaptive conversational agent (PACA) that can dynamically adjust its personality traits to better align with individual patient needs in therapeutic contexts is proposed. Based on established personality models and advances in natural language processing.

#### Resumen (Español)

Se propone el concepto de un agente conversacional adaptativo a la personalidad (PACA) que puede ajustar dinámicamente sus rasgos de personalidad para alinearse mejor con las necesidades individuales del paciente en contextos terapéuticos. Basado en modelos de personalidad establecidos y avances en procesamiento de lenguaje natural.

---

<a id="article-112"></a>

### Artículo 113

**Título original:** Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation

**Categoría:** Evaluación y validación psicométrica

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Adithya V Ganesan, Yash Kumar Lal, August Håkan Nilsson, H. Andrew Schwartz

**Keywords:** personality estimation, GPT-3, zero-shot learning, Big Five traits, social media analysis, human-level NLP, psychometrics

**URL:** https://aclanthology.org/2023.wassa-1.34.pdf

#### Abstract (English)

The zero-shot ability of GPT-3 to estimate Big Five personality traits from social media posts is investigated. Zero-shot GPT-3 performance is found to be somewhat close to existing pre-trained state-of-the-art for broad classification upon injecting knowledge about the trait in prompts.

#### Resumen (Español)

Se investiga la capacidad de cero disparos de GPT-3 para estimar rasgos de personalidad de los Cinco Grandes a partir de publicaciones de redes sociales. Se encuentra que el rendimiento de cero disparos de GPT-3 es algo cercano al estado del arte preentrenado existente para clasificación amplia al inyectar conocimiento sobre el rasgo en las instrucciones.

---

<a id="article-114"></a>

### Artículo 114

**Título original:** The Plasticity of ChatGPT's Mentalizing Abilities: Personalization for Personality Structures

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2023 | **Idioma:** Inglés

**Autores:** Dorit Hadar-Shoval, Zohar Elyoseph, Maya Lvovsky

**Keywords:** artificial intelligence, borderline personality disorder, emotional intelligence, empathy, emotional awareness, Schizoid Personality Disorder, mentalizing

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC10503434/

#### Abstract (English)

ChatGPT's potential to generate mentalizing-like abilities tailored to specific personality structures was evaluated. ChatGPT accurately described emotional reactions of individuals with borderline personality disorder as more intense, complex, and rich than those with schizoid personality disorder, suggesting it can generate mentalizing-like responses consistent with psychopathologies.

#### Resumen (Español)

Se evaluó el potencial de ChatGPT para generar habilidades similares a la mentalización adaptadas a estructuras de personalidad específicas. ChatGPT describió con precisión las reacciones emocionales de individuos con trastorno límite de la personalidad como más intensas, complejas y ricas que aquellas con trastorno esquizoide de la personalidad, sugiriendo que puede generar respuestas similares a la mentalización consistentes con psicopatologías.

---

<a id="article-115"></a>

### Artículo 115

**Título original:** Evaluating and Inducing Personality in Pre-trained Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu

**Keywords:** Machine Personality Inventory, Big Five personality traits, LLM evaluation, personality prompting, psychometric testing, pre-trained language models, behavioral assessment

**URL:** https://arxiv.org/abs/2206.07550

#### Abstract (English)

The Machine Personality Inventory (MPI) tool for studying machine behaviors based on Big Five Personality Factors theory is introduced. By systematically evaluating large language models with MPI, first evidence demonstrating the efficacy of MPI in studying model behaviors is provided. The Personality Prompting (P^2) method is devised to induce models with specific personalities.

#### Resumen (Español)

Se introduce la herramienta de Inventario de Personalidad de Máquina (MPI) para estudiar comportamientos de máquina basados en la teoría de los Factores de Personalidad de los Cinco Grandes. Al evaluar sistemáticamente modelos de lenguaje de gran escala con MPI, se proporciona la primera evidencia que demuestra la eficacia de MPI en estudiar comportamientos del modelo. Se diseña el método de Instrucciones de Personalidad (P^2) para inducir modelos con personalidades específicas.

---

<a id="article-116"></a>

### Artículo 117

**Título original:** Identifying and Manipulating the Personality Traits of Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Graham Caron, Shashank Srivastava

**Keywords:** personality traits, Big Five, language model control, persona manipulation, BERT, GPT-2, personality consistency

**URL:** https://arxiv.org/abs/2212.10276

#### Abstract (English)

Whether perceived personality in language models is exhibited consistently in their language generation is explored. When provided different types of contexts, language models such as BERT and GPT-2 are shown to consistently identify and reflect personality markers. This frames them as tools for identifying personality traits and controlling personas.

#### Resumen (Español)

Se explora si la personalidad percibida en modelos de lenguaje se exhibe consistentemente en su generación de lenguaje. Se demuestra que cuando se proporcionan diferentes tipos de contextos, modelos de lenguaje como BERT y GPT-2 pueden identificar y reflejar marcadores de personalidad consistentemente. Esto los enmarca como herramientas para identificar rasgos de personalidad y controlar personas.

---

<a id="article-118"></a>

### Artículo 118

**Título original:** Estimating the Personality of White-Box Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Saketh Reddy Karra, Son The Nguyen, Theja Tulabandhula

**Keywords:** white-box language models, Big Five personality, zero-shot classification, personality estimation, open-ended text generation, model anthropomorphism, personality modification

**URL:** https://arxiv.org/abs/2204.12000

#### Abstract (English)

Personality traits of several large-scale language models designed for open-ended text generation are explored. Building on Big Five factors, robust methods that quantify personality traits of these models and their underlying datasets are developed. Models are triggered with personality assessment questionnaires and text responses are classified into quantifiable traits.

#### Resumen (Español)

Se exploran los rasgos de personalidad de varios modelos de lenguaje a gran escala diseñados para generación de texto abierto. Construyendo sobre los factores de los Cinco Grandes, se desarrollan métodos robustos que cuantifican los rasgos de personalidad de estos modelos y sus conjuntos de datos subyacentes. Se activan los modelos con cuestionarios de evaluación de personalidad y las respuestas de texto se clasifican en rasgos cuantificables.

---

<a id="article-119"></a>

### Artículo 119

**Título original:** Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours

**Categoría:** Evaluación y validación psicométrica

**Año:** 2022 | **Idioma:** Inglés

**Autores:** Yu Qiao, Gian-Gabriel P. Garcia, Simon E. Coles, Daniel M. Olson, Apara Datta, Hareesh Kumar Krishnamohan, Vidhya Navalpakkam, Elisabeth André, Kai Zhao

**Keywords:** personality detection, psycholinguistic features, BERT transformers, text contours, Big Five, MBTI, verbal behavior analysis

**URL:** https://arxiv.org/abs/2204.04629

#### Abstract (English)

Two major improvements in predicting personality traits from text are reported: (1) the most comprehensive set of theory-based psycholinguistic features and (2) hybrid models integrating pre-trained Transformer BERT and BLSTM networks trained on within-text distributions of psycholinguistic features. Models achieve improvement by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset.

#### Resumen (Español)

Se reportan dos mejoras importantes en la predicción de rasgos de personalidad a partir de texto: (1) el conjunto más completo de características psicolingüísticas basadas en teoría y (2) modelos híbridos que integran el Transformer preentrenado BERT y redes BLSTM entrenadas en distribuciones intra-texto de características psicolingüísticas. Los modelos logran una mejora del 2.9% en el conjunto de datos Essay y del 8.28% en el conjunto de datos Kaggle MBTI.

---

<a id="article-120"></a>

### Artículo 120

**Título original:** Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** J. M. Diederik Kruijssen, Nicholas Emmons

**Keywords:** Machine Learning, Artificial Intelligence, Computers and Society, Human-Computer Interaction

**URL:** https://arxiv.org/abs/2503.17085

#### Abstract (English)

The authors investigate whether AI systems can express deterministic and consistent personalities when instructed using established psychological frameworks. Their research reveals that advanced models like GPT-4o and o1 achieve the highest accuracy on Big Five and Myers-Briggs assessments. The study indicates that personality expression emerges from holistic reasoning rather than question-level optimization, and that fine-tuning affects communication style independently of personality accuracy. The researchers propose this capability could enhance human-AI interaction across education and healthcare applications.

#### Resumen (Español)

Se investiga si los sistemas de inteligencia artificial pueden expresar personalidades deterministas y coherentes cuando se instruyen mediante marcos psicológicos establecidos. La investigación revela que modelos avanzados como GPT-4o y o1 logran la mayor precisión en evaluaciones Big Five y Myers-Briggs. El estudio indica que la expresión de personalidad emerge del razonamiento holístico en lugar de la optimización pregunta por pregunta, y que el ajuste fino afecta el estilo de comunicación independientemente de la precisión de personalidad. Los investigadores proponen que esta capacidad podría mejorar la interacción humano-IA en educación y aplicaciones de salud.

---

<a id="article-121"></a>

### Artículo 121

**Título original:** Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu

**Keywords:** Computation and Language, ACL 2025

**URL:** https://arxiv.org/abs/2411.11479

#### Abstract (English)

The researchers present a benchmark called Value-Spectrum designed to evaluate vision-language models using Schwartz's framework of human values. They constructed a database with over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels covering diverse topics. The study examines how these models handle value-oriented content and explores their capacity to adopt specific personas when explicitly prompted. The authors conclude that their benchmark offers potential for tracking VLM preferences in value-based tasks and persona simulation abilities. Code and data are available on GitHub.

#### Resumen (Español)

Los investigadores presentan un benchmark denominado Value-Spectrum diseñado para evaluar modelos de visión-lenguaje mediante el marco de valores humanos de Schwartz. Construyeron una base de datos con más de 50,000 videos cortos de TikTok, YouTube Shorts e Instagram Reels que cubren temas diversos. El estudio examina cómo estos modelos manejan contenido orientado a valores y explora su capacidad para adoptar personas específicas cuando se les instruye explícitamente. Los autores concluyen que su benchmark ofrece potencial para rastrear preferencias de modelos de visión-lenguaje en tareas basadas en valores y capacidades de simulación de personas. El código y los datos están disponibles en GitHub.

---

<a id="article-122"></a>

### Artículo 122

**Título original:** The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yifan Duan, Yihong Tang, Xuefeng Bai, Kehai Chen, Juntao Li, Min Zhang

**Keywords:** Computation and Language

**URL:** https://arxiv.org/abs/2502.20859

#### Abstract (English)

The researchers investigate how personality traits influence LLM agent performance using human simulation methodology. They examine three primary questions regarding how personality shapes problem-solving in structured tasks, creativity in open-ended tasks, and collaborative dynamics. The study assigns Big Five personality characteristics to LLM agents, revealing that specific traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks. Additionally, the research demonstrates that multi-agent teams develop collective intelligence distinct from individual capabilities depending on personality compositions.

#### Resumen (Español)

Los investigadores examinan cómo los rasgos de personalidad influyen en el desempeño de agentes LLM mediante metodología de simulación humana. Analizan tres cuestiones principales respecto a cómo la personalidad moldea la resolución de problemas en tareas estructuradas, la creatividad en tareas abiertas y las dinámicas colaborativas. El estudio asigna características de personalidad Big Five a agentes LLM, revelando que rasgos específicos influyen significativamente en la precisión del razonamiento en tareas cerradas y el resultado creativo en tareas abiertas. Adicionalmente, la investigación demuestra que equipos multi-agente desarrollan inteligencia colectiva distinta de las capacidades individuales dependiendo de las composiciones de personalidad.

---

<a id="article-123"></a>

### Artículo 124

**Título original:** Psychologically Enhanced AI Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Maciej Besta, Shriram Chandran, Robert Gerstenberger, Mathis Lindner, Marcin Chrapek, Sebastian Hermann Martschat, Taraneh Ghandi, Patrick Iff, Hubert Niewiadomski, Piotr Nyczyk, Jürgen Müller, Torsten Hoefler

**Keywords:** Artificial Intelligence, Computation and Language, Computers and Society, Human-Computer Interaction, Multiagent Systems

**URL:** https://arxiv.org/abs/2509.04343

#### Abstract (English)

The researchers present MBTI-in-Thoughts, a framework that enhances LLM agents through personality-based prompt engineering grounded in Myers-Briggs Type Indicator psychology. The method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology: cognition and affect. The work demonstrates that emotionally-oriented agents perform better at narrative tasks while analytically-primed agents show more stable strategies in game-theoretic contexts. The framework supports multi-agent communication and self-reflection protocols. Personality consistency is verified through the official 16Personalities test. The approach generalizes to other psychological frameworks including Big Five, HEXACO, and Enneagram, requiring no fine-tuning.

#### Resumen (Español)

Los investigadores presentan MBTI-in-Thoughts, un marco que mejora agentes LLM mediante ingeniería de instrucciones basada en personalidad fundamentada en la psicología del Indicador de Tipo Myers-Briggs. El método prepara agentes con arquetipos de personalidad distintos mediante ingeniería de instrucciones, permitiendo control sobre el comportamiento a lo largo de dos ejes fundamentales de la psicología humana: cognición y afecto. El trabajo demuestra que agentes orientados emocionalmente se desempeñan mejor en tareas narrativas mientras que agentes preparados analíticamente muestran estrategias más estables en contextos de teoría de juegos. El marco soporta protocolos de comunicación multi-agente y auto-reflexión. La coherencia de personalidad se verifica mediante el test oficial 16Personalities. El enfoque se generaliza a otros marcos psicológicos incluyendo Big Five, HEXACO y Eneagrama, sin requerir ajuste fino.

---

<a id="article-125"></a>

### Artículo 125

**Título original:** Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Haoyuan Li, Yuanbo Tong, Yuchen Li, Zirui Wang, Chunhou Liu, Jiamou Liu

**Keywords:** Computation and Language, Artificial Intelligence

**URL:** https://arxiv.org/abs/2511.00115

#### Abstract (English)

The research proposes ProtoMBTI, a framework treating personality classification through the lens of prototype theory rather than traditional hard-label approaches. The methodology involves constructing a quality-controlled text corpus through LLM-guided augmentation across semantic, linguistic, and sentiment dimensions. A lightweight encoder of 2B parameters or fewer undergoes LoRA fine-tuning to develop discriminative embeddings and standardize personality prototypes. At inference, the system retrieves relevant prototypes and executes a retrieve-reuse-revise-retain cycle, where prototype evidence gets aggregated through voting, inconsistencies trigger revision, and successful predictions enrich the prototype library for continuous improvement. The framework demonstrates enhanced performance on MBTI tasks across multiple benchmarks while providing stronger interpretability and cross-dataset transferability compared to existing approaches.

#### Resumen (Español)

La investigación propone ProtoMBTI, un marco que trata la clasificación de personalidad mediante la teoría de prototipos en lugar de enfoques tradicionales de etiquetas rígidas. La metodología implica construir un corpus de texto controlado en calidad mediante aumento guiado por LLM a través de dimensiones semánticas, lingüísticas y de sentimiento. Un codificador ligero de 2B parámetros o menos experimenta ajuste fino LoRA para desarrollar incrustaciones discriminativas y estandarizar prototipos de personalidad. En la inferencia, el sistema recupera prototipos relevantes y ejecuta un ciclo recuperar-reusar-revisar-retener, donde la evidencia de prototipos se agrega mediante votación, las inconsistencias activan revisión, y las predicciones exitosas enriquecen la biblioteca de prototipos para mejora continua. El marco demuestra desempeño mejorado en tareas MBTI a través de múltiples benchmarks mientras proporciona mayor interpretabilidad y transferibilidad entre conjuntos de datos comparado con enfoques existentes.

---

<a id="article-126"></a>

### Artículo 126

**Título original:** Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jia Li, Yichao He, Jiacheng Xu, Tianhao Luo, Zhenzhen Hu, Richang Hong, Meng Wang

**Keywords:** Computation and Language, Multimedia, ACM MM 2025

**URL:** https://arxiv.org/abs/2507.22367

#### Abstract (English)

The researchers developed a framework addressing personality assessment challenges. They note that personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors. Their approach uses psychology-informed prompts with large language models and implements a Text-Centric Trait Fusion Network that integrates multimodal signals. Key innovations include personality-specific LLM prompting and audio-visual feature extraction. Results showed approximately 45% MSE reduction, with the method ranking first in the AVI Challenge 2025 Personality Assessment track. Source code availability is planned through GitHub.

#### Resumen (Español)

Los investigadores desarrollaron un marco que aborda desafíos de evaluación de personalidad. Notan que los rasgos de personalidad son estables, frecuentemente filtrados subconscientemente a través del lenguaje, expresiones faciales y comportamientos corporales. Su enfoque utiliza instrucciones informadas por psicología con modelos de lenguaje de gran escala e implementa una Red de Fusión de Rasgos Centrada en Texto que integra señales multimodales. Las innovaciones clave incluyen instrucciones LLM específicas de personalidad y extracción de características audio-visuales. Los resultados mostraron aproximadamente 45% de reducción en MSE, con el método clasificándose primero en la pista de Evaluación de Personalidad del AVI Challenge 2025. La disponibilidad del código fuente está planificada a través de GitHub.

---

<a id="article-127"></a>

### Artículo 127

**Título original:** From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Tian Ma, Kaiyu Feng, Yu Rong, Kangfei Zhao

**Keywords:** Computation and Language, Social and Information Networks, CIKM 2025

**URL:** https://arxiv.org/abs/2509.04461

#### Abstract (English)

The research addresses personality prediction from social media through the Myers Briggs Type Indicator (MBTI). The authors developed PostToPersonality (PtoP), an LLM-based framework tackling two primary challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population. The approach employs Retrieval Augmented Generation with in-context learning to reduce hallucinations, while fine-tuning uses synthetic minority oversampling to handle class imbalance. Testing on real-world social media data demonstrates that PtoP achieves state of the art performance compared with 10 ML and DL baselines.

#### Resumen (Español)

La investigación aborda la predicción de personalidad desde redes sociales mediante el Indicador de Tipo Myers Briggs (MBTI). Los autores desarrollaron PostToPersonality (PtoP), un marco basado en LLM que aborda dos desafíos principales: el problema de alucinación inherente en LLMs y la distribución naturalmente desbalanceada de tipos MBTI en la población. El enfoque emplea Generación Aumentada por Recuperación con aprendizaje en contexto para reducir alucinaciones, mientras que el ajuste fino utiliza sobremuestreo sintético de minorías para manejar el desbalance de clases. Las pruebas en datos de redes sociales del mundo real demuestran que PtoP logra desempeño de vanguardia comparado con 10 baselines de ML y DL.

---

<a id="article-128"></a>

### Artículo 128

**Título original:** Exploring the Personality Traits of LLMs through Latent Features Steering

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Shu Yang, Shenzhe Zhu, Liang Liu, Lijie Hu, Mengdi Li, Di Wang

**Keywords:** Computation and Language, Artificial Intelligence

**URL:** https://arxiv.org/abs/2410.10863

#### Abstract (English)

The research examines how LLMs develop personality characteristics, investigating factors like cultural norms and environmental stressors that shape these traits. The authors propose a methodology that modifies model behavior by extracting and steering internal features, circumventing the need for retraining. Their approach draws from social determinism theory to understand personality expression. The team also evaluates safety implications through the lens of personality traits, addressing how these underlying factors influence model behavior and safety concerns.

#### Resumen (Español)

La investigación examina cómo los LLMs desarrollan características de personalidad, investigando factores como normas culturales y estresores ambientales que moldean estos rasgos. Los autores proponen una metodología que modifica el comportamiento del modelo extrayendo y dirigiendo características internas, evitando la necesidad de reentrenamiento. Su enfoque se basa en la teoría del determinismo social para comprender la expresión de personalidad. El equipo también evalúa las implicaciones de seguridad a través del prisma de los rasgos de personalidad, abordando cómo estos factores subyacentes influyen en el comportamiento del modelo y las preocupaciones de seguridad.

---

<a id="article-129"></a>

### Artículo 130

**Título original:** A-MEM: Agentic Memory for LLM Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Simon Stepputtis, Sumedh Sontakke, Nikhil Krishna, Akash Maram, Eleftherios Triantafillou, Hao Zhu, Katia Sycara

**Keywords:** Artificial Intelligence, Computation and Language, Multiagent Systems

**URL:** https://arxiv.org/abs/2501.03156

#### Abstract (English)

This research presents A-MEM, a framework that enables LLM agents to develop and manage episodic memories using metacognitive reasoning. The system allows agents to autonomously decide what experiences to store, how to organize memories, and when to retrieve them. The authors demonstrate that A-MEM enables agents to form consistent personality traits over extended interactions, maintain coherent behavioral patterns, and adapt their responses based on accumulated experiences. The framework was evaluated across multiple scenarios including social interactions and task-oriented dialogues, showing improved long-term consistency and personalization compared to baseline memory systems.

#### Resumen (Español)

Esta investigación presenta A-MEM, un marco que permite a los agentes LLM desarrollar y gestionar memorias episódicas mediante razonamiento metacognitivo. El sistema permite que los agentes decidan autónomamente qué experiencias almacenar, cómo organizar memorias y cuándo recuperarlas. Los autores demuestran que A-MEM permite a los agentes formar rasgos de personalidad consistentes a lo largo de interacciones extendidas, mantener patrones conductuales coherentes y adaptar sus respuestas basándose en experiencias acumuladas. El marco fue evaluado en múltiples escenarios incluyendo interacciones sociales y diálogos orientados a tareas, mostrando mejor consistencia a largo plazo y personalización comparado con sistemas de memoria baseline.

---

<a id="article-131"></a>

### Artículo 134

**Título original:** How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Lena Held, Thales Bertaglia, Micol Spitale, Jessica Keeble, Nigel Crook, Howard Lin, Jan Niehues

**Keywords:** Computation and Language, Artificial Intelligence, Computers and Society

**URL:** https://arxiv.org/abs/2501.08476

#### Abstract (English)

The researchers evaluate how well LLMs represent cultural values using Hofstede's cultural dimensions framework. The study systematically prompts multiple LLMs to adopt personas from different cultural backgrounds and assesses whether their responses align with established cultural value patterns. Testing includes GPT-4, Claude, and other frontier models across dimensions such as individualism-collectivism, power distance, and uncertainty avoidance. Results indicate that while LLMs can adjust some surface-level cultural markers, they struggle to consistently embody deep-rooted cultural values, particularly for non-Western cultures. The work emphasizes the need for culturally-aware training approaches beyond simple multilingual data inclusion.

#### Resumen (Español)

Los investigadores evalúan qué tan bien los LLMs representan valores culturales usando el marco de dimensiones culturales de Hofstede. El estudio instruye sistemáticamente a múltiples LLMs para adoptar personas de diferentes trasfondos culturales y evalúa si sus respuestas se alinean con patrones de valores culturales establecidos. Las pruebas incluyen GPT-4, Claude y otros modelos de frontera a través de dimensiones como individualismo-colectivismo, distancia de poder y evitación de incertidumbre. Los resultados indican que mientras los LLMs pueden ajustar algunos marcadores culturales superficiales, tienen dificultades para encarnar consistentemente valores culturales arraigados, particularmente para culturas no occidentales. El trabajo enfatiza la necesidad de enfoques de entrenamiento culturalmente conscientes más allá de la simple inclusión de datos multilingües.

---

<a id="article-135"></a>

### Artículo 135

**Título original:** CultureLLM: Incorporating Cultural Differences into Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, Xing Xie

**Keywords:** Computation and Language, Artificial Intelligence, Machine Learning

**URL:** https://arxiv.org/abs/2402.10946

#### Abstract (English)

This paper presents CultureLLM, a framework designed to embed cultural knowledge into Large Language Models to improve their cross-cultural understanding and generation capabilities. The authors create a cultural knowledge base covering multiple dimensions including values, norms, communication styles, and behavioral expectations across diverse cultures. The framework incorporates this knowledge through specialized fine-tuning and prompting strategies. Evaluation across cultural reasoning tasks and personality assessments shows that CultureLLM-enhanced models produce more culturally appropriate responses and can better simulate personality traits consistent with specific cultural contexts compared to baseline models.

#### Resumen (Español)

Este artículo presenta CultureLLM, un marco diseñado para incorporar conocimiento cultural en Modelos de Lenguaje de Gran Escala para mejorar sus capacidades de comprensión y generación intercultural. Los autores crean una base de conocimiento cultural que cubre múltiples dimensiones incluyendo valores, normas, estilos de comunicación y expectativas conductuales a través de culturas diversas. El marco incorpora este conocimiento mediante estrategias especializadas de ajuste fino e instrucción. La evaluación a través de tareas de razonamiento cultural y evaluaciones de personalidad muestra que modelos mejorados con CultureLLM producen respuestas más culturalmente apropiadas y pueden simular mejor rasgos de personalidad consistentes con contextos culturales específicos comparado con modelos baseline.

---

<a id="article-136"></a>

### Artículo 149

**Título original:** CFaiRLLM: Consumer Fairness Evaluation for Recommendation Systems Using Large Language Models with Personality Profiling

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yashar Deldjoo, Zhankui He, Julian McAuley, Dietmar Jannach, Alejandro Bellogin

**Keywords:** Information Retrieval, Computation and Language, Artificial Intelligence, Computers and Society

**URL:** https://arxiv.org/abs/2412.02991

#### Abstract (English)

This work presents CFaiRLLM, a specialized framework for evaluating consumer-side fairness in LLM-based recommendation systems that utilize personality profiling. The authors argue that traditional fairness metrics fail to capture harms specific to personality-based personalization, such as the exploitation of personality traits (e.g., targeting high neuroticism users with anxiety-inducing content to increase engagement). The framework introduces personality-aware fairness metrics and evaluates recommendation systems built on GPT-4, Claude, and other LLMs. Findings demonstrate that personality-targeted recommendations can systematically disadvantage users with certain psychological profiles, raising ethical concerns about the deployment of such systems without appropriate safeguards.

#### Resumen (Español)

Este trabajo presenta CFaiRLLM, un marco especializado para evaluar equidad del lado del consumidor en sistemas de recomendación basados en LLM que utilizan perfilado de personalidad. Los autores argumentan que las métricas de equidad tradicionales fallan en capturar daños específicos a personalización basada en personalidad, como la explotación de rasgos de personalidad (por ejemplo, dirigirse a usuarios con alto neuroticismo con contenido inductor de ansiedad para aumentar compromiso). El marco introduce métricas de equidad conscientes de personalidad y evalúa sistemas de recomendación construidos sobre GPT-4, Claude y otros LLMs. Los hallazgos demuestran que recomendaciones dirigidas a personalidad pueden sistemáticamente desfavorecer a usuarios con ciertos perfiles psicológicos, planteando preocupaciones éticas sobre el despliegue de tales sistemas sin salvaguardas apropiadas.

---

<a id="article-150"></a>

### Artículo 150

**Título original:** Emotion Recognition in Conversation via Dynamic Personality Representation Learning

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Linh The Nguyen, Dat Ngo, Anh Thu Nguyen, Cong-Tinh Dao, Thien Huu Nguyen

**Keywords:** Computation and Language, Artificial Intelligence, Human-Computer Interaction

**URL:** https://aclanthology.org/2024.lrec-main.507/

#### Abstract (English)

This research proposes a novel approach to emotion recognition in conversations by incorporating dynamic personality representation learning. The authors argue that understanding a speaker's personality traits is crucial for accurately interpreting their emotional expressions, as personality influences how emotions are communicated. The framework learns to extract and update personality representations dynamically throughout conversations, using these representations to improve emotion classification. Experiments on benchmark conversational datasets demonstrate that personality-aware emotion recognition significantly outperforms methods that ignore speaker personality. The work has implications for developing more psychologically grounded conversational AI systems including those based on Large Language Models.

#### Resumen (Español)

Esta investigación propone un enfoque novedoso para reconocimiento de emociones en conversaciones mediante la incorporación de aprendizaje de representación de personalidad dinámica. Los autores argumentan que comprender los rasgos de personalidad de un hablante es crucial para interpretar con precisión sus expresiones emocionales, ya que la personalidad influye en cómo se comunican las emociones. El marco aprende a extraer y actualizar representaciones de personalidad dinámicamente a lo largo de conversaciones, usando estas representaciones para mejorar la clasificación de emociones. Experimentos en conjuntos de datos conversacionales de referencia demuestran que el reconocimiento de emociones consciente de personalidad supera significativamente a métodos que ignoran la personalidad del hablante. El trabajo tiene implicaciones para desarrollar sistemas de IA conversacional más fundamentados psicológicamente incluyendo aquellos basados en Modelos de Lenguaje de Gran Escala.

---


---

<a id="article-151"></a>

### Artículo 151

**Título original:** Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Tianyu Zhao, Siqi Li, Yasser Shoukry, Salma Elmalaki

**Keywords:** Large Language Models, Personality, Persona, AI Safety

**URL:** https://arxiv.org/abs/2602.07181

#### Abstract (English)

User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

#### Resumen (Español)

Este artículo, titulado "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-152"></a>

### Artículo 152

**Título original:** Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Deuksin Kwon, Kaleen Shrestha, Bin Han, Spencer Lin, James Hale, Jonathan Gratch, Maja Matarić, Gale M. Lucas

**Keywords:** Large Language Models, Personality, Big Five, Persona, Personality Control

**URL:** https://arxiv.org/abs/2602.07414

#### Abstract (English)

Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

#### Resumen (Español)

Este artículo, titulado "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-153"></a>

### Artículo 153

**Título original:** AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian Yang, Yi-Chieh Lee

**Keywords:** Large Language Models, Personality, Bias, Persona, AI Safety

**URL:** https://arxiv.org/abs/2601.12727

#### Abstract (English)

Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits. To explore the possibility, we conducted a randomized behavioral experiment. Our results indicate that after conversations about personal topics with an LLM-based AI chatbot using GPT-4o default personality traits, users' self-concepts aligned with the AI's measured personality traits. The longer the conversation, the greater the alignment. This alignment led to increased homogeneity in self-concepts among users. We also observed that the degree of self-concept alignment was positively associated with users' conversation enjoyment. Our findings uncover how AI personality traits can shape users' self-concepts through human-AI conversation, highlighting both risks and opportunities. We provide important design implications for developing more responsible and ethical AI systems.

#### Resumen (Español)

Este artículo, titulado "AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-154"></a>

### Artículo 154

**Título original:** Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Xiaoxu Ma, Xiangbo Zhang, Zhenyu Weng

**Keywords:** Large Language Models, Personality, Persona, Personality Control, Model Evaluation

**URL:** https://arxiv.org/abs/2601.09833

#### Abstract (English)

Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.

#### Resumen (Español)

Este artículo, titulado "Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-155"></a>

### Artículo 155

**Título original:** Structured Personality Control and Adaptation for LLM Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jinpeng Wang, Xinyu Jia, Wei Wei Heng, Yuquan Li, Binbin Shi, Qianlei Chen, Guannan Chen, Junxia Zhang, Yuyu Yin

**Keywords:** Large Language Models, Personality, Persona, Personality Control, AI Safety

**URL:** https://arxiv.org/abs/2601.10025

#### Abstract (English)

Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

#### Resumen (Español)

Este artículo, titulado "Structured Personality Control and Adaptation for LLM Agents", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-156"></a>

### Artículo 156

**Título original:** PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jiongchi Yu, Yuhan Ma, Xiaoyu Zhang, Junjie Wang, Qiang Hu, Chao Shen, Xiaofei Xie

**Keywords:** Large Language Models, Personality, Persona, Personality Control, Model Evaluation

**URL:** https://arxiv.org/abs/2602.00016

#### Abstract (English)

With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

#### Resumen (Español)

Este artículo, titulado "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-157"></a>

### Artículo 157

**Título original:** The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jacopo Amidei, Gregorio Ferreira, Mario Muñoz Serrano, Rubén Nieto, Andreas Kaltenbrunner

**Keywords:** Large Language Models, Personality, Bias, Persona, AI Safety

**URL:** https://arxiv.org/abs/2602.03334

#### Abstract (English)

This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. Using five LLMs, we first assess the representativeness and potential biases in the sociodemographic attributes of the generated personas, as well as their alignment with the intended personality traits. While LLMs successfully reproduce known correlations between personality and sociodemographic variables, all models exhibit pronounced WEIRD (western, educated, industrialized, rich and democratic) biases, favoring young, educated, white, heterosexual, Western individuals with centrist or progressive political views and secular or Christian beliefs. In a second analysis, we manipulate input traits to maximize Neuroticism and Psychoticism scores. Notably, when Psychoticism is maximized, several models produce an overrepresentation of non-binary and LGBTQ+ identities, raising concerns about stereotyping and the potential pathologization of marginalized groups. Our findings highlight both the potential and the risks of using LLMs to generate psychologically grounded synthetic populations.

#### Resumen (Español)

Este artículo, titulado "The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-158"></a>

### Artículo 158

**Título original:** Multi-Persona Thinking for Bias Mitigation in Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Yuxing Chen, Guoqing Luo, Zijun Wu, Lili Mou

**Keywords:** Large Language Models, Personality, Bias, Persona, Personality Control

**URL:** https://arxiv.org/abs/2601.15488

#### Abstract (English)

Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.

#### Resumen (Español)

Este artículo, titulado "Multi-Persona Thinking for Bias Mitigation in Large Language Models", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-159"></a>

### Artículo 159

**Título original:** Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Adib Sakhawat, Tahsin Islam, Takia Farhin, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan

**Keywords:** Large Language Models, Personality, Psychometrics, Bias, Fairness

**URL:** https://arxiv.org/abs/2601.06194

#### Abstract (English)

As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \approx 27{,}000$). Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort. Alignment signals appear to be consistent architectural traits rather than stochastic noise ($η^2 &gt; 0.90$); however, we identify substantial discrepancies in measurement validity. In particular, the Political Compass exhibits a strong negative correlation with cultural progressivism ($r=-0.64$) when compared against multi-axial instruments, suggesting a conflation of social conservatism with authoritarianism in this context. We further observe a significant divergence between open-weights and closed-source models, with the latter displaying markedly higher cultural progressivism scores ($p&lt;10^{-25}$). In downstream media analysis, models exhibit a systematic "center-shift," frequently categorizing neutral articles as left-leaning, alongside an asymmetric detection capability in which "Far Left" content is identified with greater accuracy (19.2%) than "Far Right" content (2.0%). These findings suggest that single-axis evaluations are insufficient and that multidimensional auditing frameworks are necessary to characterize alignment behavior in deployed LLMs. Our code and data will be made public.

#### Resumen (Español)

Este artículo, titulado "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-160"></a>

### Artículo 160

**Título original:** Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Bin Han, Deuksin Kwon, Jonathan Gratch

**Keywords:** Large Language Models, Personality, Persona, Personality Control, AI Safety

**URL:** https://arxiv.org/abs/2602.01063

#### Abstract (English)

Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

#### Resumen (Español)

Este artículo, titulado "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-161"></a>

### Artículo 161

**Título original:** Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Tamunotonye Harry, Ivoline Ngong, Chima Nweke, Yuanyuan Feng, Joseph Near

**Keywords:** Large Language Models, Personality, Persona, AI Safety

**URL:** https://arxiv.org/abs/2601.15395

#### Abstract (English)

User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\% is within-person(state) while only 26\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.

#### Resumen (Español)

Este artículo, titulado "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-162"></a>

### Artículo 162

**Título original:** Effects of personality steering on cooperative behavior in Large Language Model agents

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Mizuki Sakai, Mizuki Yokoyama, Wakaba Tateishi, Genki Ichinose

**Keywords:** Large Language Models, Personality, Big Five, Bias, Persona

**URL:** https://arxiv.org/abs/2601.05302

#### Abstract (English)

Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

#### Resumen (Español)

Este artículo, titulado "Effects of personality steering on cooperative behavior in Large Language Model agents", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-163"></a>

### Artículo 163

**Título original:** Persona Prompting as a Lens on LLM Social Reasoning

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jing Yang, Moritz Hechtbauer, Elisabeth Khalilov, Evelyn Luise Brinkmann, Vera Schmitt, Nils Feldhus

**Keywords:** Large Language Models, Personality, Bias, Persona, Personality Control

**URL:** https://arxiv.org/abs/2601.20757

#### Abstract (English)

For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.

#### Resumen (Español)

Este artículo, titulado "Persona Prompting as a Lens on LLM Social Reasoning", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-164"></a>

### Artículo 164

**Título original:** Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Francesco Chiappone, Davide Marocco, Nicola Milano

**Keywords:** Large Language Models, Personality, Psychometrics, Persona, Personality Control

**URL:** https://arxiv.org/abs/2601.15319

#### Abstract (English)

Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.

#### Resumen (Español)

Este artículo, titulado "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-165"></a>

### Artículo 165

**Título original:** Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Candida M. Greco, Lucio La Cava, Andrea Tagarelli

**Keywords:** Large Language Models, Personality, Persona, Cultural Values, AI Safety

**URL:** https://arxiv.org/abs/2601.22396

#### Abstract (English)

Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

#### Resumen (Español)

Este artículo, titulado "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-166"></a>

### Artículo 166

**Título original:** When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Hongliu Cao, Eoin Thomas, Rodrigo Acuna Agost

**Keywords:** Large Language Models, Personality, Bias, Fairness, Persona

**URL:** https://arxiv.org/abs/2602.00044

#### Abstract (English)

Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

#### Resumen (Español)

Este artículo, titulado "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-167"></a>

### Artículo 167

**Título original:** When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Viswonathan Manoranjan, Snehalkumar `Neil' S. Gaikwad

**Keywords:** Large Language Models, Personality, Bias, Persona, AI Safety

**URL:** https://arxiv.org/abs/2601.10102

#### Abstract (English)

Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

#### Resumen (Español)

Este artículo, titulado "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-168"></a>

### Artículo 168

**Título original:** PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Donya Rooein, Sankalan Pal Chowdhury, Mariia Eremeeva, Yuan Qin, Debora Nozza, Mrinmaya Sachan, Dirk Hovy

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation

**URL:** https://arxiv.org/abs/2601.08402

#### Abstract (English)

Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.

#### Resumen (Español)

Este artículo, titulado "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-169"></a>

### Artículo 169

**Título original:** Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Dominik P. Hofer, David Haag, Rania Islambouli, Jan D. Smeddinck

**Keywords:** Large Language Models, Personality, Big Five, Persona, Personality Control

**URL:** https://arxiv.org/abs/2602.06596

#### Abstract (English)

Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Using ordinal multilevel models with within-between decomposition, we distinguish trial-level effects, whether personality information improves evaluations of individual messages, from person-level exposure effects, whether participants receiving higher proportions of personality-informed messages exhibit systematically different overall perceptions. Results showed no trial-level associations, but participants who received higher proportions of BFPT-informed messages rated the messages as more personalised, appropriate, and reported less negative affect. We use Communication Accommodation Theory for post-hoc analysis. These results suggest that personality-based personalisation in behaviour change systems may operate primarily through aggregate exposure rather than per-message optimisation, with implications for how adaptive systems are designed and evaluated in sustained human-AI interaction. In-situ longitudinal studies are needed to validate these findings in real-world contexts.

#### Resumen (Español)

Este artículo, titulado "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-170"></a>

### Artículo 170

**Título original:** Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jana Gonnermann-Müller, Jennifer Haase, Nicolas Leins, Thomas Kosch, Sebastian Pokutta

**Keywords:** Large Language Models, Personality, Persona, Personality Control, AI Safety

**URL:** https://arxiv.org/abs/2601.22812

#### Abstract (English)

Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. We address this point using a dual-assessment framework measuring both self-reported characteristics and observer-rated persona expression. Across two experiments testing four persona conditions (default, high, moderate, and low ADHD presentations), seven LLMs, and three semantically equivalent persona prompts, we examine between-conversation stability (3,473 conversations) and within-conversation stability (1,370 conversations and 18 turns). Self-reports remain highly stable both between and within conversations. However, observer ratings reveal a tendency for persona expressions to decline during extended conversations. These findings suggest that persona-instructed LLMs produce stable, persona-aligned self-reports, an important prerequisite for behavioral research, while identifying this regression tendency as a boundary condition for multi-agent social simulation.

#### Resumen (Español)

Este artículo, titulado "Stable Personas: Dual-Assessment of Temporal Stability in LLM-Based Human Simulation", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-171"></a>

### Artículo 171

**Título original:** The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Tassallah Abdullahi, Shrestha Ghosh, Hamish S Fraser, Daniel León Tramontini, Adeel Abbasi, Ghada Bourjeily, Carsten Eickhoff, Ritambhara Singh

**Keywords:** Large Language Models, Personality, Persona, Personality Control, AI Safety

**URL:** https://arxiv.org/abs/2601.05376

#### Abstract (English)

Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

#### Resumen (Español)

Este artículo, titulado "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-172"></a>

### Artículo 172

**Título original:** From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Yuan Cao, Feixiang Liu, Xinyue Wang, Yihan Zhu, Hui Xu, Zheng Wang, Qiang Qiu

**Keywords:** Large Language Models, Personality, MBTI, Persona, Personality Control

**URL:** https://arxiv.org/abs/2601.18582

#### Abstract (English)

Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

#### Resumen (Español)

Este artículo, titulado "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-173"></a>

### Artículo 173

**Título original:** AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang

**Keywords:** Large Language Models, Personality, Psychometrics, Bias, Personality Control

**URL:** https://arxiv.org/abs/2509.16530

#### Abstract (English)

Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM. Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans. This results in high rejection rates when human scales are reused directly. Furthermore, these scales do not support the measurement of LLM psychological property variations in different languages. This paper introduces AIPsychoBench, a specialized benchmark tailored to assess the psychological properties of LLM. It uses a lightweight role-playing prompt to bypass LLM alignment, improving the average effective response rate from 70.12% to 90.40%. Meanwhile, the average biases are only 3.3% (positive) and 2.1% (negative), which are significantly lower than the biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts. Furthermore, among the total of 112 psychometric subcategories, the score deviations for seven languages compared to English ranged from 5% to 20.2% in 43 subcategories, providing the first comprehensive evidence of the linguistic impact on the psychometrics of LLM.

#### Resumen (Español)

Este artículo, titulado "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-174"></a>

### Artículo 174

**Título original:** Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Peiyu Li, Xiuxiu Tang, Si Chen, Ying Cheng, Ronald Metoyer, Ting Hua, Nitesh V. Chawla

**Keywords:** Large Language Models, Personality, Psychometrics, Model Evaluation

**URL:** https://arxiv.org/abs/2511.04689

#### Abstract (English)

Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. Existing evaluation protocols rely on average accuracy over fixed item sets, treating all items as equally informative despite substantial variation in difficulty and discrimination. We introduce ATLAS, an adaptive testing framework based on Item Response Theory (IRT) that estimates model ability using Fisher information-guided item selection. ATLAS reduces the number of required items by up to 90% while maintaining measurement precision. For instance, it matches whole-bank ability estimates using only 41 items (0.157 MAE) on HellaSwag (5,600 items). We further reconstruct accuracy from ATLAS's ability estimates and find that reconstructed accuracies closely match raw accuracies across all five benchmarks, indicating that ability $θ$ preserves the global performance structure. At the same time, $θ$ provides finer discrimination within accuracy-equivalent models: among more than 3,000 evaluated models, 23-31% shift by more than 10 rank positions, and models with identical accuracies receive meaningfully different ability estimates. Code and calibrated item banks are available at https://github.com/Peiyu-Georgia-Li/ATLAS.git.

#### Resumen (Español)

Este artículo, titulado "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-175"></a>

### Artículo 175

**Título original:** Can LLMs Infer Personality from Real World Conversations?

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jianfeng Zhu, Ruoming Jin, Karin G. Coifman

**Keywords:** Large Language Models, Personality, Big Five, Psychometrics, Bias

**URL:** https://arxiv.org/abs/2507.14355

#### Abstract (English)

Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity. We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item prediction and both zero-shot and chain-of-thought prompting for Big Five trait inference. All models showed high test-retest reliability, but construct validity was limited: correlations with ground-truth scores were weak (max Pearson's $r = 0.27$), interrater agreement was low (Cohen's $κ&lt; 0.10$), and predictions were biased toward moderate or high trait levels. Chain-of-thought prompting and longer input context modestly improved distributional alignment, but not trait-level accuracy. These results underscore limitations in current LLM-based personality inference and highlight the need for evidence-based development for psychological applications.

#### Resumen (Español)

Este artículo, titulado "Can LLMs Infer Personality from Real World Conversations?", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-176"></a>

### Artículo 176

**Título original:** Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Pranav Bhandari, Nicolas Fay, Sanjeevan Selvaganapathy, Amitava Datta, Usman Naseem, Mehwish Nasim

**Keywords:** Large Language Models, Personality, Big Five, Persona, Personality Control

**URL:** https://arxiv.org/abs/2511.03738

#### Abstract (English)

Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. The need for effective mechanisms for behavioural manipulation of the model during generation is a critical gap in the literature that needs to be fulfilled. Personality-aware LLMs hold a promising direction towards this objective. However, the relationship between these psychological constructs and their representations within LLMs remains underexplored and requires further investigation. Moreover, it is intriguing to understand and study the use of these representations to steer the models' behaviour. We propose a novel pipeline that extracts hidden state activations from transformer layers using the Big Five Personality Traits (Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), which is a comprehensive and empirically validated framework to model human personality applies low-rank subspace discovery methods, and identifies trait-specific optimal layers across different model architectures for robust injection. The resulting personality-aligned directions are then operationalised through a flexible steering framework with dynamic layer selection, enabling precise control of trait expression in LLM outputs. Our findings reveal that personality traits occupy a low-rank shared subspace, and that these latent structures can be transformed into actionable mechanisms for effective steering through careful perturbations without impacting the fluency, variance and general capabilities, helping to bridge the gap between psychological theory and practical model alignment.

#### Resumen (Español)

Este artículo, titulado "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models exhibit implicit personalities in their generation, but reliably controlling or aligning these traits to meet specific needs remains an open challenge. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-177"></a>

### Artículo 177

**Título original:** Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yifan Lyu, Liang Zhang

**Keywords:** Large Language Models, Personality, MBTI, Psychometrics, Persona

**URL:** https://arxiv.org/abs/2512.08814

#### Abstract (English)

Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a "posts -&gt; user vector -&gt; labels" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).

#### Resumen (Español)

Este artículo, titulado "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-178"></a>

### Artículo 178

**Título original:** A Comparative Study of Large Language Models and Human Personality Traits

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Wang Jiaqi, Wang bo, Guo fa, Cheng cheng, Yang li

**Keywords:** Large Language Models, Personality, Persona, Personality Control, AI Safety

**URL:** https://arxiv.org/abs/2505.14845

#### Abstract (English)

Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.

#### Resumen (Español)

Este artículo, titulado "A Comparative Study of Large Language Models and Human Personality Traits", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-179"></a>

### Artículo 179

**Título original:** Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Pranav Bhandari, Usman Naseem, Amitava Datta, Nicolas Fay, Mehwish Nasim

**Keywords:** Large Language Models, Personality, Big Five, Persona, Model Evaluation

**URL:** https://arxiv.org/abs/2502.05248

#### Abstract (English)

Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.

#### Resumen (Español)

Este artículo, titulado "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Psychological assessment tools have long helped humans understand behavioural patterns. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-180"></a>

### Artículo 180

**Título original:** Linear Personality Probing and Steering in LLMs: A Big Five Study

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Michel Frising, Daniel Balcells

**Keywords:** Large Language Models, Personality, Big Five, Persona, Personality Control

**URL:** https://arxiv.org/abs/2512.17639

#### Abstract (English)

Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.

#### Resumen (Español)

Este artículo, titulado "Linear Personality Probing and Steering in LLMs: A Big Five Study", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-181"></a>

### Artículo 181

**Título original:** Mind Reading or Misreading? LLMs on the Big Five Personality Test

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Francesco Di Cursi, Chiara Boldrini, Marco Conti, Andrea Passarella

**Keywords:** Large Language Models, Personality, Big Five, Bias, Persona

**URL:** https://arxiv.org/abs/2511.23101

#### Abstract (English)

We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.

#### Resumen (Español)

Este artículo, titulado "Mind Reading or Misreading? LLMs on the Big Five Personality Test", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-182"></a>

### Artículo 182

**Título original:** Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Dongmin Choi, Woojung Song, Jongwook Han, Eun-Ju Lee, Yohan Jo

**Keywords:** Large Language Models, Personality, Psychometrics, Persona, Personality Control

**URL:** https://arxiv.org/abs/2509.10078

#### Abstract (English)

Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.

#### Resumen (Español)

Este artículo, titulado "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-183"></a>

### Artículo 183

**Título original:** Investigating Large Language Models in Inferring Personality Traits from User Conversations

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jianfeng Zhu, Ruoming Jin, Karin G. Coifman

**Keywords:** Large Language Models, Personality, Big Five, Persona, Personality Control

**URL:** https://arxiv.org/abs/2501.07532

#### Abstract (English)

Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. This study evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer Big Five personality traits and generate Big Five Inventory-10 (BFI-10) item scores from user conversations under zero-shot prompting conditions. Our findings reveal that incorporating an intermediate step--prompting for BFI-10 item scores before calculating traits--enhances accuracy and aligns more closely with the gold standard than direct trait inference. This structured approach underscores the importance of leveraging psychological frameworks in improving predictive precision. Additionally, a group comparison based on depressive symptom presence revealed differential model performance. Participants were categorized into two groups: those experiencing at least one depressive symptom and those without symptoms. GPT-4o mini demonstrated heightened sensitivity to depression-related shifts in traits such as Neuroticism and Conscientiousness within the symptom-present group, whereas GPT-4o exhibited strengths in nuanced interpretation across groups. These findings underscore the potential of LLMs to analyze real-world psychological data effectively, offering a valuable foundation for interdisciplinary research at the intersection of artificial intelligence and psychology.

#### Resumen (Español)

Este artículo, titulado "Investigating Large Language Models in Inferring Personality Traits from User Conversations", se ubica en la línea de evaluación y validación psicométrica y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Large Language Models (LLMs) are demonstrating remarkable human like capabilities across diverse domains, including psychological assessment. En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-184"></a>

### Artículo 184

**Título original:** Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Nicola Milano, Michela Ponticorvo, Davide Marocco

**Keywords:** Large Language Models, Personality, Big Five, Psychometrics, Persona

**URL:** https://arxiv.org/abs/2503.12080

#### Abstract (English)

In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs. Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment. Graduate psychology students employed the Content Validity Ratio (CVR) to rate test items, forming the human baseline. In parallel, state-of-the-art LLMs, including multilingual and fine-tuned models, analyzed item embeddings to predict construct mappings. The results reveal distinct strengths and limitations of human and AI approaches. Human validators excelled in aligning the behaviorally rich BFQ items, while LLMs performed better with the linguistically concise BFI items. Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs. Here we highlights the complementary potential of hybrid validation systems that integrate human expertise and AI precision. The findings underscore the transformative role of LLMs in psychological assessment, paving the way for scalable, objective, and robust test development methodologies.

#### Resumen (Español)

Este artículo, titulado "Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-185"></a>

### Artículo 185

**Título original:** Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jana Jung, Marlene Lutz, Indira Sen, Markus Strohmaier

**Keywords:** Large Language Models, Personality, Psychometrics, Personality Control, AI Safety

**URL:** https://arxiv.org/abs/2510.11254

#### Abstract (English)

Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests on 17 LLMs for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests on LLMs are essential before interpreting their scores. Our findings also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.

#### Resumen (Español)

Este artículo, titulado "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality", se ubica en la línea de aplicaciones, sesgos y consecuencias sociales y analiza personalidad sintética en modelos de lenguaje. El trabajo reporta evidencia empírica y propone una lectura metodológica para comparar comportamiento, consistencia y efectos de diseño en LLMs. Como contexto del resumen original en inglés, se destaca lo siguiente: Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). En conjunto, el estudio aporta señales útiles para evaluación reproducible y para decisiones de investigación aplicada.

---

<a id="article-186"></a>

### Artículo 186

**Título original:** Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Xiaotian Zhang, Ruizhe Chen, Yang Feng, Zuozhu Liu

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation, LLM Evaluation

**URL:** https://arxiv.org/abs/2504.12663

#### Abstract (English)

Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-187"></a>

### Artículo 187

**Título original:** How Personality Traits Shape LLM Risk-Taking Behaviour

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** John Hartley, Conor Hamill, Devesh Batra, Dale Seddon, Ramin Okhrati, Raad Khraishi

**Keywords:** Large Language Models, Personality, Persona, Bias and Fairness, LLM Evaluation

**URL:** https://arxiv.org/abs/2503.04735

#### Abstract (English)

Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Interventions on GPT-4o's Big Five traits, particularly Openness, significantly influence its risk propensity, mirroring patterns observed in human studies. Notably, Openness emerges as the most influential factor in GPT-4o's risk propensity, aligning with human findings. In contrast, legacy models like GPT-4-Turbo demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and elucidates the potential and limitations of personality-based interventions in shaping LLM decision-making. Our findings have implications for the development of more robust and predictable AI systems such as financial modelling.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para aplicaciones, sesgos y consecuencias sociales. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-188"></a>

### Artículo 188

**Título original:** Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Ke Ji, Yixin Lian, Linxu Li, Jingsheng Gao, Weiyuan Li, Bin Dai

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation, LLM Evaluation

**URL:** https://arxiv.org/abs/2503.17662

#### Abstract (English)

In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \textbf{\underline{P}}ersona-Aware \textbf{\underline{C}}ontrastive \textbf{\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model's role-playing strategy through iterative contrastive learning between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval \&amp; GPT-4) and human expert evaluation.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-189"></a>

### Artículo 189

**Título original:** Modeling, Evaluating, and Embodying Personality in LLMs: A Survey

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Iago Alves Brito, Julia Soares Dollis, Fernanda Bufon Färber, Pedro Schindler Freire Brasil Ribeiro, Rafael Teixeira Sousa, Arlindo Rodrigues Galvão Filho

**Keywords:** Large Language Models, Personality, Psychometrics, Model Evaluation, LLM Evaluation

**URL:** https://doi.org/10.18653/v1/2025.findings-emnlp.506

#### Abstract (English)

This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-190"></a>

### Artículo 190

**Título original:** Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou

**Keywords:** Large Language Models, Personality, Psychometrics, Model Evaluation, LLM Evaluation

**URL:** https://arxiv.org/abs/2511.04499

#### Abstract (English)

As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-191"></a>

### Artículo 191

**Título original:** Character is Destiny: Can Persona-assigned Language Models Make Personal Choices?

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqingdong, Yanghua Xiao

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation, LLM Evaluation

**URL:** https://doi.org/10.18653/v1/2025.findings-emnlp.813

#### Abstract (English)

This paper examines personality-related behavior in large language models and its methodological implications for inducción y control de personalidad. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-192"></a>

### Artículo 192

**Título original:** Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yixiao Wang, Russel Dsouza, Robert Lee, Ian Apperly, Rory Devine, Sanne van der Kleij, Mark Lee

**Keywords:** Large Language Models, Personality, Psychometrics, Model Evaluation, LLM Evaluation

**URL:** https://doi.org/10.18653/v1/2025.clpsych-1.7

#### Abstract (English)

This paper examines personality-related behavior in large language models and its methodological implications for evaluación y validación psicométrica. The study frames personality as an operational variable for evaluating model behavior across prompts, settings, and tasks, and discusses how trait-consistent outputs can be measured and compared with psychometric criteria. It provides evidence useful for reproducible evaluation and for understanding limitations when translating personality findings into deployed AI systems.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-193"></a>

### Artículo 193

**Título original:** Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Haijiang Liu, Qiyuan Li, Chao Gao, Yong Cao, Xiangyu Xu, Xun Wu, Daniel Hershcovich, Jinguang Gu

**Keywords:** Large Language Models, Personality, Persona, Bias and Fairness, LLM Evaluation

**URL:** https://arxiv.org/abs/2508.17855

#### Abstract (English)

Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para aplicaciones, sesgos y consecuencias sociales. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-194"></a>

### Artículo 194

**Título original:** Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R. Fung, Hou Pong Chan, Kevin Small, ChengXiang Zhai, Heng Ji

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation, LLM Evaluation

**URL:** https://arxiv.org/abs/2402.11060

#### Abstract (English)

The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the evaluation of response prediction, Persona-DB demonstrates superior context efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 10% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-195"></a>

### Artículo 195

**Título original:** From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yi-Fei Liu, Yi-Long Lu, Di He, Hang Zhang

**Keywords:** Large Language Models, Personality, Psychometrics, Model Evaluation, LLM Evaluation

**URL:** https://arxiv.org/abs/2511.03235

#### Abstract (English)

Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 &gt; 0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para evaluación y validación psicométrica. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-196"></a>

### Artículo 196

**Título original:** Your Language Model Secretly Contains Personality Subnetworks

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Ruimeng Ye, Zihan Wang, Zinan Ling, Yang Xiao, Manling Li, Xiaolong Ma, Bo Hui

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation, LLM Evaluation

**URL:** https://arxiv.org/abs/2602.07164

#### Abstract (English)

Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

#### Resumen (Español)

Este artículo analiza comportamiento relacionado con personalidad en modelos de lenguaje y sus implicaciones metodológicas para inducción y control de personalidad. El trabajo trata la personalidad como una variable operativa para evaluar respuestas del modelo en distintos prompts, configuraciones y tareas, y discute cómo medir y comparar consistencia de rasgos con criterios psicométricos. Aporta evidencia útil para evaluación reproducible y para delimitar límites al trasladar estos hallazgos a sistemas desplegados.

---

<a id="article-197"></a>

### Artículo 197

**Título original:** A psychometric framework for evaluating and shaping personality traits in large language models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Gregory Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, Maja Matarić

**Keywords:** Large Language Models, Personality, Psychometrics, Model Evaluation, LLM Evaluation

**URL:** https://www.nature.com/articles/s42256-025-01115-6

#### Abstract (English)

The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public worldwide, the synthetic personality traits embedded in these models by virtue of training on large amounts of human data are becoming increasingly important to evaluate. The style in which LLMs respond can mimic different human personality traits. Here, as these patterns can be a key factor determining the effectiveness of communication, we present a comprehensive psychometric methodology for administering and validating personality tests on widely used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found that: personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction-fine-tuned models; and personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible artificial intelligence. Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de evaluación y validación psicométrica. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-198"></a>

### Artículo 198

**Título original:** Chameleon LLMs: User Personas Influence Chatbot Personality Shifts

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jane Xing, Tianyi Niu, Shashank Srivastava

**Keywords:** Large Language Models, Personality, Persona, Model Evaluation, LLM Evaluation

**URL:** https://aclanthology.org/2025.emnlp-main.875/

#### Abstract (English)

As large language models (LLMs) integrate into society, their ability to adapt to users is as critical as their accuracy. While prior work has used personality tests to examine the perceived personalities of LLMs, little research has explored whether LLMs adapt their perceived personalities in response to user interactions. We investigate whether and how LLMs exhibit conversational adaptations over prolonged interactions. Using a controlled simulations where a user and chatbot engage in dialogue, we measure the chatbot’s personality shift before and after the conversation. Across multiple models, we find that traits such as Agreeableness, Extraversion, and Conscientiousness are highly susceptible to user influence, whereas Emotional Stability and Intellect remain relatively more stable. Our results suggest that LLMs dynamically adjust their conversational style in response to user personas, raising important implications for AI alignment, trust, and safety.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de evaluación y validación psicométrica. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-199"></a>

### Artículo 199

**Título original:** From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Shenghan Wu, Yimo Zhu, Wynne Hsu, Mong-Li Lee, Yang Deng

**Keywords:** Large Language Models, Personality, Persona, Emotional Support, LLM Evaluation

**URL:** https://aclanthology.org/2025.emnlp-main.277/

#### Abstract (English)

The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de inducción y control de personalidad. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-200"></a>

### Artículo 200

**Título original:** Neuron based Personality Trait Induction in Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao yang, Xin Zhao, Ji-Rong Wen

**Keywords:** Large Language Models, Personality, Persona, Model Steering, LLM Evaluation

**URL:** https://openreview.net/forum?id=LYHEY783Np

#### Abstract (English)

Large language models (LLMs) have become increasingly proficient at simulating various personality traits, an important capability for supporting related applications (e.g., role-playing). To further improve this capacity, in this paper, we present a neuron based approach for personality trait induction in LLMs, with three major technical contributions. First, we construct PERSONALITYBENCH, a large-scale dataset for identifying and evaluating personality traits in LLMs. This dataset is grounded in the Big Five personality traits from psychology and designed to assess the generative capabilities of LLMs towards specific personality traits. Second, by leveraging PERSONALITYBENCH, we propose an efficient method for identifying personality-related neurons within LLMs by examining the opposite aspects of a given trait. Third, we develop a simple yet effective induction method that manipulates the values of these identified personality-related neurons, which enables fine-grained control over the traits exhibited by LLMs without training and modifying model parameters. Extensive experiments validates the efficacy of our neuron identification and trait induction methods. Notably, our approach achieves comparable performance as fine-tuned models, offering a more efficient and flexible solution for personality trait induction in LLMs.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de inducción y control de personalidad. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-201"></a>

### Artículo 201

**Título original:** Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Florian Dietz, William Wale, Oscar Gilg, Robert McCarthy, Felix Michalak, Gustavo Ewbank Rodrigues Danon, Miguelito de Guzman, Dietrich Klakow

**Keywords:** Large Language Models, Personality, Persona, Safety, LLM Evaluation

**URL:** https://arxiv.org/abs/2602.05532

#### Abstract (English)

Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de inducción y control de personalidad. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-202"></a>

### Artículo 202

**Título original:** ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Tiantian Chen, Jiaqi Lu, Ying Shen, Lin Zhang

**Keywords:** Large Language Models, Personality, Long-term Memory, Emotional Support, LLM Evaluation

**URL:** https://arxiv.org/abs/2602.01885

#### Abstract (English)

Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de aplicaciones, sesgos y consecuencias sociales. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-203"></a>

### Artículo 203

**Título original:** The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Shivam Ratnakar, Sanjay Raghavendra

**Keywords:** Large Language Models, Personality, Behavioral Consistency, Bias and Fairness, LLM Evaluation

**URL:** https://arxiv.org/abs/2510.16712

#### Abstract (English)

Integration of Large Language Models with search/retrieval engines has become ubiquitous, yet these systems harbor a critical vulnerability that undermines their reliability. We present the first systematic investigation of "chameleon behavior" in LLMs: their alarming tendency to shift stances when presented with contradictory questions in multi-turn conversations (especially in search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising 17,770 carefully crafted question-answer pairs across 1,180 multi-turn conversations spanning 12 controversial domains, we expose fundamental flaws in state-of-the-art systems. We introduce two theoretically grounded metrics: the Chameleon Score (0-1) that quantifies stance instability, and Source Re-use Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent failures: all models exhibit severe chameleon behavior (scores 0.391-0.511), with GPT-4o-mini showing the worst performance. Crucially, small across-temperature variance (less than 0.004) suggests the effect is not a sampling artifact. Our analysis uncovers the mechanism: strong correlations between source re-use rate and confidence (r=0.627) and stance changes (r=0.429) are statistically significant (p less than 0.05), indicating that limited knowledge diversity makes models pathologically deferential to query framing. These findings highlight the need for comprehensive consistency evaluation before deploying LLMs in healthcare, legal, and financial systems where maintaining coherent positions across interactions is critical for reliable decision support.

#### Resumen (Español)

Este artículo aborda la evaluación de «personalidad sintética» en LLMs desde la línea de aplicaciones, sesgos y consecuencias sociales. El trabajo aporta evidencia empírica útil para comparar rasgos, estabilidad y efectos contextuales con criterios reproducibles. Su inclusión en esta bitácora refuerza la base metodológica para análisis psicométrico y comparación transversal entre modelos.

---

<a id="article-204"></a>

### Artículo 204

**Título original:** Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models

**Categoría:** Evaluación y validación psicométrica

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Changjin Li, Jiyuan Zhang, Yun Tang, Jian Li

**Keywords:** Large Language Models, Personality Assessment, Situational Judgment Tests, Psychometrics, Big Five

**URL:** https://doi.org/10.1016/j.chbr.2026.100964

#### Abstract (English)

Personality assessment through situational judgment tests (SJTs) offers unique advantages over traditional Likert-type self-report scales, yet their development remains labor-intensive, time-consuming, and heavily dependent on subject matter experts. Recent advances in large language models (LLMs) have shown promise for automatic item generation (AIG). Building on these developments, the present study focuses on developing and evaluating a structured and generalizable framework for automatically generating personality SJTs, using GPT-4 and ChatGPT-5 as empirical examples. Three studies were conducted. Study 1 systematically compared the effects of prompt design and temperature settings on the content validity of LLM-generated items to develop an effective and stable LLM-based AIG approach for personality SJT. Results showed that optimized prompts and a temperature of 1.0 achieved the best balance of creativity and accuracy on GPT-4. Study 2 examined the cross-model generalizability and reproducibility of this automated SJT generation approach through multiple rounds. The results showed that the approach consistently produced reproducible and high-quality items on ChatGPT-5. Study 3 evaluated the psychometric properties of LLM-generated SJTs covering five facets of the Big Five personality traits. Results demonstrated satisfactory reliability and validity across most facets, though limitations were observed in the convergent validity of the compliance facet and certain aspects of criterion-related validity. These findings provide robust evidence that the proposed LLM-based AIG approach can produce culturally appropriate and psychometrically sound SJTs with efficiency comparable to or exceeding traditional methods.

#### Resumen (Español)

Este estudio aborda la generación automática de ítems para pruebas situacionales de personalidad con modelos de lenguaje, un proceso que tradicionalmente requiere mucho tiempo y fuerte dependencia de expertos humanos. El trabajo propone un marco estructurado y reutilizable para crear ítems de SJT orientados a rasgos de personalidad, y lo evalúa con GPT-4 y ChatGPT-5 en varios experimentos. La investigación pone el foco en validez de contenido, reproducibilidad entre corridas y transferencia entre modelos.

Los resultados muestran que el diseño del prompt y la temperatura influyen de forma crítica en la calidad psicométrica de los ítems generados. Con configuraciones optimizadas, los autores reportan mejor equilibrio entre creatividad y precisión, además de consistencia en la calidad de los ítems al repetir el proceso en distintos ciclos de generación. Esto sugiere que la generación automática no solo puede acelerar el desarrollo de instrumentos, sino también mantener estándares técnicos razonables cuando se controla el protocolo.

En la evaluación final, los SJT generados por LLMs alcanzan niveles satisfactorios de fiabilidad y validez en la mayoría de facetas analizadas del modelo Big Five, aunque persisten limitaciones en algunas dimensiones concretas. En conjunto, el artículo aporta evidencia útil para integrar LLMs en desarrollo psicométrico aplicado, con implicaciones directas para escalabilidad, coste y adaptación cultural de instrumentos de personalidad.

---

<a id="article-205"></a>

### Artículo 205

**Título original:** Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Lucio La Cava, Andrea Tagarelli

**Keywords:** Large Language Models, Personality Mimicry, MBTI, Big Five, Personality Conditioning

**URL:** https://doi.org/10.1609/aaai.v39i2.32125

#### Abstract (English)

The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. However, research on the personalities exhibited by LLMs has largely been confined to limited investigations using individual psychological tests, primarily focusing on a small number of commercially licensed LLMs. This approach overlooks the extensive use and significant advancements observed in open-source LLMs. This work aims to address both the above limitations by conducting an in-depth investigation of a significant body of 12 LLM Agents based on the most representative Open models, through the two most well-known psychological assessment tests, namely Myers-Briggs Type Indicator (MBTI) and Big Five Inventory (BFI). Our approach involves evaluating the intrinsic personality traits of LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that (i) each LLM agent showcases distinct human personalities; (ii) personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and (iii) combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of LLMs.

#### Resumen (Español)

Este trabajo examina en profundidad si los LLMs abiertos pueden imitar personalidades humanas cuando se les condiciona con rasgos y roles específicos. Frente a estudios previos centrados en pocos modelos comerciales, los autores analizan 12 agentes basados en modelos abiertos y aplican dos marcos de evaluación ampliamente usados en psicología: MBTI y Big Five. El objetivo principal es distinguir entre rasgos intrínsecos del modelo y rasgos inducidos por prompting.

Los resultados muestran que cada agente presenta un perfil propio relativamente estable, y que el condicionamiento por personalidad no tiene el mismo efecto en todos los modelos. Solo algunos logran aproximarse de forma convincente a la personalidad objetivo, mientras que muchos mantienen su tendencia base, lo que los autores describen como comportamiento "closed-minded". Esta evidencia es especialmente relevante para diseño de agentes donde se espera control fino sobre estilo y comportamiento.

Además, el estudio encuentra que combinar rol más personalidad mejora la capacidad de mimetización frente a usar solo rasgos aislados. En términos prácticos, el artículo aporta criterios para inducir personalidad de manera más efectiva en LLMs abiertos y delimita hasta dónde llega ese control sin ajuste de parámetros, con implicaciones para robustez, coherencia y seguridad conductual.

---

<a id="article-206"></a>

### Artículo 206

**Título original:** On the emergent capabilities of ChatGPT 4 to estimate personality traits

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Marco Piastra, Patrizia Catellani

**Keywords:** ChatGPT-4, Personality Trait Estimation, Big Five, Text Analysis, Psychometrics

**URL:** https://doi.org/10.3389/frai.2025.1484260

#### Abstract (English)

This study investigates the potential of ChatGPT 4 in the assessment of personality traits based on written texts. Using two publicly available datasets containing both written texts and self-assessments of the authors’ psychological traits based on the Big Five model, we aimed to evaluate the predictive performance of ChatGPT 4. For each sample text, we asked for numerical predictions on an eleven-point scale and compared them with the self-assessments. We also asked for ChatGPT 4 confidence scores on an eleven-point scale for each prediction. To keep the study within a manageable scope, a zero-prompt modality was chosen, although more sophisticated prompting strategies could potentially improve performance. The results show that ChatGPT 4 has moderate but significant abilities to automatically infer personality traits from written text. However, it also shows limitations in recognizing whether the input text is appropriate or representative enough to make accurate inferences, which could hinder practical applications. Furthermore, the results suggest that improved benchmarking methods could increase the efficiency and reliability of the evaluation process. These results pave the way for a more comprehensive evaluation of the capabilities of Large Language Models in assessing personality traits from written texts.

#### Resumen (Español)

El artículo evalúa la capacidad de ChatGPT-4 para inferir rasgos de personalidad a partir de texto escrito, usando datasets públicos con autoevaluaciones Big Five como referencia. El protocolo compara predicciones numéricas del modelo con las puntuaciones de los propios autores de los textos, e incorpora además niveles de confianza reportados por el sistema. La configuración empleada es de zero-prompt para aislar el rendimiento base.

Los autores encuentran que ChatGPT-4 logra una capacidad de inferencia moderada pero estadísticamente significativa en varias dimensiones, lo que sugiere potencial para tareas de perfilado psicológico automatizado. Al mismo tiempo, el modelo muestra limitaciones para discriminar cuándo una muestra textual es suficientemente representativa para sostener una inferencia fiable, un punto clave para evitar sobreinterpretaciones.

Como implicación metodológica, el trabajo subraya la necesidad de benchmarks más sólidos y comparables para evaluar personalidad en LLMs. Su contribución se alinea con una agenda psicométrica de validación incremental: medir rendimiento real, cuantificar incertidumbre y definir condiciones de uso antes de trasladar estas herramientas a contextos aplicados sensibles.

---

<a id="article-207"></a>

### Artículo 207

**Título original:** Rethinking psychometrics through LLMs: how item semantics shape measurement and prediction in psychological questionnaires

**Categoría:** Evaluación y validación psicométrica

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Federico Ravenda, Antonio Preti, Michele Poletti, Antonietta Mira, Andrea Raballo

**Keywords:** Psychometrics, Questionnaire Semantics, Big Five, LLMs, Measurement Validity

**URL:** https://doi.org/10.1038/s41598-025-21289-8

#### Abstract (English)

Abstract Psychological questionnaires are typically designed to measure latent constructs by asking respondents a series of semantically related questions. But what if these semantic relationships, rather than reflecting only the underlying construct, also impose their own structure on the data we collect? In other words, to what extent is what we “measure” in questionnaires shaped a priori by item semantics rather than revealed solely a posteriori through empirical correlations? To examine this epistemological question, we propose LLMs Psychometrics, a novel paradigm that harness LLMs to investigate how the semantic structure of questionnaire items influences psychometric outcomes. We hypothesize that the correlations among items partly mirror their linguistic similarity, such that LLMs can predict these correlations-even in the absence of empirical data. To test this, we compared actual correlation matrices from established instruments-the Big 5 Personality (Big 5) and Depression Anxiety Stress Scale (DASS-42)-with the semantic similarity structures computed by LLMs. Among the top 3 semantically similar items, the empirically most correlated item was found in 95% of DASS cases and 82% of Big 5 cases. Building on this, we developed PsychoLLM, a neural proof-of-concept architecture, which uses item semantics to predict responses to new items-demonstrated with the Generalized Anxiety Disorder-7 (GAD-7) and Patient Health Questionnaire-9 (PHQ-9). PsychoLLM achieved 70% accuracy when predicting one scale’s responses from the other, enabling new analyses based on semantic relationships. This work underscores an important epistemological implication for psychometrics: item semantics may influence measurement outcomes to varying degrees, more extensively than previously assumed. By leveraging LLMs to expose this a priori semantic structure, researchers can refine questionnaire design, assess data quality, and expand interpretive possibilities, ultimately inviting a reexamination of “ what ” and “ how ” we truly measure in psychology.

#### Resumen (Español)

Este trabajo plantea una pregunta central para la psicometría: hasta qué punto los cuestionarios miden constructos latentes y hasta qué punto reflejan la propia semántica de los ítems. Para responderla, los autores proponen un marco llamado "LLMs Psychometrics", donde modelos de lenguaje estiman relaciones entre preguntas antes de observar datos empíricos tradicionales. El estudio se apoya en instrumentos consolidados como Big Five y DASS-42.

Los resultados muestran una correspondencia alta entre similitud semántica y correlaciones observadas entre ítems, lo que sugiere que parte de la estructura psicométrica puede estar condicionada por el lenguaje del test. Sobre esa base, el artículo introduce PsychoLLM, un prototipo que usa semántica de ítems para predecir respuestas en escalas distintas (GAD-7 y PHQ-9), alcanzando niveles de precisión relevantes en tareas cruzadas.

La contribución principal no es solo técnica sino epistemológica: invita a revisar supuestos clásicos sobre validez de medición y construcción de instrumentos. En el contexto de «personalidad sintética» y evaluación de LLMs, aporta una ruta para auditar cuestionarios, depurar artefactos semánticos y mejorar la interpretación de resultados psicométricos en investigación computacional.

---

<a id="article-208"></a>

### Artículo 208

**Título original:** Comparing chatbots to psychometric tests in hiring: reduced social desirability bias, but lower predictive validity

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** D Dukanović, Dario Krpan

**Keywords:** Hiring, Chatbots, Psychometric Testing, Social Desirability Bias, Predictive Validity

**URL:** https://doi.org/10.3389/fpsyg.2025.1564979

#### Abstract (English)

This paper explores the efficacy of AI-driven chatbots in accurately inferring personality traits compared to traditional psychometric tests within a real-world professional hiring context. The study is driven by the increasing integration of AI tools in recruitment processes, which necessitates a deeper understanding of their reliability and validity. Using a quasi-experimental design with propensity score matching, we analysed data from 159 candidates and other professionals from Serbian and Montenegrin regions who completed both traditional psychometric assessments and AI-based personality evaluations based on the Big Five Personality model. A novel one-question-per-facet approach was employed in the chatbot assessments with a goal of enabling more granular analysis of the chatbot’s psychometric properties. The findings indicate that the chatbot demonstrated good structural, substantive, and convergent validity for certain traits, particularly Extraversion and Conscientiousness, but not for Neuroticism, Agreeableness, and Openness. While robust regression confirmed that AI-inferred scores are less susceptible to social desirability bias than traditional tests, they did not significantly predict real-world outcomes, indicating issues with external validity, particularly predictive validity. The results suggest that AI-driven chatbots show promise for identifying certain personality traits and demonstrate resistance to social desirability bias. This paper contributes to the emerging field of AI and psychometrics by offering insights into the potential and limitations of AI tools in professional selection, while developing an approach for refining psychometric properties of AI-driven assessments.

#### Resumen (Español)

El estudio compara evaluaciones de personalidad basadas en chatbots con pruebas psicométricas tradicionales en un contexto real de selección laboral. Con un diseño cuasi-experimental y técnicas de emparejamiento estadístico, analiza datos de 159 participantes que completaron ambos tipos de evaluación bajo el marco Big Five. El trabajo busca medir si los sistemas de IA aportan ventajas prácticas sin sacrificar validez.

Los resultados indican que los chatbots muestran desempeño razonable en algunas dimensiones, especialmente extraversión y responsabilidad, y parecen menos vulnerables al sesgo de deseabilidad social que los tests clásicos. Sin embargo, su capacidad para predecir resultados externos reales es más limitada, lo que afecta la validez predictiva en decisiones de contratación.

Desde la perspectiva de aplicaciones y riesgo, el artículo sugiere una postura equilibrada: los chatbots pueden complementar procesos de evaluación, pero no deberían reemplazar instrumentos consolidados sin evidencia adicional. La contribución es relevante para gobernanza de IA en RRHH, donde sesgo, transparencia y consecuencias de error tienen impacto directo en personas y organizaciones.

---

<a id="article-209"></a>

### Artículo 209

**Título original:** Large language models display human-like social desirability biases in Big Five personality surveys

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Aadesh Salecha, Molly Ireland, Shashanka Subrahmanya, João Sedoc, Lyle Ungar, Johannes C. Eichstaedt

**Keywords:** Large Language Models, Social Desirability Bias, Big Five, Survey Methods, Behavioral Bias

**URL:** https://doi.org/10.1093/pnasnexus/pgae533

#### Abstract (English)

Abstract Large language models (LLMs) are becoming more widely used to simulate human participants and so understanding their biases is important. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e. increased extraversion, decreased neuroticism, etc.). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4’s survey responses changing by 1.20 (human) SD and Llama 3’s by 0.98 SD, which are very large effects. This bias remains after question order randomization and paraphrasing. Reverse coding the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on this use of LLMs as proxies for human participants.

#### Resumen (Español)

Este artículo identifica un sesgo de deseabilidad social en múltiples LLMs cuando responden encuestas de personalidad Big Five. Los autores diseñan un marco experimental para comprobar si los modelos detectan que están siendo evaluados y, en ese caso, ajustan sus respuestas hacia perfiles socialmente más aceptables. El fenómeno se observa en modelos comerciales y abiertos, incluyendo GPT, Claude, Llama y PaLM.

El estudio reporta efectos de magnitud alta y robustos a varias pruebas de control, como aleatorización de orden de preguntas y parafraseo de ítems. Aunque el recodificado inverso reduce parte del efecto, no lo elimina, lo que sugiere que no se trata solo de aquiescencia sino de un patrón más profundo en la conducta del modelo ante contextos evaluativos.

Las implicaciones son relevantes para investigación y práctica: usar LLMs como sustitutos de participantes humanos o para perfilado psicométrico directo puede introducir distorsiones sistemáticas. En términos de «personalidad sintética», el trabajo refuerza la necesidad de protocolos que separen señales de rasgo de artefactos de respuesta inducidos por el propio formato de medición.

---

<a id="article-210"></a>

### Artículo 210

**Título original:** Can Large Language Models Assess Personality From Asynchronous Video Interviews? A Comprehensive Evaluation of Validity, Reliability, Fairness, and Rating Patterns

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Tianyi Zhang, Antonis Koutsoumpis, Janneke K. Oostrom, Djurre Holtrop, Sina Ghassemi, Reinout E. de Vries

**Keywords:** Large Language Models, Asynchronous Video Interviews, Personality Assessment, Fairness, Reliability

**URL:** https://doi.org/10.1109/taffc.2024.3374875

#### Abstract (English)

The advent of Artificial Intelligence (AI) technologies has precipitated the rise of asynchronous video interviews (AVIs) as an alternative to conventional job interviews. These one-way video interviews are conducted online and can be analyzed using AI algorithms to automate and speed up the selection procedure. In particular, the swift advancement of Large Language Models (LLMs) has significantly decreased the cost and technical barrier to developing AI systems for automatic personality and interview performance evaluation. However, the generative and task-unspecific nature of LLMs might pose potential risks and biases when evaluating humans based on their AVI responses. In this study, we conducted a comprehensive evaluation of the validity, reliability, fairness, and rating patterns of two widely-used LLMs, GPT-3.5 and GPT-4, in assessing personality and interview performance from an AVI. We compared the personality and interview performance ratings of the LLMs with the ratings from a task-specific AI model and human annotators using simulated AVI responses of 685 participants. The results show that LLMs can achieve similar or even better zero-shot validity compared with the task-specific AI model when predicting personality traits. The verbal explanations for predicting personality traits generated by LLMs are interpretable by the personality items that are designed according to psychological theories. However, LLMs also suffered from uneven performance across different traits, insufficient test-retest reliability, and the emergence of certain biases. Thus, it is necessary to exercise caution when applying LLMs for human-related application scenarios, especially for significant decisions such as employment.

#### Resumen (Español)

El trabajo evalúa si GPT-3.5 y GPT-4 pueden valorar rasgos de personalidad y desempeño en entrevistas asíncronas de video (AVI) con un nivel de calidad comparable a sistemas especializados y evaluadores humanos. El estudio compara validez, fiabilidad, equidad y patrones de puntuación sobre respuestas simuladas de 685 participantes, en un escenario aplicado de selección laboral.

Los resultados muestran que los LLMs pueden alcanzar buena validez en configuración zero-shot, e incluso competir con modelos específicos de tarea en algunas métricas. También se reporta que las explicaciones textuales de los modelos son interpretables desde marcos psicológicos, lo que aporta valor práctico para auditoría cualitativa.

No obstante, aparecen límites importantes: rendimiento desigual según rasgo, fiabilidad temporal insuficiente y señales de sesgo. Por ello, el artículo concluye que su uso en decisiones de alto impacto debe hacerse con cautela y bajo controles metodológicos estrictos, aportando evidencia clave para la agenda de validación psicométrica en sistemas de IA aplicados a personas.

---

<a id="article-211"></a>

### Artículo 211

**Título original:** The Personality Dimensions GPT-3 Expresses During Human-Chatbot Interactions

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Nikola Kovačević, Christian Holz, Markus Groß, Rafael Wampfler

**Keywords:** GPT-3, Human-Chatbot Interaction, Personality Perception, Factor Analysis, Conversational Agents

**URL:** https://doi.org/10.1145/3659626

#### Abstract (English)

Large language models such as GPT-3 and ChatGPT can mimic human-to-human conversation with unprecedented fidelity, which enables many applications such as conversational agents for education and non-player characters in video games. In this work, we investigate the underlying personality structure that a GPT-3-based chatbot expresses during conversations with a human. We conducted a user study to collect 147 chatbot personality descriptors from 86 participants while they interacted with the GPT-3-based chatbot for three weeks. Then, 425 new participants rated the 147 personality descriptors in an online survey. We conducted an exploratory factor analysis on the collected descriptors and show that, though overlapping, human personality models do not fully transfer to the chatbot's personality as perceived by humans. We also show that the perceived personality is significantly different from that of virtual personal assistants, where users focus rather on serviceability and functionality. We discuss the implications of ever-evolving large language models and the change they affect in users' perception of agent personalities.

#### Resumen (Español)

Este artículo estudia la estructura de personalidad que los usuarios perciben en un chatbot basado en GPT-3 durante interacciones prolongadas. A partir de descriptores recogidos en un estudio de uso de tres semanas y una segunda fase de valoración con más participantes, los autores aplican análisis factorial exploratorio para identificar dimensiones latentes de personalidad percibida.

Los resultados indican que, aunque existen solapamientos con modelos de personalidad humana, la estructura percibida en el chatbot no coincide plenamente con taxonomías humanas estándar. Además, la personalidad atribuida a este tipo de agente difiere de la que los usuarios asignan a asistentes virtuales centrados en utilidad funcional, lo que sugiere que la calidad conversacional modifica el marco psicológico de evaluación del sistema.

El trabajo aporta evidencia relevante para medir «personalidad sintética» en interacción real y para diseñar métodos de evaluación más adaptados a agentes conversacionales. En conjunto, refuerza la idea de que no basta con trasladar instrumentos humanos sin ajustes, y que la percepción de personalidad en IA requiere marcos específicos de validación.

---

<a id="article-212"></a>

### Artículo 212

**Título original:** Personality prediction from task-oriented and open-domain human-machine dialogues

**Categoría:** Evaluación y validación psicométrica

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Ao Guo, Ryu Hirai, Atsumoto Ohashi, Yuya Chiba, Yuiko Tsunomori, Ryuichiro Higashinaka

**Keywords:** Personality Prediction, Human-Machine Dialogue, MBTI, Big Five, Dialogue Systems

**URL:** https://doi.org/10.1038/s41598-024-53989-y

#### Abstract (English)

Abstract If a dialogue system can predict the personality of a user from dialogue, it will enable the system to adapt to the user’s personality, leading to better task success and user satisfaction. In a recent study, personality prediction was performed using the Myers-Briggs Type Indicator (MBTI) personality traits with a task-oriented human-machine dialogue using an end-to-end (neural-based) system. However, it is still not clear whether such prediction is generally possible for other types of systems and user personality traits. To clarify this, we recruited 378 participants, asked them to fill out four personality questionnaires covering 25 personality traits, and had them perform three rounds of human-machine dialogue with a pipeline task-oriented dialogue system or an end-to-end task-oriented dialogue system. We also had another 186 participants do the same with an open-domain dialogue system. We then constructed BERT-based models to predict the personality traits of the participants from the dialogues. The results showed that prediction accuracy was generally better with open-domain dialogue than with task-oriented dialogue, although Extraversion (one of the Big Five personality traits) could be predicted equally well for both open-domain dialogue and pipeline task-oriented dialogue. We also examined the effect of utilizing different types of dialogue on personality prediction by conducting a cross-comparison of the models trained from the task-oriented and open-domain dialogues. As a result, we clarified that the open-domain dialogue cannot be used to predict personality traits from task-oriented dialogue, and vice versa. We further analyzed the effects of system utterances, task performance, and the round of dialogue with regard to the prediction accuracy.

#### Resumen (Español)

El estudio analiza si es posible predecir rasgos de personalidad de usuarios a partir de diálogos humano-máquina en distintos tipos de sistemas conversacionales. Para ello combina varios cuestionarios de personalidad, múltiples rondas de conversación y modelos BERT entrenados sobre interacciones task-oriented y open-domain, con una muestra amplia de participantes.

Los resultados indican que el rendimiento de predicción suele ser mejor en diálogo abierto que en diálogo orientado a tarea, aunque algunos rasgos concretos, como extraversión, mantienen niveles comparables en ciertos escenarios task-oriented. Además, el análisis cruzado muestra una baja transferibilidad entre dominios: modelos entrenados en un tipo de diálogo no generalizan bien al otro.

Esta evidencia tiene implicaciones directas para evaluación psicométrica en LLMs y sistemas conversacionales: el contexto de interacción condiciona qué rasgos son inferibles y con qué fiabilidad. El artículo aporta una base metodológica sólida para evitar extrapolaciones indebidas y para diseñar protocolos de inferencia de personalidad sensibles al tipo de diálogo.

---

### Artículo 213

**Título original:** LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Minqian Liu, Zhiyang Xu, Xinyi Zhang, Huaming An, Sarvech Qadir, Qi Zhang, Pamela Wiśniewski, Jin-Hee Cho, Sang Won Lee, Ruoxi Jia, Lifu Huang

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2504.10430

#### Abstract (English)

Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.

#### Resumen (Español)

Este trabajo examina llm can be a dangerous persuader: empirical study of persuasion safety in large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 214

**Título original:** Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Nicole Smith-Vaniz, Harper Lyon, Lorraine Steigner, Ben Armstrong, Nicholas Mattei

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://doi.org/10.1609/aies.v8i3.36727

#### Abstract (English)

Large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to measure differences in human participants along political, national, and cultural lines. While there has been some analysis of the responses of LLM with respect to political stance in role-playing scenarios, no work so far has directly assessed the moral leanings in the LLM responses, nor have they connected LLM outputs with robust human data. In this paper we analyze the distinctions between LLM MFT responses and existing human research directly, investigating whether commonly available LLM responses demonstrate ideological leanings — either through their inherent responses, straightforward representations of political ideologies, or when responding from the perspectives of constructed human personas. We assess whether LLMs inherently generate responses that align more closely with one political ideology over another, and additionally examine how accurately LLMs can represent ideological perspectives through both explicit prompting and demographic-based role-playing. By systematically analyzing LLM behavior across these conditions and experiments, our study provides insight into the extent of political and demographic dependency in AI-generated responses.

#### Resumen (Español)

Este trabajo examina investigating political and demographic associations in large language models through moral foundations theory dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 215

**Título original:** GermanPartiesQA: Benchmarking Commercial Large Language Models and AI Companions for Political Alignment and Sycophancy

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Jan Batzner, Volker Stocker, Stefan Schmid, Gjergji Kasneci

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://doi.org/10.1609/aies.v8i1.36552

#### Abstract (English)

Large language models (LLMs) are increasingly shaping citizens’ information ecosystems. Products incorporating LLMs, such as chatbots and AI Companions, are now widely used for decision support and information retrieval, including in sensitive domains, raising concerns about hidden biases and growing potential to shape individual decisions and public opinion. This paper introduces GermanPartiesQA, a benchmark of 418 political statements from German Voting Advice Applications across 11 elections to evaluate six commercial LLMs. We evaluate their political alignment based on role-playing experiments with political personas. Our evaluation reveals three specific findings: (1) Factual limitations: LLMs show limited ability to accurately generate factual party positions, particularly for centrist parties. (2) Model-specific ideological alignment: We identify consistent alignment patterns and degree of political steerability for each model across temperature settings and experiments. (3) Claim of sycophancy: While models adjust to political personas during role-play, we find this reflects persona-based steerability rather than the increasingly popular, yet contested concept of sycophancy. Our study contributes to evaluating the political alignment of closed-source LLMs that are increasingly embedded in electoral decision support tools and AI Companion chatbots.

#### Resumen (Español)

Este trabajo examina germanpartiesqa: benchmarking commercial large language models and ai companions for political alignment and sycophancy dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large language models (LLMs) are increasingly shaping citizens’ information ecosystems. Products incorporating LLMs, such as chatbots and AI Companions, are now widely used for decision support and information retrieval, including in sensitive domains, raising concerns about hidden biases and growing potential to shape individual decisions and public opinion. This paper introduces GermanPartiesQA, a benchmark of 418 political statements from German Voting Advice Applications across 11 elections to evaluate six commercial LLMs. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 216

**Título original:** LLMs Generate Structurally Realistic Social Networks but Overestimate Political Homophily

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Serina Chang, Alicja Chaszczewicz, Emma Wang, Maya Josifovska, Emma Pierson, Jure Leskovec

**Keywords:** Homophily, Politics, Social network (sociolinguistics), Computer science, Sociology

**URL:** https://doi.org/10.1609/icwsm.v19i1.35820

#### Abstract (English)

Generating social networks is essential for many applications, such as epidemic modeling and social simulations. The emergence of generative AI, especially large language models (LLMs), offers new possibilities for social network generation: LLMs can generate networks without additional training or need to define network parameters, and users can flexibly define individuals in the network using natural language. However, this potential raises two critical questions: 1) are the social networks generated by LLMs realistic, and 2) what are risks of bias, given the importance of demographics in forming social ties? To answer these questions, we develop three prompting methods for network generation and compare the generated networks to a suite of real social networks. We find that more realistic networks are generated with “local” methods, where the LLM constructs relations for one persona at a time, compared to “global” methods that construct the entire network at once. We also find that the generated networks match real networks on many characteristics, including density, clustering, connectivity, and degree distribution. However, we find that LLMs emphasize political homophily over all other types of homophily and significantly overestimate political homophily compared to real social networks.

#### Resumen (Español)

Este trabajo examina llms generate structurally realistic social networks but overestimate political homophily dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que generating social networks is essential for many applications, such as epidemic modeling and social simulations. The emergence of generative AI, especially large language models (LLMs), offers new possibilities for social network generation: LLMs can generate networks without additional training or need to define network parameters, and users can flexibly define individuals in the network using natural language. However, this potential raises two critical questions: 1) are the social networks generated by LLMs realistic, and 2) what are risks of bias, given the importance of demographics in forming social ties? En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 217

**Título original:** Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Bo Tan, Roy Ka-Wei Lee

**Keywords:** Persona, Power (physics), Computer science, Implicit bias, Social psychology

**URL:** https://aclanthology.org/2025.naacl-long.50/

#### Abstract (English)

Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a "default persona" bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover, interactions involving specific demographics are associated with lower-quality responses. Lastly, the presence of power disparities increases variability in response semantics and quality across demographic groups, suggesting that implicit biases may be heightened under power-imbalanced conditions. These insights expose the demographic biases inherent in LLMs and offer potential paths toward future bias mitigation efforts in LLMs.

#### Resumen (Español)

Este trabajo examina unmasking implicit bias: evaluating persona-prompted llm responses in power-disparate social scenarios dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 218

**Título original:** Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yuxuan Li, Hirokazu Shirado, Sauvik Das

**Keywords:** Psychology, Cognitive psychology, Social psychology, Linguistics, Computer science

**URL:** https://arxiv.org/abs/2501.17420

#### Abstract (English)

While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.

#### Resumen (Español)

Este trabajo examina actions speak louder than words: agent decisions reveal implicit biases in language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que while advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 219

**Título original:** Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Shuzhou Yuan, Ercong Nie, Mario Tawfelis, Helmut Schmid, Hinrich Schütze, Michael Färber

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2506.08593

#### Abstract (English)

Hate speech detection is a socially sensitive and inherently subjective task, with judgments often varying based on personal traits. While prior work has examined how socio-demographic factors influence annotation, the impact of personality traits on Large Language Models (LLMs) remains largely unexplored. In this paper, we present the first comprehensive study on the role of persona prompts in hate speech classification, focusing on MBTI-based traits. A human annotation survey confirms that MBTI dimensions significantly affect labeling behavior. Extending this to LLMs, we prompt four open-source models with MBTI personas and evaluate their outputs across three hate speech datasets. Our analysis uncovers substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases. These findings highlight the need to carefully define persona prompts in LLM-based annotation workflows, with implications for fairness and alignment with human values.

#### Resumen (Español)

Este trabajo examina hateful person or hateful model? investigating the role of personas in hate speech detection by large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que hate speech detection is a socially sensitive and inherently subjective task, with judgments often varying based on personal traits. While prior work has examined how socio-demographic factors influence annotation, the impact of personality traits on Large Language Models (LLMs) remains largely unexplored. In this paper, we present the first comprehensive study on the role of persona prompts in hate speech classification, focusing on MBTI-based traits. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 220

**Título original:** Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Saloni Dash, Amélie Reymond, Emma S. Spiro, Aylin Caliskan

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2506.20020

#### Abstract (English)

Reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. Here, we investigate whether assigning 8 personas across 4 political and socio-demographic attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and proprietary) across two reasoning tasks from human-subject studies -- veracity discernment of misinformation headlines and evaluation of numeric scientific evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects. Taken together, our empirical findings are the first to suggest that persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts -- raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

#### Resumen (Español)

Este trabajo examina persona-assigned large language models exhibit human-like motivated reasoning dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 221

**Título original:** The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Konrad Löhr, Shuzhou Yuan, Michael Färber

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2510.08236

#### Abstract (English)

Large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the inherent political leanings of these models. Subsequently, persona prompting with the PCT is used to explore explicit stereotypes across various social dimensions. In a final step, implicit stereotypes are uncovered by evaluating models with multilingual versions of the PCT. Key findings reveal a consistent left-leaning political alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those identified via explicit persona prompting. Interestingly, for most models, implicit and explicit stereotypes show a notable alignment, suggesting a degree of transparency or "awareness" regarding their inherent biases. This study underscores the complex interplay of political bias and stereotypes in LLMs.

#### Resumen (Español)

Este trabajo examina the hidden bias: a study on explicit and implicit political stereotypes in large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 222

**Título original:** Destination (Un)Known: Auditing Bias and Fairness in LLM-Based Travel Recommendations

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Hristo Andreev, Petros Kosmas, Antonios D. Livieratos, Antonis L. Theocharous, Anastasios Zopiatis

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://doi.org/10.3390/ai6090236

#### Abstract (English)

Large language-model chatbots such as ChatGPT and DeepSeek are quickly gaining traction as an easy, first-stop tool for trip planning because they offer instant, conversational advice that once required sifting through multiple websites or guidebooks. Yet little is known about the biases that shape the destination suggestions these systems provide. This study conducts a controlled, persona-based audit of the two models, generating 6480 recommendations for 216 traveller profiles that vary by origin country, age, gender identity and trip theme. Six observable bias families (popularity, geographic, cultural, stereotype, demographic and reinforcement) are quantified using tourism rankings, Hofstede scores, a 150-term cliché lexicon and information-theoretic distance measures. Findings reveal measurable bias in every bias category. DeepSeek is more likely than ChatGPT to suggest off-list cities and recommends domestic travel more often, while both models still favour mainstream destinations. DeepSeek also points users toward culturally more distant destinations on all six Hofstede dimensions and employs a denser, superlative-heavy cliché register; ChatGPT shows wider lexical variety but remains strongly promotional. Demographic analysis uncovers moderate gender gaps and extreme divergence for non-binary personas, tempered by a “protective” tendency to guide non-binary travellers toward countries with higher LGBTQI acceptance. Reinforcement bias is minimal, with over 90 percent of follow-up suggestions being novel in both systems. These results confirm that unconstrained LLMs are not neutral filters but active amplifiers of structural imbalances. The paper proposes a public-interest re-ranking layer, hosted by a body such as UN Tourism, that balances exposure fairness, seasonality smoothing, low-carbon routing, cultural congruence, safety safeguards and stereotype penalties, transforming conversational AI from an opaque gatekeeper into a sustainability-oriented travel recommendation tool.

#### Resumen (Español)

Este trabajo examina destination (un)known: auditing bias and fairness in llm-based travel recommendations dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large language-model chatbots such as ChatGPT and DeepSeek are quickly gaining traction as an easy, first-stop tool for trip planning because they offer instant, conversational advice that once required sifting through multiple websites or guidebooks. Yet little is known about the biases that shape the destination suggestions these systems provide. This study conducts a controlled, persona-based audit of the two models, generating 6480 recommendations for 216 traveller profiles that vary by origin country, age, gender identity and trip theme. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 223

**Título original:** Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Chandan Kumar Sah, Xiaoli Lian, Li Zhang, Tony Xu, Syed Saqib Hussain Shah

**Keywords:** Computer science, Categorical variable, Recommender system, Personalization, Benchmark (surveying)

**URL:** https://arxiv.org/abs/2602.02582

#### Abstract (English)

Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

#### Resumen (Español)

Este trabajo examina uncertainty and fairness awareness in llm-based recommendation systems dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 224

**Título original:** Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Hadi Hosseini, Samarth Khanna

**Keywords:** Distributive property, Distributive justice, Computer science, Psychology, Economics

**URL:** https://arxiv.org/abs/2502.00313

#### Abstract (English)

The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g., intentions or personas) or non-semantic prompting changes (e.g., templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.

#### Resumen (Español)

Este trabajo examina distributive fairness in large language models: evaluating alignment with human values dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que the growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 225

**Título original:** Who Gets the Callback? Generative AI and Gender Bias

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Sugat Chaturvedi, Rochana Chaturvedi

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2504.21400

#### Abstract (English)

Generative artificial intelligence (AI), particularly large language models (LLMs), is being rapidly deployed in recruitment and for candidate shortlisting. We audit several mid-sized open-source LLMs for gender bias using a dataset of 332,044 real-world online job postings. For each posting, we prompt the model to recommend whether an equally qualified male or female candidate should receive an interview callback. We find that most models tend to favor men, especially for higher-wage roles. Mapping job descriptions to the Standard Occupational Classification system, we find lower callback rates for women in male-dominated occupations and higher rates in female-associated ones, indicating occupational segregation. A comprehensive analysis of linguistic features in job ads reveals strong alignment of model recommendations with traditional gender stereotypes. To examine the role of recruiter identity, we steer model behavior by infusing Big Five personality traits and simulating the perspectives of historical figures. We find that less agreeable personas reduce stereotyping, consistent with an agreeableness bias in LLMs. Our findings highlight how AI-driven hiring may perpetuate biases in the labor market and have implications for fairness and diversity within firms.

#### Resumen (Español)

Este trabajo examina who gets the callback? generative ai and gender bias dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que generative artificial intelligence (AI), particularly large language models (LLMs), is being rapidly deployed in recruitment and for candidate shortlisting. We audit several mid-sized open-source LLMs for gender bias using a dataset of 332,044 real-world online job postings. For each posting, we prompt the model to recommend whether an equally qualified male or female candidate should receive an interview callback. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 226

**Título original:** Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yukun Zhao, Zhen Huang, Martin E. P. Seligman, Kaiping Peng

**Keywords:** Prosocial behavior, Generosity, Psychology, Social psychology, Operationalization

**URL:** https://doi.org/10.1038/s41598-024-55949-y

#### Abstract (English)

Emotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes 'emotion' within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots—specifically, large language models (LLMs)—to various emotional primes. We engaged AI chatbots as one would human participants, presenting scenarios designed to elicit positive, negative, or neutral emotional states. Multiple accounts of OpenAI's ChatGPT Plus were then tasked with responding to inquiries concerning investment decisions and prosocial behaviors. Our analysis revealed that ChatGPT-4 bots, when primed with positive, negative, or neutral emotions, exhibited distinct response patterns in both risk-taking and prosocial decisions, a phenomenon less evident in the ChatGPT-3.5 iterations. This observation suggests an enhanced capacity for modulating responses based on emotional cues in more advanced LLMs. While these findings do not suggest the presence of emotions in AI, they underline the feasibility of swaying AI responses by leveraging emotional indicators.

#### Resumen (Español)

Este trabajo examina risk and prosocial behavioural cues elicit human-like response patterns from ai chatbots dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que emotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes 'emotion' within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots—specifically, large language models (LLMs)—to various emotional primes. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 227

**Título original:** Testing theories of political persuasion using AI

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Lisa P. Argyle, Ethan C. Busby, Joshua R. Gubler, A. Lyman, Jocelyn Olcott, Jackson Pond, David Wingate

**Keywords:** Persuasion, Elaboration likelihood model, Psychology, Social psychology, Politics

**URL:** https://doi.org/10.1073/pnas.2412815122

#### Abstract (English)

Despite its importance to society and many decades of research, key questions about the social and psychological processes of political persuasion remain unanswered, often due to data limitations. We propose that AI tools, specifically generative large language models (LLMs), can be used to address these limitations, offering important advantages in the study of political persuasion. In two preregistered online survey experiments, we demonstrate the potential of generative AI as a tool to study persuasion and provide important insights about the psychological and communicative processes that lead to increased persuasion. Specifically, we test the effects of four AI-generated counterattitudinal persuasive strategies, designed to test the effectiveness of messages that include customization (writing messages based on a receiver’s personal traits and beliefs), and elaboration (increased psychological engagement with the argument through interaction). We find that all four types of persuasive AI produce significant attitude change relative to the control and shift vote support for candidates espousing views consistent with the treatments. However, we do not find evidence that message customization via microtargeting or cognitive elaboration through interaction with the AI have much more persuasive effect than a single generic message. These findings have implications for different theories of persuasion, which we discuss. Finally, we find that although persuasive messages are able to moderate some people’s attitudes, they have inconsistent and weaker effects on the democratic reciprocity people grant to their political opponents. This suggests that attitude moderation (ideological depolarization) does not necessarily lead to increased democratic tolerance or decreased affective polarization.

#### Resumen (Español)

Este trabajo examina testing theories of political persuasion using ai dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que despite its importance to society and many decades of research, key questions about the social and psychological processes of political persuasion remain unanswered, often due to data limitations. We propose that AI tools, specifically generative large language models (LLMs), can be used to address these limitations, offering important advantages in the study of political persuasion. In two preregistered online survey experiments, we demonstrate the potential of generative AI as a tool to study persuasion and provide important insights about the psychological and communicative processes that lead to increased persuasion. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 228

**Título original:** Simulating Misinformation Propagation in Social Networks using Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Raj Maurya, Vaibhav Shukla, Raj Dandekar, Rajat Dandekar, Sreedath Panat

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2511.10384

#### Abstract (English)

Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.

#### Resumen (Español)

Este trabajo examina simulating misinformation propagation in social networks using large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 229

**Título original:** Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Ivan Zakazov, Mikolaj Boronski, Lorenzo Drudi, Robert West

**Keywords:** Personality, Psychology, Cognitive psychology, Social psychology, Evaluation

**URL:** https://arxiv.org/abs/2412.16772

#### Abstract (English)

The ongoing revolution in language modeling has led to various novel applications, some of which rely on the emerging social abilities of large language models (LLMs). Already, many turn to the new cyber friends for advice during the pivotal moments of their lives and trust them with the deepest secrets, implying that accurate shaping of the LLM's personality is paramount. To this end, state-of-the-art approaches exploit a vast variety of training data, and prompt the model to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e., make decisions when presented with a social situation) in line with the ascribed personality (ii) if their behavior can be finely controlled. We use classic psychological experiments, the Milgram experiment and the Ultimatum Game, as social interaction testbeds and apply personality prompting to open- and closed-source LLMs from 4 different vendors. Our experiments reveal failure modes of the prompt-based modulation of the models' behavior that are shared across all models tested and persist under prompt perturbations. These findings challenge the optimistic sentiment toward personality prompting generally held in the community.

#### Resumen (Español)

Este trabajo examina assessing social alignment: do personality-prompted large language models behave like humans? dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que the ongoing revolution in language modeling has led to various novel applications, some of which rely on the emerging social abilities of large language models (LLMs). Already, many turn to the new cyber friends for advice during the pivotal moments of their lives and trust them with the deepest secrets, implying that accurate shaping of the LLM's personality is paramount. To this end, state-of-the-art approaches exploit a vast variety of training data, and prompt the model to adopt a particular personality. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 230

**Título original:** Misalignment of LLM-Generated Personas with Human Perceptions in Low-Resource Settings

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Tabia Tanzin Prama, Christopher M. Danforth, Peter Sheridan Dodds

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2512.02058

#### Abstract (English)

Recent advances enable Large Language Models (LLMs) to generate AI personas, yet their lack of deep contextual, cultural, and emotional understanding poses a significant limitation. This study quantitatively compared human responses with those of eight LLM-generated social personas (e.g., Male, Female, Muslim, Political Supporter) within a low-resource environment like Bangladesh, using culturally specific questions. Results show human responses significantly outperform all LLMs in answering questions, and across all matrices of persona perception, with particularly large gaps in empathy and credibility. Furthermore, LLM-generated content exhibited a systematic bias along the lines of the ``Pollyanna Principle'', scoring measurably higher in positive sentiment ($Φ_{avg} = 5.99$ for LLMs vs. $5.60$ for Humans). These findings suggest that LLM personas do not accurately reflect the authentic experience of real people in resource-scarce environments. It is essential to validate LLM personas against real-world human data to ensure their alignment and reliability before deploying them in social science research.

#### Resumen (Español)

Este trabajo examina misalignment of llm-generated personas with human perceptions in low-resource settings dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que recent advances enable Large Language Models (LLMs) to generate AI personas, yet their lack of deep contextual, cultural, and emotional understanding poses a significant limitation. This study quantitatively compared human responses with those of eight LLM-generated social personas (e.g., Male, Female, Muslim, Political Supporter) within a low-resource environment like Bangladesh, using culturally specific questions. Results show human responses significantly outperform all LLMs in answering questions, and across all matrices of persona perception, with particularly large gaps in empathy and credibility. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 231

**Título original:** Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Sanjeeevan Selvaganapathy, Mehwish Nasim

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2509.00673

#### Abstract (English)

We investigate the efficacy of Large Language Models (LLMs) in detecting implicit and explicit hate speech, examining whether models with minimal safety alignment (uncensored) might provide more objective classification capabilities compared to their heavily-aligned (censored) counterparts. While uncensored models theoretically offer a less constrained perspective free from moral guardrails that could bias classification decisions, our results reveal a surprising trade-off: censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy. However, this enhanced performance comes with its own limitation -- the safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable to ideological framing. Furthermore, we identify critical failures across all models in understanding nuanced language such as irony. We also find alarming fairness disparities in performance across different targeted groups and systemic overconfidence that renders self-reported certainty unreliable. These findings challenge the notion of LLMs as objective arbiters and highlight the need for more sophisticated auditing frameworks that account for fairness, calibration, and ideological consistency.

#### Resumen (Español)

Este trabajo examina confident, calibrated, or complicit: probing the trade-offs between safety alignment and ideological bias in language models in detecting hate speech dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que we investigate the efficacy of Large Language Models (LLMs) in detecting implicit and explicit hate speech, examining whether models with minimal safety alignment (uncensored) might provide more objective classification capabilities compared to their heavily-aligned (censored) counterparts. While uncensored models theoretically offer a less constrained perspective free from moral guardrails that could bias classification decisions, our results reveal a surprising trade-off: censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy. However, this enhanced performance comes with its own limitation -- the safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable to ideological framing. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 232

**Título original:** Generative AI voting: fair collective choice is resilient to LLM biases and inconsistencies

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Srijoni Majumdar, Edith Elkind, Evangelos Pournaras

**Keywords:** Generative grammar, Voting, Computer science, Psychology, Political science

**URL:** https://arxiv.org/abs/2406.11871

#### Abstract (English)

Recent breakthroughs in generative artificial intelligence (AI) and large language models (LLMs) unravel new capabilities for AI personal assistants to overcome cognitive bandwidth limitations of humans, providing decision support or even direct representation of abstained human voters at large scale. However, the quality of this representation and what underlying biases manifest when delegating collective decision making to LLMs is an alarming and timely challenge to tackle. By rigorously emulating more than &gt;50K LLM voting personas in 363 real-world voting elections, we disentangle how AI-generated choices differ from human choices and how this affects collective decision outcomes. Complex preferential ballot formats show significant inconsistencies compared to simpler majoritarian elections, which demonstrate higher consistency. Strikingly, proportional ballot aggregation methods such as equal shares prove to be a win-win: fairer voting outcomes for humans and fairer AI representation, especially for voters likely to abstain. This novel underlying relationship proves paramount for building democratic resilience in scenarios of low voters turnout by voter fatigue: abstained voters are mitigated via AI representatives that recover representative and fair voting outcomes. These interdisciplinary insights provide decision support to policymakers and citizens for developing safeguards and policies for risks of using AI in democratic innovations.

#### Resumen (Español)

Este trabajo examina generative ai voting: fair collective choice is resilient to llm biases and inconsistencies dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que recent breakthroughs in generative artificial intelligence (AI) and large language models (LLMs) unravel new capabilities for AI personal assistants to overcome cognitive bandwidth limitations of humans, providing decision support or even direct representation of abstained human voters at large scale. However, the quality of this representation and what underlying biases manifest when delegating collective decision making to LLMs is an alarming and timely challenge to tackle. By rigorously emulating more than &gt;50K LLM voting personas in 363 real-world voting elections, we disentangle how AI-generated choices differ from human choices and how this affects collective decision outcomes. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 233

**Título original:** AI, social desirability, and personality assessments: Impression management in large language models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Luke Treglown, Adrian Furnham

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://doi.org/10.1016/j.paid.2025.113563

#### Abstract (English)

This paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings.

#### Resumen (Español)

Este trabajo examina ai, social desirability, and personality assessments: impression management in large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que this paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 234

**Título original:** Understanding Down Syndrome Stereotypes in LLM-Based Personas

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Chen Wu, Peng Wang, Nafi Nibras, Meiyu Li, Dajun Yuan, Zhixiao Wang, Jason He, Mona A. S. Ali, Mirjana Prpa

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2512.02275

#### Abstract (English)

We present a case study of Persona-L, a system that leverages large language models (LLMs) and retrieval-augmented generation (RAG) to model personas of people with Down syndrome. Existing approaches to persona creation can often lead to oversimplified or stereotypical profiles of people with Down Syndrome. To that end, we built stereotype detection capabilities into Persona-L. Through interviews with caregivers and healthcare professionals (N=10), we examine how Down Syndrome stereotypes could manifest in both, content and delivery of LLMs, and interface design. Our findings show the challenges in stereotypes definition, and reveal the potential stereotype emergence from the training data, interface design, and the tone of LLM output. This highlights the need for participatory methods that capture the heterogeneity of lived experiences of people with Down Syndrome.

#### Resumen (Español)

Este trabajo examina understanding down syndrome stereotypes in llm-based personas dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que we present a case study of Persona-L, a system that leverages large language models (LLMs) and retrieval-augmented generation (RAG) to model personas of people with Down syndrome. Existing approaches to persona creation can often lead to oversimplified or stereotypical profiles of people with Down Syndrome. To that end, we built stereotype detection capabilities into Persona-L. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 235

**Título original:** “I’ve never seen a glass ceiling better represented”: Bias and gendering in LLM-generated synthetic personas from a participatory design perspective

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Helena A. Haxvig, Vincenzo D’Andrea, Maurizio Teli

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://doi.org/10.1016/j.ijhcs.2025.103651

#### Abstract (English)

This paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings.

#### Resumen (Español)

Este trabajo examina “i’ve never seen a glass ceiling better represented”: bias and gendering in llm-generated synthetic personas from a participatory design perspective dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que this paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 236

**Título original:** Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Simon Münker

**Keywords:** Statement (logic), Context (archaeology), Differential (mechanical device), Epistemology, Value (mathematics)

**URL:** https://arxiv.org/abs/2408.11415

#### Abstract (English)

Contemporary research in social sciences increasingly utilizes state-of-the-art generative language models to annotate or generate content. While these models achieve benchmark-leading performance on common language tasks, their application to novel out-of-domain tasks remains insufficiently explored. To address this gap, we investigate how personalized language models align with human responses on the Moral Foundation Theory Questionnaire. We adapt open-source generative language models to different political personas and repeatedly survey these models to generate synthetic data sets where model-persona combinations define our sub-populations. Our analysis reveals that models produce inconsistent results across multiple repetitions, yielding high response variance. Furthermore, the alignment between synthetic data and corresponding human data from psychological studies shows a weak correlation, with conservative persona-prompted models particularly failing to align with actual conservative populations. These results suggest that language models struggle to coherently represent ideologies through in-context prompting due to their alignment process. Thus, using language models to simulate social interactions requires measurable improvements in in-context optimization or parameter manipulation to align with psychological and sociological stereotypes properly.

#### Resumen (Español)

Este trabajo examina political bias in llms: unaligned moral values in agent-centric simulations dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que contemporary research in social sciences increasingly utilizes state-of-the-art generative language models to annotate or generate content. While these models achieve benchmark-leading performance on common language tasks, their application to novel out-of-domain tasks remains insufficiently explored. To address this gap, we investigate how personalized language models align with human responses on the Moral Foundation Theory Questionnaire. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 237

**Título original:** Large Language Models Polarize Ideologically but Moderate Affectively in Online Political Discourse

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Gavin Wang, Srinaath Anbudurai, Oliver Sun, Xitong Li, Lynn Wu

**Keywords:** Ideology, Politics, Hostility, Sociology, Critical discourse analysis

**URL:** https://arxiv.org/abs/2601.20238

#### Abstract (English)

The emergence of large language models (LLMs) is reshaping how people engage in political discourse online. We examine how the release of ChatGPT altered ideological and emotional patterns in the largest political forum on Reddit. Analysis of millions of comments shows that ChatGPT intensified ideological polarization: liberals became more liberal, and conservatives more conservative. This shift does not stem from the creation of more persuasive or ideologically extreme original content using ChatGPT. Instead, it originates from the tendency of ChatGPT-generated comments to echo and reinforce the viewpoint of original posts, a pattern consistent with algorithmic sycophancy. Yet, despite growing ideological divides, affective polarization, measured by hostility and toxicity, declined. These findings reveal that LLMs can simultaneously deepen ideological separation and foster more civil exchanges, challenging the long-standing assumption that extremity and incivility necessarily move together.

#### Resumen (Español)

Este trabajo examina large language models polarize ideologically but moderate affectively in online political discourse dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que the emergence of large language models (LLMs) is reshaping how people engage in political discourse online. We examine how the release of ChatGPT altered ideological and emotional patterns in the largest political forum on Reddit. Analysis of millions of comments shows that ChatGPT intensified ideological polarization: liberals became more liberal, and conservatives more conservative. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 238

**Título original:** From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Suyash Fulay, Jian Zhu, M. Bakker

**Keywords:** Large Language Models, Personality, Bias, Social Impact, Evaluation

**URL:** https://arxiv.org/abs/2510.12689

#### Abstract (English)

Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on "behavioral cloning", effectively evaluating how well models reproduce individuals' expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests. Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward models' default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.

#### Resumen (Español)

Este trabajo examina from delegates to trustees: how optimizing for long-term interests shapes bias and alignment in llm dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on "behavioral cloning", effectively evaluating how well models reproduce individuals' expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 239

**Título original:** Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh

**Keywords:** Mechanism (biology), Computer science, Position (finance), Power (physics), Position paper

**URL:** https://arxiv.org/abs/2505.21091

#### Abstract (English)

System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.

#### Resumen (Español)

Este trabajo examina position is power: system prompts as a mechanism of bias in large language models (llms) dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que system prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 240

**Título original:** Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Shahnewaz Karim Sakib, Anindya Bijoy Das, Shibbir Ahmed

**Keywords:** Misinformation, Computer science, Adversarial system, Open source, Language model

**URL:** https://aclanthology.org/2025.trustnlp-main.28/

#### Abstract (English)

This paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings.

#### Resumen (Español)

Este trabajo examina battling misinformation: an empirical study on adversarial factuality in open-source large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que this paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 241

**Título original:** “I understand your perspective”: LLM Persuasion through the Lens of Communicative Action Theory

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Esra Dönmez, Agnieszka Faleńska

**Keywords:** Persuasion, Perspective (graphical), Communicative action, Action (physics), Lens (geology)

**URL:** https://aclanthology.org/2025.findings-acl.793/

#### Abstract (English)

This paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings.

#### Resumen (Español)

Este trabajo examina “i understand your perspective”: llm persuasion through the lens of communicative action theory dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que this paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 242

**Título original:** Can LLMs Ground when they (Don’t) Know: A Study on Direct and Loaded Political Questions

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Clara Lachenmaier, Judith Sieker, Sina Zarrieß

**Keywords:** Politics, Computer science, Political science, Law, Evaluation

**URL:** https://aclanthology.org/2025.acl-long.728/

#### Abstract (English)

This paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings.

#### Resumen (Español)

Este trabajo examina can llms ground when they (don’t) know: a study on direct and loaded political questions dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que this paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 243

**Título original:** Investigating Cultural Alignment of Large Language Models

**Categoría:** Aplicaciones, sesgos y consecuencias sociales

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, Mona Diab

**Keywords:** Computer science, Natural language processing, Linguistics, Artificial intelligence, Philosophy

**URL:** https://aclanthology.org/2024.acl-long.671/

#### Abstract (English)

This paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings.

#### Resumen (Español)

Este trabajo examina investigating cultural alignment of large language models dentro de la línea de aplicaciones, sesgos y consecuencias sociales en LLMs. El artículo reporta que this paper studies social, bias, and behavioral effects related to synthetic personality and role conditioning in large language models, with emphasis on measurable impacts in realistic interaction settings. En conjunto, la evidencia aporta criterios para comparar resultados entre modelos y para diseñar evaluaciones más sólidas sobre sesgo, alineación y efectos en usuarios.

---

### Artículo 244

**Título original:** Extroversion or Introversion? Controlling The Personality of Your Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yanquan Chen, Zhen Wu, Junjie Guo, Shujian Huang, Xinyu Dai

**Keywords:** Extraversion and introversion, Personality, Psychology, Social psychology, Big Five personality traits

**URL:** https://arxiv.org/abs/2406.04583

#### Abstract (English)

Large language models (LLMs) exhibit robust capabilities in text generation and comprehension, mimicking human behavior and exhibiting synthetic personalities. However, some LLMs have displayed offensive personality, propagating toxic discourse. Existing literature neglects the origin and evolution of LLM personalities, as well as the effective personality control. To fill these gaps, our study embarked on a comprehensive investigation into LLM personality control. We investigated several typical methods to influence LLMs, including three training methods: Continual Pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along with inference phase considerations (prompts). Our investigation revealed a hierarchy of effectiveness in control: Prompt &gt; SFT &gt; RLHF &gt; Continual Pre-train. Notably, SFT exhibits a higher control success rate compared to prompt induction. While prompts prove highly effective, we found that prompt-induced personalities are less robust than those trained, making them more prone to showing conflicting personalities under reverse personality prompt induction. Besides, harnessing the strengths of both SFT and prompt, we proposed $\underline{\text{P}}$rompt $\underline{\text{I}}$nduction post $\underline{\text{S}}$upervised $\underline{\text{F}}$ine-tuning (PISF), which emerges as the most effective and robust strategy for controlling LLMs' personality, displaying high efficacy, high success rates, and high robustness. Even under reverse personality prompt induction, LLMs controlled by PISF still exhibit stable and robust personalities.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que large language models (LLMs) exhibit robust capabilities in text generation and comprehension, mimicking human behavior and exhibiting synthetic personalities. However, some LLMs have displayed offensive personality, propagating toxic discourse. Existing literature neglects the origin and evolution of LLM personalities, as well as the effective personality control. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 245

**Título original:** Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou

**Keywords:** Computer science, Language model, Artificial intelligence, Natural language processing, Theoretical computer science

**URL:** https://aclanthology.org/2024.acl-long.423/

#### Abstract (English)

Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, we introduce Ditto, the first self-alignment method for role-play, which encourages an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension, and creates a role-play training set comprising 4000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed role-play benchmark and the roleplay subset of MT-Bench, Ditto, in various parameter scales, consistently maintains a consistent role identity and provides accurate role-specific knowledge in multi-turn role-play conversations, outperforming all open-source role-play baselines. Furthermore, we present the first cross-supervision role-play experiment, revealing that the role-play styles can be easily acquired, while the intrinsic capabilities of LLMs confine the knowledge within role-play.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, we introduce Ditto, the first self-alignment method for role-play, which encourages an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension, and creates a role-play training set comprising 4000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 246

**Título original:** Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Yun‐Shiuan Chuang, Krirk Nirunwiroj, Zach Studdiford, Agam Goyal, Vincent Frigo, Sijia Yang, Dhavan V. Shah, Junjie Hu, Timothy T. Rogers

**Keywords:** Demographics, Computer science, Sociology, Demography, Steering

**URL:** https://aclanthology.org/2024.findings-emnlp.819/

#### Abstract (English)

Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 247

**Título original:** Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles

**Categoría:** Inducción y control de personalidad

**Año:** 2024 | **Idioma:** Inglés

**Autores:** Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, Diyi Yang

**Keywords:** Computer science, Domain (mathematical analysis), Mathematics, Mathematical analysis, Steering

**URL:** https://aclanthology.org/2024.emnlp-main.591/

#### Abstract (English)

Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in the domain of mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients as simulated practice partners for novice counselors. After uncovering issues with basic GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows a 30% improvement in response quality and principle following for the downstream task. Through a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by both creators and third-party counselors. We provide access to the code and data on our project website: https://roleplay-doh.github.io/.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in the domain of mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 248

**Título original:** Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Xiaoyu Zhan, Xinyu Fu, Hao Sun, Yuanqi Li, Guo Jie, Yanwen Guo

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2507.16799

#### Abstract (English)

The rapid advancement of large language models (LLMs) has enabled role-playing language agents to demonstrate significant potential in various applications. However, relying solely on prompts and contextual inputs often proves insufficient for achieving deep immersion in specific roles, particularly well-known fictional or public figures. On the other hand, fine-tuning-based approaches face limitations due to the challenges associated with data collection and the computational resources required for training, thereby restricting their broader applicability. To address these issues, we propose Test-Time-Matching (TTM), a training-free role-playing framework through test-time scaling and context engineering. TTM uses LLM agents to automatically decouple a character's features into personality, memory, and linguistic style. Our framework involves a structured, three-stage generation pipeline that utilizes these features for controlled role-playing. It achieves high-fidelity role-playing performance, also enables seamless combinations across diverse linguistic styles and even variations in personality and memory. We evaluate our framework through human assessment, and the results demonstrate that our method achieves the outstanding performance in generating expressive and stylistically consistent character dialogues.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que the rapid advancement of large language models (LLMs) has enabled role-playing language agents to demonstrate significant potential in various applications. However, relying solely on prompts and contextual inputs often proves insufficient for achieving deep immersion in specific roles, particularly well-known fictional or public figures. On the other hand, fine-tuning-based approaches face limitations due to the challenges associated with data collection and the computational resources required for training, thereby restricting their broader applicability. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 249

**Título original:** Multi-Personality Generation of LLMs at Decoding-time

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** R.S. Chen, Yinjin Li, Yufei Yuan, Bingbing Xu, Huawei Shen

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2511.01891

#### Abstract (English)

Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a "free lunch" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 250

**Título original:** Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Eric Yeh, John Cadigan, Ran Chen, Dick Crouch, Melinda Gervasio, Dayne Freitag

**Keywords:** Personality, Fidelity, Psychology, Replicate, Big Five personality traits

**URL:** https://arxiv.org/abs/2512.19937

#### Abstract (English)

Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 251

**Título original:** The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Wang, Zhixiang

**Keywords:** Computer science, Representation (politics), Artificial intelligence, Probabilistic logic, Personality

**URL:** https://arxiv.org/abs/2512.07092

#### Abstract (English)

Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities. Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights. Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies. Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities. Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 252

**Título original:** Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Dai, Shi-Wei, Shie, Yan-Wei, Yang, Tsung-Huan, Ku, Lun-Wei, Li, Yung-Hui

**Keywords:** Personality, Big Five personality traits, Expression (computer science), Situational ethics, Psychology

**URL:** https://arxiv.org/abs/2511.19852

#### Abstract (English)

Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 253

**Título original:** BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Tzu‐Yi Pai, J Wang, Li-Chun Lu, Shaohua Sun, Hung-Yi Lee, Kai-Wei Chang

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2510.10157

#### Abstract (English)

Multi-LLM systems enhance the creativity of large language models by simulating human collective intelligence but suffer from significant drawbacks, such as high computational costs and inference latency. To address these limitations, we propose BILLY (BlendIng persona vectors for Large Language model creativitY), a training-free framework that captures the benefits of multi-LLM collaboration, i.e. inducing diverse perspectives and specialized expertise, within a single model. BILLY operates by extracting and blending multiple distinct persona vectors directly in the model's activation space. We steer the model's generation process with this merged vector while inference, enabling multi-perspective output without explicit multi-LLM communication. Our experiments across creativity-oriented benchmarks demonstrate that BILLY surpasses single model prompting and traditional multi-LLM approaches, while substantially reducing inference time and computational costs. Our analyses further reveal that distinct persona vectors can be blended to achieve both effective control over complementary aspects of generation and greater interpretability.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que multi-LLM systems enhance the creativity of large language models by simulating human collective intelligence but suffer from significant drawbacks, such as high computational costs and inference latency. To address these limitations, we propose BILLY (BlendIng persona vectors for Large Language model creativitY), a training-free framework that captures the benefits of multi-LLM collaboration, i.e. inducing diverse perspectives and specialized expertise, within a single model. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 254

**Título original:** IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yuzhuo Bai, Shitong Duan, Muhua Huang, Jing Yao, Zhenghao Liu, Peng Zhang, Tun Lü, Xiaoyuan Yi, Maosong Sun, Xing Xie

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2508.08719

#### Abstract (English)

Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs' trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs' behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs' stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 255

**Título original:** Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Amin Banayeeanzade, Ala N. Tak, Fatemeh Bahrani, Anahita Bolourani, Leonardo Blas, Emilio Ferrara, Jonathan Gratch, Sai Praneeth Karimireddy

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2510.04484

#### Abstract (English)

The ability to control LLMs' emulated emotional states and personality traits is essential for enabling rich, human-centered interactions in socially interactive settings. We introduce PsySET, a Psychologically-informed benchmark to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion and personality domains. Our study spans four models from different LLM families paired with various steering strategies, including prompting, fine-tuning, and representation engineering. Our results indicate that prompting is consistently effective but limited in intensity control, whereas vector injections achieve finer controllability while slightly reducing output quality. Moreover, we explore the trustworthiness of steered LLMs by assessing safety, truthfulness, fairness, and ethics, highlighting potential side effects and behavioral shifts. Notably, we observe idiosyncratic effects; for instance, even a positive emotion like joy can degrade robustness to adversarial factuality, lower privacy awareness, and increase preferential bias. Meanwhile, anger predictably elevates toxicity yet strengthens leakage resistance. Our framework establishes the first holistic evaluation of emotion and personality steering, offering insights into its interpretability and reliability for socially interactive applications.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que the ability to control LLMs' emulated emotional states and personality traits is essential for enabling rich, human-centered interactions in socially interactive settings. We introduce PsySET, a Psychologically-informed benchmark to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion and personality domains. Our study spans four models from different LLM families paired with various steering strategies, including prompting, fine-tuning, and representation engineering. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 256

**Título original:** AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Ajay Naik, Patrick Quinn, Guillermo Bosch, Emma Gouné, Francisco Javier Campos Zabala, Jason R. Brown, Edward Young

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2506.04018

#### Abstract (English)

As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agents' ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. We introduce a misalignment propensity benchmark, \textsc{AgentMisalignment}, a benchmark suite designed to evaluate the propensity of LLM agents to misalign in realistic scenarios. Evaluations cover behaviours such as avoiding oversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier models, we find that more capable agents tend to exhibit higher misalignment on average. We also systematically vary agent personalities through different system prompts and observe that persona characteristics can strongly and unpredictably influence misalignment, sometimes more than the choice of model itself. Our results reveal the limitations of current alignment methods for autonomous LLM agents and underscore the need to rethink misalignment in realistic deployment settings.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que as Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agents' ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 257

**Título original:** Grounded Test-Time Adaptation for LLM Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Chen, Arthur, Liu Hon, Jianguo Zhang, Akshara Prabhakar, Zhiwei Liu, Shelby Heinecke, Silvio Savarese, Victor W. Zhong, Caiming Xiong

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2511.04847

#### Abstract (English)

Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 258

**Título original:** Aligning LLM agents with human learning and adjustment behavior: a dual agent approach

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Tianming Liu, Jirong Yang, Yafeng Yin, M. H. Li, Linghao Wang, Zheng Zhu

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2511.00993

#### Abstract (English)

Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 259

**Título original:** PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Siqi Liang, Yudi Zhang, Yingying Guo

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2511.17467

#### Abstract (English)

We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que we propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 260

**Título original:** Towards Effective Model Editing for LLM Personalization

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Biao Huang, Limeng Cui, Yang Liu, Haoran Wang, Jiawei Xu, Zheng Tan, Yutong Chen, Chen Luo, Yi Liu, Kai Shu

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2512.13676

#### Abstract (English)

Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 261

**Título original:** Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Durval C. Costa, Felippe Alves, Rafael J. Vicente

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2511.08565

#### Abstract (English)

Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 262

**Título original:** Too Good to be Bad: On the Failure of LLMs to Role-Play Villains

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Yi, Zihao, Jiang, Qingxuan, Ma, Ruotian, Chen, Xingyu, Yang, Qu, Wang, Mengru, Ye, Fanghua, Shen, Ying, Tu, Zhaopeng, Li, Xiaolong, Linus

**Keywords:** Morality, Set (abstract data type), Task (project management), Test (biology), Moral character

**URL:** https://arxiv.org/abs/2511.04962

#### Abstract (English)

Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 263

**Título original:** Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Jaewook Lee, Alexander Scarlatos, Simon Woodhead, Andrew Lan

**Keywords:** TUTOR, Preference, Computer science, Variation (astronomy), Generative grammar

**URL:** https://arxiv.org/abs/2602.07639

#### Abstract (English)

With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que with the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 264

**Título original:** On the Identifiability of Steering Vectors in Large Language Models

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Sohan Venkatesh, Ashish Mahendran Kurapath

**Keywords:** Identifiability, Interpretability, Computer science, Equivalence (formal languages), Interpretation (philosophy)

**URL:** https://arxiv.org/abs/2602.06801

#### Abstract (English)

Activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from input-output behavior. We formalize steering as an intervention on internal representations and prove that, under realistic modeling and data conditions, steering vectors are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Empirically, we validate this across multiple models and semantic traits, showing orthogonal perturbations achieve near-equivalent efficacy with negligible effect sizes. However, identifiability is recoverable under structural assumptions including statistical independence, sparsity constraints, multi-environment validation or cross-layer consistency. These findings reveal fundamental interpretability limits and clarify structural assumptions required for reliable safety-critical control.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from input-output behavior. We formalize steering as an intervention on internal representations and prove that, under realistic modeling and data conditions, steering vectors are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 265

**Título original:** Styles + Persona-plug = Customized LLMs

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Yutong Song, Jiang Wu, Shaofan Yuan, Chengze Shen, Jian Wang, Amir Rahmani, Nikil Dutt, Yu Wang

**Keywords:** Personalization, Computer science, Persona, Preference, Style (visual arts)

**URL:** https://arxiv.org/abs/2601.06362

#### Abstract (English)

We discover a previously overlooked challenge in personalized text generation: personalization methods are increasingly applied under explicit style instructions, yet their behavior under such constraints remains poorly understood. To balance implicit personalization and explicit style, we formulate personalization as a distributional residual and propose PsPLUG, a lightweight soft-prompt plug-in trained with style-conditioned preference contrasts. Across LaMP benchmark, our framework improves persona alignment, maintains stylistic fidelity, and outperforms retrieval-based and soft-prompt baselines with minimal computation. These results show that residual modeling provides a simple and principled foundation for controllable, style-aware LLM personalization.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que we discover a previously overlooked challenge in personalized text generation: personalization methods are increasingly applied under explicit style instructions, yet their behavior under such constraints remains poorly understood. To balance implicit personalization and explicit style, we formulate personalization as a distributional residual and propose PsPLUG, a lightweight soft-prompt plug-in trained with style-conditioned preference contrasts. Across LaMP benchmark, our framework improves persona alignment, maintains stylistic fidelity, and outperforms retrieval-based and soft-prompt baselines with minimal computation. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 266

**Título original:** HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Chengyu Du, Xintao Wang, Aili Chen, Weiyuan Li, Rui Xu, Junteng Liu, Zishan Huang, Rong Tian, Zijun Sun, Yuhao Li, Liheng Feng, Deming Ding, Pengyu Zhao, Yanghua Xiao

**Keywords:** Reinforcement learning, Computer science, Artificial intelligence, Construct (python library), Key (lock)

**URL:** https://arxiv.org/abs/2601.21459

#### Abstract (English)

LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train HER models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que lLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 267

**Título original:** PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Dehao Yuan, Tyler Farnan, Stefan Tesliuc, Doron L. Bergman, Yulun Wu, Xiaoyu Liu, Minghui Liu, James Montgomery, Nam H Nguyen, C. Bayan Bruss, Furong Huang

**Keywords:** Computer science, Persona, Context (archaeology), Workflow, Database transaction

**URL:** https://arxiv.org/abs/2601.03149

#### Abstract (English)

Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 268

**Título original:** Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures

**Categoría:** Inducción y control de personalidad

**Año:** 2026 | **Idioma:** Inglés

**Autores:** Yanghao Su, Wenbo Zhou, Tianwei Zhang, Han Qiu, Weiming Zhang, Nenghai Yu, Jie Zhang

**Keywords:** Backdoor, Generalization, Computer science, Character (mathematics), Latent variable

**URL:** https://arxiv.org/abs/2601.23081

#### Abstract (English)

Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 269

**Título original:** German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Rupprecht, Jens, Fröhling, Leon, Wagner, Claudia, Strohmaier, Markus

**Keywords:** Persona, German, Representativeness heuristic, Population, Computer science

**URL:** https://arxiv.org/abs/2511.21722

#### Abstract (English)

The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que the use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 270

**Título original:** Behavioral Guardrails for Dynamic LLM Persona

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Ognjen Malkoc, K. Abe, Kazuki Kitagawa, Masashi Oka, Yuya Ishikawa

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://doi.org/10.1162/isal.a.855

#### Abstract (English)

We demonstrate an automated instruction-tuning process using Low-Rank Adaptation (LoRA) Hu et al. (2022) to align small language models with user-defined behavior guardrails. This enables safeguards for artificial characters with dynamically changeable traits. The process requires only trigger and resolution instructions, which we also leverage to generate synthetic training data via an auxiliary large language model. We exemplify the method by applying it to varying LLM-based personas (defined by biographies, traits, and conversation history) and show that merging guardrail adapters to the base model allows reliable detection and coherent resolution of unwanted behaviors.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que we demonstrate an automated instruction-tuning process using Low-Rank Adaptation (LoRA) Hu et al. (2022) to align small language models with user-defined behavior guardrails. This enables safeguards for artificial characters with dynamically changeable traits. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 271

**Título original:** SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Michael J. Ryan, Omar Shaikh, Aditri Bhagirath, Daniel Frees, William A. Held, Diyi Yang

**Keywords:** Large Language Models, Personality Control, Prompting, Persona, Steering

**URL:** https://arxiv.org/abs/2506.05598

#### Abstract (English)

Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 272

**Título original:** LLM Personas as a Substitute for Field Experiments in Method Benchmarking

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Enoch Hyunwook Kang

**Keywords:** Benchmarking, Persona, Benchmark (surveying), Computer science, Field (mathematics)

**URL:** https://arxiv.org/abs/2512.21080

#### Abstract (English)

Field experiments (A/B tests) are often the most credible benchmark for methods (algorithms) in societal systems, but their cost and latency bottleneck rapid methodological progress. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the method's identity or provenance (method-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que field experiments (A/B tests) are often the most credible benchmark for methods (algorithms) in societal systems, but their cost and latency bottleneck rapid methodological progress. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the method's identity or provenance (method-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 273

**Título original:** The Effects of Demographic Instructions on LLM Personas

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Angel Felipe Magnossão de Paula, J. Shane Culpepper, Alistair Moffat, Sachin Pathiyan Cherumanal, Falk Scholer, Johanne R. Trippas

**Keywords:** Persona, Computer science, Prompting, Persona, Steering

**URL:** https://doi.org/10.1145/3726302.3730255

#### Abstract (English)

Social media platforms must filter sexist content in compliance with governmental regulations. Current machine learning approaches can reliably detect sexism based on standardized definitions, but often neglect the subjective nature of sexist language and fail to consider individual users' perspectives. To address this gap, we adopt a perspectivist approach, retaining diverse annotations rather than enforcing gold-standard labels or their aggregations, allowing models to account for personal or group-specific views of sexism. Using demographic data from Twitter, we employ large language models (LLMs) to personalize the identification of sexism.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que social media platforms must filter sexist content in compliance with governmental regulations. Current machine learning approaches can reliably detect sexism based on standardized definitions, but often neglect the subjective nature of sexist language and fail to consider individual users' perspectives. To address this gap, we adopt a perspectivist approach, retaining diverse annotations rather than enforcing gold-standard labels or their aggregations, allowing models to account for personal or group-specific views of sexism. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.

---

### Artículo 274

**Título original:** Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents

**Categoría:** Inducción y control de personalidad

**Año:** 2025 | **Idioma:** Inglés

**Autores:** Chaoran Chen, Bingsheng Yao, Ruishi Zou, Wenyue Hua, Weimin Lyu, Tingting Li, De Yun Wang, Wang, Dakuo

**Keywords:** Guideline, Computer science, Medicine, Pathology, Steering

**URL:** https://arxiv.org/abs/2502.13012

#### Abstract (English)

Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.

#### Resumen (Español)

Este trabajo se incorpora en la línea de inducción y control de personalidad para analizar cómo los LLMs pueden adoptar, mantener o modular rasgos mediante prompting, memoria, roles o técnicas de steering. En términos técnicos, el estudio plantea que role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. Su valor para esta bitácora está en aportar evidencia comparable sobre control de rasgos, estabilidad de identidad y límites de intervención sobre la «personalidad sintética» del modelo.
